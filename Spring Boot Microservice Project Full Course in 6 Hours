00:00 in this video you're going to learn how
00:02 to develop a spring boot application
00:03 using microservices architecture this is
00:06 going to be a highly practical tutorial
00:07 so to follow along I expect you to have
00:10 at least a basic understanding about
00:11 spring Boot and basic understanding of
00:13 what microservices architecture is and
00:15 why we use them so we'll be building a
00:18 simple online shop application using
00:19 microservices architectural patterns
00:21 using mainly spring Boot and the latest
00:23 spring Cloud Technologies so we're going
00:26 to cover all the interesting and
00:27 important architectural patterns like
00:28 service Discovery centralized
00:30 configuration distributed tracing event
00:33 driven architecture centralized logging
00:35 and much more if you are not aware of
00:37 what are these fancy terms don't worry
00:38 we'll cover this in the tutorial so
00:40 before going ahead and implementing our
00:42 micro Services let's take a moment to
00:44 understand what is spring cloud
00:46 spring cloud is a project under the
00:49 spring projects ecosystem so it's
00:51 another project like spring boot spring
00:53 data Spring Security or any other spring
00:55 project but it mainly concentrates on
00:57 helping us to build the reliable and
01:00 robust micro Services it provides us a
01:03 common set of design patterns which are
01:05 implemented already like the
01:07 configuration Management Service
01:09 Discovery circuit breakers so and there
01:12 are many other patterns which are
01:14 implemented into the spring Cloud
01:15 project we'll have a look at it in depth
01:18 in this tutorial you don't need to worry
01:20 about that we are going to use spring
01:21 Cloud to mainly develop the micro
01:24 Services that's what you need to know as
01:26 you understood what is spring Cloud now
01:28 let's go ahead and let's go ahead and
01:31 check what are the different Services
01:33 we're going to build in this tutorial so
01:35 the first service we're going to build
01:37 is going to be a product service so this
01:40 service provides us API to create and
01:43 view products in our application and it
01:46 also Acts like a product catalog where
01:48 we can view all the products inside our
01:50 inside our application
01:52 so the next service is going to be an
01:54 order service where we can order the
01:56 products which are available in the app
01:58 and after that we're also going to have
02:01 an inventory service where the order
02:03 service can check if the product is in
02:06 stock or not before submitting the order
02:09 and lastly we are also going to have a
02:11 notification service where we can send
02:13 email notifications after the order is
02:16 placed successfully
02:18 and out of these four Services we are
02:20 going to mainly have the ordered service
02:22 inventory service and the notification
02:25 service talk to each other we mainly
02:28 have the in synchronous and as well as
02:30 synchronous communication as part of
02:33 these three services so now let's go
02:35 ahead and check the overall solution
02:37 architecture of our project
02:39 all right so this is how the solution
02:41 architecture diagram looks like so
02:43 mainly we are going to first look at the
02:46 services we are developing so we have
02:48 the product service which is talking to
02:50 a mongodb and an order service which is
02:53 talking to the mySQL database and the
02:55 inventory service is also talking to the
02:57 mySQL database to store all the
02:59 inventory information and we have the
03:02 notification service which is like a
03:03 stateless service which it does not have
03:06 any database but it is responsible to
03:08 just send out notifications to users if
03:11 you are talking about a micro service
03:12 architecture you cannot call it as a
03:16 microservice architecture if the
03:17 services are not talking with each other
03:19 right so here the order service is
03:21 talking to the inventory service so
03:23 whenever the whenever order is being
03:26 placed we will first check whether the
03:28 inventory is available or Not by making
03:29 a synchronous communication
03:31 and after that we are also going to send
03:34 out the notification once the order is
03:36 being placed successfully so for that we
03:38 are going to make an asynchronous
03:40 communication with the notification
03:41 service if you are not sure what is
03:43 synchronous and asynchronous
03:44 communication don't worry we will also
03:46 cover this in detail in the in this
03:48 tutorial
03:49 coming to the asynchronous communication
03:51 we are going to make use of two
03:53 softwares like we will call it as
03:55 message queues so we are going to see
03:58 how to deal with both the popular Market
03:60 message queues in the market like the
04:01 rabbitmq as well as Kafka so we will see
04:04 how to configure both of these
04:07 um message queues to communicate tool do
04:09 the asynchronous communication and
04:11 coming to the outside world we have the
04:14 surrounding uh Services which are which
04:17 enable the microservice architecture we
04:19 have the API Gateway which acts like a
04:22 gateway to send the request from the
04:24 user to different services so for
04:27 example if I am a user and I want to
04:29 communicate with the product service I
04:31 don't want to give the host name or the
04:34 IP address of the product service to
04:36 communicate with it right so the API
04:38 Gateway acts as like a gatekeeper to
04:41 send out the requests to different
04:43 Services we like is also so our
04:45 applications is also being secured using
04:47 the authorization server called as key
04:50 clock we'll also have a look at it in
04:52 the detail and other than that we also
04:54 have different services like Eureka the
04:56 config server you're also going to use
04:58 Vault to store the secrets we're going
05:00 to use Zipkin to do the distributed
05:03 tracing and also to do the customer to
05:06 do the login to do the centralized
05:07 working you're also going to use
05:09 elasticsearch log stash and kibana and
05:11 again don't worry don't be overwhelmed
05:13 with all these details we will have a
05:15 look at each of them in detail in this
05:16 tutorial
05:18 all right so if you'll have a look at
05:19 the logical architecture of each of the
05:21 services we have a controller service
05:24 and a repository layer for each of the
05:26 services so in the controller layer we
05:29 will be receiving the HTTP request from
05:32 the from the clients and from there the
05:35 business logic is being executed on the
05:37 service layer and in some of the
05:39 services we are also going to
05:41 communicate with a message queue so
05:43 that's why we have this message queue
05:45 communication and mainly in the order
05:47 service we are going to make use of the
05:49 message queue and also the notification
05:51 service
05:51 and after that we are also going to
05:54 store the information in the database so
05:56 for that we are also going to maintain a
05:58 repository layer which mainly talks to
05:60 the database so we have a postgres and
06:02 also the mongodb database so both of
06:04 them will follow the same pattern will
06:06 have a repository which talks to the
06:08 database this is how almost all the
06:10 services are being structured so we'll
06:13 follow the same process for all the
06:15 services we have
06:17 all right so let's start coding so first
06:20 we are going to develop the product
06:22 service for that I'm opening this
06:24 website start.spring.io where you can
06:27 generate the initial version of the code
06:29 so for that I'm going to select this
06:31 latest spring boot version as of
06:32 creating this tutorial that is 2.6.4
06:36 and for the group ID I'm going to
06:38 provide
06:40 the text com programming techie and the
06:43 artifact is going to be product service
06:47 and the description it's going to be
06:49 also product service you can you can
06:51 enter whatever description you like
06:54 I'm going to use Java 17 and for the
06:57 dependencies I am going to first make
06:60 use of long work
07:01 to reduce the boilerplate code and I
07:04 also need access to Spring web so that
07:08 we are going to make use where as we are
07:09 building a rest API so I am going to use
07:12 spring web and lastly I'm also going to
07:16 add
07:18 spring data mongodb as the product
07:21 service is going to communicate with a
07:23 mongodb database this depends after
07:25 adding all these dependencies I am going
07:27 to click on generate and the product
07:29 service is going to be downloaded to
07:31 your machine and this so I'm going to
07:32 first unzip and open it on my favorite
07:34 IDE the first thing we have to do is to
07:36 configure the mongodb URI inside the
07:39 application.properties file I am going
07:40 to give the database name as product
07:42 service if you want a detailed
07:44 explanation on how to configure spring
07:46 boot with mongodb have a look at the
07:48 spring boot mogodb playlist in my
07:50 channel you can also find this in the
07:52 description section after that we have
07:53 to create our domain model which is the
07:56 product class so let's create a package
07:58 called as model
07:60 and inside the package I am going to
08:02 create a class called last product
08:07 to Define this product as a mongodb
08:09 document I am going to add the document
08:11 annotation to the top of the product
08:13 class and I'm also going to add the
08:16 lombok annotations to generate the
08:18 getter Setter methods and the
08:19 constructors
08:21 inside the class I am going to create
08:22 the required fields which are ID name
08:26 description and price and to the field
08:29 ID I am going to add the ID annotation
08:32 from Spring data to specify that this is
08:35 a unique identifier for our product and
08:38 to store this product inside the
08:39 database we have to create a spring data
08:42 repository for that I am going to create
08:44 a package called as repository and
08:46 inside this package I am going to create
08:48 an interface called as product
08:49 Repository
08:51 and I'm going to extend this interface
08:53 with the spring data mongodbs
08:56 repository interface
08:57 and I'm going to pass in the generic
08:59 argument as product and for the
09:02 identifier I am going to pass a string
09:05 as the the second generic argument
09:07 because this is the type of the ID field
09:09 in our product class
09:11 now it's time to create the controller
09:13 class so for that I am going to create a
09:15 package called as controller
09:17 and inside the package I am going to
09:19 create a class called as product
09:21 controller
09:22 as we are exposing a rest API we have to
09:25 add the rest controller annotation to
09:27 the top of this class
09:29 and also the request mapping annotation
09:31 with value slash API slash product
09:39 so now let's create the API to create
09:42 the products so for that I am going to
09:45 create a method called as public void
09:49 create product
09:51 and this method as this creates some
09:54 products we are going to send a post
09:56 request for this method so I am going to
09:59 add the post mapping annotation and as a
10:02 response we are going to type in
10:04 response status and we are going to send
10:07 out the HTTP status as
10:13 status as created so I'm going to select
10:16 HTTP status created right and as a
10:20 request body
10:23 we are going to receive the product
10:25 information for that we are going to
10:26 create a new class called as product
10:30 request
10:32 right so this is going to act as a DDO
10:35 so I'm going to create this class inside
10:37 a new package called as dto
10:41 and also this class is going to have a
10:44 long walk annotation to create the
10:46 Gators and Setters to less data
10:49 I am going to use the Builder annotation
10:51 to create the Builder method and also
10:54 the allux Constructor
10:56 and the nox Constructor
10:60 so inside the product request we're
11:02 going to mainly have three Fields
11:04 similar to what we have in product so
11:06 I'm just going to copy paste this three
11:09 Fields inside the product request
11:13 all right so we will have name
11:14 description and as well as the price so
11:17 now we have the product request and
11:19 let's go back to the product controller
11:20 and we are receiving the product request
11:22 as the request body and to actually
11:25 create the product we have to do the
11:28 processing like the processing of the
11:29 business logic in the service layer you
11:31 can't do it on the controller layer so
11:33 for that I am going to create a new
11:35 package called as service
11:39 and in here I'm going to create a new
11:41 class called as product service
11:45 so as this is a service we are going to
11:47 add a spring boot annotation called
11:50 service
11:51 and we are also going to create a new
11:54 method inside here called as public void
11:57 create product
11:59 which takes in the product request
12:03 has the method parameter
12:06 and inside here we have to first map the
12:09 product request to the product model so
12:12 for that I am going to create
12:15 a variable of type product so let's also
12:18 import the product from the package from
12:21 the model package
12:23 and create a reference variable called
12:25 Product and I'm going to make use of
12:27 product dot Builder method so to build
12:31 the product object so for that I am
12:34 going to type in the name as
12:36 productrequest dot get name
12:39 and the description as productrequest
12:41 dot get description
12:44 lastly the price as productrequest dot
12:47 get price and once we have set up all
12:50 these information I am going to call the
12:52 build method so that it will create the
12:55 object of type product with all the
12:57 requested details and lastly we have to
12:59 we have create so now we've created the
13:02 instance of the product object now we
13:05 have to save this into the database so
13:07 for that we need to access the product
13:08 repository so I'm going to inject
13:11 product repository into a service class
13:14 the product service class mainly I'm
13:16 going to make use of Constructor
13:17 injection so I'm going to type in
13:19 private final
13:21 product Repository
13:25 and after I have added the Declaration
13:28 of product repository you can see that
13:30 IntelliJ is already complaining that the
13:32 product repository is not initialized so
13:35 I'm going to add the Constructor
13:36 parameter here
13:38 so you can see the Constructor is
13:39 automatically generated so instead of
13:42 doing this manually for all the classes
13:44 what I can do is I can just remove this
13:46 Constructor which is generated
13:48 and I can add The annotation required
13:52 hugs Constructor from lombok so what
13:55 this will do is at the compile time it
13:56 will create the Constructor for us which
13:59 all the required Constructor arguments
14:00 right so this is something handy which
14:03 we have kids you can use so now I'm
14:04 going to save this product inside the
14:07 database by typing product repository
14:10 dot save product
14:14 so after doing that the product will be
14:16 saved successfully to the database we
14:18 can also add some blocks here
14:21 by making use of slf4j so I'm going to
14:24 type in slf4j so this is coming from
14:27 lombok
14:28 and to set to add the logs I'm just
14:31 going to create a new line under the
14:34 save method I'm just going to type in
14:36 log dot info
14:38 product
14:40 is saved right so we also have to give
14:43 the information which product is saved
14:44 right so maybe a product ID would be
14:47 useful so for that instead of
14:49 concatenating the product ID like this
14:53 red product dot get ID
14:56 something like this using slf4j we can
14:58 actually make use of a placeholder so
15:01 I'm just going to add a placeholder like
15:03 a double curly brace and after that I'm
15:06 just going to type in comma and add
15:08 product dot get ID
15:11 so what this will do is it will
15:12 dynamically it will get the ID of the
15:14 product and it will replace it on the
15:17 placeholder so when you execute this you
15:19 will get product followed by the product
15:21 ID is saved
15:23 right so we have the create product
15:25 method so now let's also create the so
15:28 now let's call this create product
15:29 method from the product controller so as
15:32 we did similarly for the product service
15:34 we also have to inject the product
15:35 service inside the product controller so
15:38 for that I am going to type in again
15:41 private final
15:43 product service
15:46 and to inject product service I am going
15:48 to make use of again the required
15:54 so now we have the product service I'm
15:56 just going to call inside the create
15:58 product method
16:01 productservice.createproduct and I'm
16:02 going to pass in the product request
16:06 so this will create the endpoint to
16:08 create the products so now I'm going to
16:10 create another endpoint to retrieve all
16:13 the products so for that I am going to
16:15 create another dto called as product
16:17 response
16:19 for that response
16:23 and
16:25 similar to what we did for the product
16:27 request so I'm just going to copy
16:30 everything we have inside all the
16:32 annotations we have defined for product
16:34 request
16:36 called Product response
16:39 and inside the product response we are
16:42 also going to send the ID
16:45 in addition to the name description and
16:47 price so for that I'm just going to copy
16:49 everything we have inside the
16:53 product model class so I'm just going to
16:55 type in product response so now you may
16:59 be confused why I have created a
17:01 separate class called as product
17:03 response right so maybe why can't we
17:05 directly use the product class from the
17:08 model package instead of creating the
17:10 product response this is because it's a
17:12 good practice to separate your model
17:15 entities and the dtos so ideally you
17:18 should not expose your model entities to
17:21 the outside world because if in the
17:23 future in this product class if I've
17:25 added some other two fields which are
17:28 like necessary for the business model we
17:31 should not expose these two fields which
17:33 are not necessary outside to the outside
17:35 world right so for this reason it's a
17:37 good practice to separate your model
17:39 entities and also your data transfer
17:42 objects so in this case the product
17:44 request and the product response acts as
17:46 the data transfer objects so now I've
17:48 created the product response so to
17:49 create the get endpoint so get all
17:52 products endpoint I'm going to first
17:54 type in a get mapping annotation
17:57 and this response status annotation is
18:01 going to send a response status of HTTP
18:05 status okay right and now I'm going to
18:08 type in public list of
18:10 product response
18:13 and I'm going to call it as get all
18:15 products
18:18 and I'm going to import this
18:21 list class from java.util package
18:24 and now I'm going to create a method
18:27 inside product service to get all the
18:29 products so I'm going to call in
18:30 productservice Dot
18:33 get all products
18:36 so I'm going to create this method as
18:38 this is not existing yet inside the
18:40 product service
18:41 so this get all products should return
18:45 a list of
18:48 product response
18:51 so
18:52 I'm also going to import this list from
18:55 java.utl package again quickly
18:57 and in here I'm going to type in product
18:59 repository dot find all because I want
19:02 to first read all the products inside
19:04 the database so I'm going to store it
19:07 inside a variable called as products
19:11 and now I have to map this products this
19:15 product class into a product response
19:17 class right so for that I'm going to
19:19 type in products dot stream dot map I'm
19:23 going to make use of java 8 streams and
19:26 with the map function and I'm going to
19:29 map each product into a
19:32 product response object so for that I am
19:34 going to create a new method called as
19:36 map to
19:38 product response
19:42 create a new method inside the product
19:44 service so this is not should not return
19:47 an object but should return a product
19:50 response right so here I'm going to
19:53 create
19:54 a object for product response using the
19:58 Builder method so I'm going to select
20:00 response dot Builder dot build
20:03 and as part of the Builder I'm going to
20:05 provide the ID as
20:08 um you know what we don't have the
20:10 product as an input argument so let me
20:13 add the product as the input argument
20:15 here
20:17 to the map to product response method so
20:21 now we have the product so I can just
20:23 map the ID of the product to the ID of
20:25 the product response by typing in
20:27 product.getid
20:29 and again name as product dot get name
20:33 so let me just scroll down
20:35 to make it visible clearly so again we
20:39 have the description so I'm going to
20:41 type in product Dot get a description
20:46 followed by the price so I'm going to
20:48 type in product dot get price
20:51 so now we are creating the object of
20:55 product response and returning it back
20:57 to the forget all products method and in
20:60 here IntelliJ is suggesting some changes
21:03 here so as we are using a method inside
21:06 the class we can actually make use of
21:09 the method reference instead of Lambda
21:11 so I'm just going to select replace
21:13 Lambda with method reference
21:15 and now after my after we mapped it we
21:20 are going to collect it into a list so
21:22 I'm going to call to list method and
21:26 finally I'm going to return this list
21:29 back to the controller so
21:32 we have the get all products which is
21:34 returning a list of product responses if
21:37 a list of product response objects so
21:39 now I'm going back to the product
21:40 controller and I'm going to add
21:43 a written statement here
21:46 all right so now we have both the create
21:49 product and the get all products
21:50 endpoint implemented so now let's start
21:54 the application and check whether the
21:55 application is going to start without
21:57 any errors or not
22:01 all right so you can see that we are
22:03 able to start the application on the
22:05 port 8080 without any problems so now
22:08 let's quickly go ahead and test whether
22:10 our application our rest API is working
22:13 correctly or not
22:14 so I'm going to open Pulse map so here I
22:18 prepared a post request for the URL HTTP
22:22 localhost aviate localhost 8080 slash
22:25 API slash product so this is going to be
22:28 a post request so for that I'm going to
22:30 provide a request body raw in the type
22:33 of raw Json
22:35 and here I am going to create
22:39 a request body name as
22:43 iPhone 13.
22:46 description
22:49 has iPhone 13.
22:52 and price
22:54 as maybe twelve hundred dollars right so
22:58 I'm going to send this request
22:60 and we can see that we've got two zero
23:02 one created response back from the our
23:05 application now I'm going to create I'm
23:08 going to copy this URL and I'm going to
23:10 create a new tab and paste in this URL
23:14 and now I'm going to send a get response
23:17 get request to your API and you can see
23:20 that we received the product which we
23:23 have created in the previous step
23:25 so our product service is working
23:26 correctly but we are doing this testing
23:29 manually the ideal situation would be to
23:31 have some automated tests in the project
23:33 so let's go ahead and write some
23:35 automatic tests for our product service
23:37 next so to be able to write tests for
23:40 our product service application we can
23:43 open the package Source test and under
23:46 Java we have the test product service
23:48 application test which is auto generated
23:50 by the spring initializer for us so
23:53 we're going to write the test in this
23:55 class but before going ahead and writing
23:57 the test we are mainly interested to
23:59 write integration tests for this product
24:00 service test right so we are not going
24:02 to write unit test but we are going to
24:04 create an integration test where we will
24:07 spin up the application context the
24:09 whole application and we will test
24:11 whether the post API and the get API is
24:13 working or not so before going ahead and
24:16 writing our tests I'm going to first
24:18 install a library called as test
24:20 containers called as test containers in
24:22 our project so if you are not aware of
24:24 what continuous Library this is a Java
24:28 library that supports us to write J unit
24:31 tests by providing some throw away
24:34 instances of common software we need
24:36 like databases web browsers for selenium
24:39 and
24:40 rabbitmq message queues and so on and so
24:44 forth whatever external infrastructure
24:46 we'll be relying on we can provide we
24:49 can use dot as external infrastructure
24:52 software as Docker containers and we can
24:54 run integration tests in our in our
24:57 project right so some examples are you
24:59 can test the data access layer
25:01 integration tests using the docker
25:04 instances of MySQL postgres or any other
25:06 database
25:07 and you can also use the docker
25:10 containers for message queues web
25:12 servers and also this can be used in the
25:15 UI and acceptance test using selenium so
25:18 it provides many different uh Docker
25:21 containers so that we can use this in
25:23 our tests so that's why we have the name
25:25 test containers so if you have a look at
25:27 the modules which are supported you can
25:29 see that it has different databases
25:32 elasticsearch container gcloud module
25:34 engines module Kafka containers so on
25:38 and so forth we have lot of modules
25:40 which are available as of now in our
25:42 unit in our in our tests in our
25:45 integration test we want to have a
25:47 container for mongodb so for that I'm
25:50 just going to click on the databases and
25:52 select the mongodb module and here you
25:55 can see the documentation like the usage
25:58 example how we can use this mongodb
26:00 container in our tests right so before
26:04 we go ahead and use this mongodb
26:06 container we have to first install test
26:08 continues in our project right so for
26:10 that I'm going to click on home
26:13 and I'm just going to scroll down until
26:15 I find the section maybe dependencies so
26:17 if you want to install the test
26:19 containers
26:20 individually you can just click on Maven
26:23 and copy the whole dependency so you can
26:26 use this particular dependency the other
26:28 thing you can do is you can have if you
26:31 want to have multiple test container
26:32 dependencies right we can if we want to
26:35 have one test container dependency for
26:37 mongodb one for Kafka and one for I
26:40 don't know any other module it's a good
26:42 thing to have a Bill of material
26:45 something called as a bill of materials
26:47 where you can just provide the version
26:48 as in one place and all the dependencies
26:52 all the other dependencies you use for
26:54 the test containers will take the same
26:56 version I'm just going to copy this
26:58 particular section and I'm going to open
27:00 my form.xml and I'm going to paste this
27:04 particular code about the build section
27:06 right inside the
27:09 dependency management section and I'm
27:11 just going to load the maven changes all
27:14 right so once this is done now I can
27:16 install the mongodb container in our
27:20 application in our project so for that
27:23 I'm just going to go back to the test
27:26 containers
27:27 website and I'm going to click on
27:29 modules
27:30 databases
27:32 and mongodb module
27:35 and here I'm just going to scroll down
27:37 until I find the adding this module to
27:39 your project dependencies section I'm
27:41 going to click on maven
27:43 and click on copy
27:45 to go back to my palm.xml and under the
27:48 springboard started test dependency I'm
27:50 just going to paste in
27:52 the mongodb test container dependency
27:54 and again I'm going to load the changes
27:57 so once this dependency is downloaded to
27:59 your machine I can remove the version
28:01 because as I'm already
28:04 I'm already defining the version inside
28:07 the dependencies management for the test
28:09 containers bill of materials I don't
28:11 need to explicitly create the version
28:13 one more time inside the dependencies
28:16 section right so I'm just going to again
28:18 reload the changes so in this way if I
28:21 want to no matter how many modules I use
28:24 for the test containers I can just
28:26 Define the version only in one place
28:28 here inside the test containers bill of
28:30 materials right so this is the advantage
28:32 of using the bill of materials inside
28:34 the dependency management section and
28:36 apart from this as we are using junit 5
28:39 to write the tests we also need to add
28:41 the junit 5 supporting module for test
28:44 containers so for that I am going to
28:46 open again the test containers website
28:49 documentation website and I am going to
28:51 click on the section test framework
28:53 integration and go to junit 5 and in
28:56 here all the way down in the adding test
28:59 containers to J unit 5
29:01 section I'm going to click on Maven and
29:03 I'm going to copy this particular junit
29:06 Jupiter dependency one more time
29:08 and going back to the form.xml I'm just
29:12 going to paste this below the mongodb
29:15 dependency and similarly I'm just also
29:18 going to remove the version
29:20 tag
29:21 from the J unit Jupiter
29:24 disc container dependency unless I'm
29:25 going to load this Maven changes so now
29:28 let's go ahead to the product service
29:30 application test class and start writing
29:33 the integration test for our product
29:35 service
29:36 so to get started I'm going to add the
29:39 Monger so to get started the first thing
29:41 I'm going to do is to add the test
29:44 containers annotation
29:47 on top of the glass so that jnit5
29:50 understands that we are going to use
29:52 test containers to run this particular
29:54 test and after that I have to define the
29:56 mongodb container inside our test right
29:59 so for that I can type in
30:02 mongodb container
30:05 and create an object for the class
30:07 mongodb container and to this mongodb
30:10 container you can see that this mongodb
30:13 container class is deprecated because
30:15 the NOAA Constructor we cannot use the
30:18 Nova Constructor anymore we have to
30:20 manually specify the version of the
30:22 mongodb we want to use
30:24 so for the augment Constructor we have
30:27 to provide the docker image name for
30:29 whatever software you want to use so I
30:32 am going to provide the docker image
30:33 name as and I'm going to provide
30:36 the tag as 4 Dot 4.2
30:41 right so this is the version I
30:43 want to use for this test so I'm going
30:46 to add another
30:48 annotation on top of this declaration
30:50 called as
30:52 container
30:54 so that jnit5 will understand that this
30:57 is a mongodb container and I'm also
30:60 going to add the static declaration
31:02 because I want to statically add
31:06 statically access this particular
31:08 mongodb container and fetch the URL of
31:11 the mongodb database URI right so
31:15 because as we are going to use this you
31:17 know we need to set the URI of this
31:19 mongodb container in our test properties
31:22 so let's do that by first adding a
31:25 method called as
31:27 static void set properties
31:32 and this method is going to take a
31:36 class an object of type Dynamic Property
31:39 registry
31:42 and inside this method I am going to
31:44 type in registry
31:45 Dynamic Property registry dot add
31:49 and I'm going to type in Spring dot dot
31:52 data dot mongodb dot URI
31:55 and I'm going to here passing for
31:58 passing the mongodb dot mongodb
32:02 container get
32:05 get replica set URL
32:07 variable
32:09 so in this way at the start a time of
32:12 starting the integration test first of
32:14 all the test will start the mongodb
32:16 container by taking by downloading the
32:19 4.4.2 image and then after
32:23 starting the container it will get the
32:25 replica set URL and add it to the spring
32:28 data mongodb URI property dynamically at
32:31 the time of creating the test right
32:33 because if you remember if you if you
32:36 have a look at our source main resources
32:38 folder we have defined the spring data
32:40 mongodb URI manually in the
32:43 application.properties file right but as
32:45 here we are doing an integration test we
32:48 also have to provide the spring data
32:50 mongodb URI dynamically at the time of
32:52 creating the test because as we are not
32:54 using the local database local mongodb
32:56 database we are using a mongodb Docker
32:59 container we have to provide the uh
33:02 spring data mongodb or a property
33:03 dynamically so to be able to provide
33:06 this property dynamically we also have
33:08 to add another annotation on top of this
33:10 set properties method called as Dynamic
33:14 Property source which will add this
33:17 property to our spring context
33:18 dynamically at the time of running the
33:20 test
33:21 right so after adding all these
33:24 configuration we can go ahead and start
33:26 writing our integration test so the
33:29 first test I am going to create is for
33:32 the product controller create product
33:34 endpoint right I want to send a send a
33:39 product object a product request object
33:41 and I expect the response status to be
33:44 created right so that's what I expect
33:47 Whenever I send a product request object
33:49 I expect the status to be created so
33:52 let's test this Behavior
33:53 I am going to open product service
33:55 application test and this I'm going to
33:58 change this context loads test to create
34:03 should create
34:05 product and to be able to make a request
34:08 from our test from our integration test
34:12 I'm going to make use of something
34:15 called as a mock and we see some object
34:17 called as mock MVC which will provide us
34:20 a mocked servlet environment where we
34:23 can call the product our controllers
34:25 where we can call our controller
34:27 endpoints and we can receive a response
34:30 as per the response which is defined
34:32 inside the controllers right if you want
34:34 a refresher on the testing concept of
34:36 spring boot testing Concepts you can
34:38 have a look at this springboard testing
34:40 tutorial crash course I have on my
34:42 YouTube channel it will provide a nice
34:44 overview about what is the what are all
34:47 the aspects you need to know to be able
34:49 to write to be able to write tests using
34:52 springboard right so for now I'm going
34:55 to make use of the mock MVC object to
34:59 make requests to the product controller
35:01 so for that I'm just going to
35:03 Auto wire
35:06 the mock MVC object
35:09 into our test
35:11 right and you can see that we are
35:14 getting an error could not Auto wire no
35:16 beans of mock MVC type font because we
35:19 have to Auto wire we have to Auto
35:21 configure the mock MVC in our test so
35:23 for that I'm just going to add
35:26 an annotation called as Auto configure
35:28 mock MBC
35:30 now you can see that the error is gone
35:31 that means our mock MBC is auto
35:34 configured and then now we can Auto wire
35:36 mock MVC in our test right so now I'm
35:39 going to make use of mock MVC object and
35:43 I'm going to type in mock MVC dot
35:46 perform this perform object this perform
35:48 method will take in a request Builder
35:52 and an object of type request Builder so
35:55 for that I am going to type in mock MBC
35:57 request Builders dot post as I want to
36:01 create as I want to do a post request I
36:03 am choosing the post method and in here
36:06 I am going to just type in slash API
36:09 slash product because this is the root
36:13 URL of the product controller I'm just
36:16 going to copy this particular URL
36:18 endpoint and provide it as the
36:21 provided as the URL template and after
36:23 that we also have to provide a request
36:25 body of type Json so for that I'm just
36:28 going to type in Dot
36:32 content type as media type application
36:36 Json and I am going to provide content
36:41 as a string right I want to provide the
36:43 content as a string so for that first we
36:46 have to create the request body of type
36:48 product request because as we are
36:51 receiving as we are taking a request
36:53 body of type product request first of
36:55 all I have to create an object of my
36:58 product request for that I am just going
37:00 to create a method called as get
37:03 product
37:05 request
37:07 I am going to create a method inside
37:10 the test class and in here I'm just
37:14 going to return
37:17 product request
37:20 dot Builder
37:22 dot name is going to be
37:25 iPhone 13.
37:28 and description is going to be also
37:31 iPhone 13.
37:34 and the price is going to be big decimal
37:38 dot value of
37:40 twelve hundred dollars maybe right and
37:42 I'm going to finally call the build
37:44 method to be able to construct an object
37:47 of type product request and let me also
37:50 just
37:53 format this correctly and also this
37:56 method should return an object of type
37:58 product request but not void all right
38:01 so now we have the product request
38:04 object ready so I'm just going to also
38:07 store this particular return type return
38:11 written object inside a local variable
38:13 called Product request and now I want to
38:16 send the product request object as the
38:18 request body but this content method
38:22 will only take the object of type string
38:26 so now what I have to do is I have to
38:28 convert this product request object into
38:31 a string right so for that I am going to
38:34 make use of something called as an
38:36 object mapper
38:39 private object mapper
38:43 from Jackson data bind package
38:46 so what this object mapper will do is it
38:48 will convert a pojo object to Json and
38:52 Json to Pooja object it will help us to
38:55 convert this particular objects to Json
38:58 and vice versa so I'm going to make use
39:00 of object mapper and I'm going to type
39:02 in object mapper dot write value as
39:07 string
39:08 and I'm just going to pass in the
39:10 product request
39:13 and this write value as string is going
39:15 to throw an exception so I'm just going
39:17 to add exception to the method signature
39:19 and finally I'm going to store this
39:21 written object into a local variable
39:24 called as product request
39:28 string I am going to provide this now
39:31 into the content
39:33 method
39:35 and finally I'm just going to also
39:38 Define another exception from the
39:40 perform Method All right so now we have
39:42 provided all the required input for
39:44 calling the create product API now the
39:46 next thing I want to do is I want to
39:48 verify whether the request is going to
39:51 be the response status is going to be
39:53 201 created or not right so for that I'm
39:56 just going to type enter and type and
39:60 expect
40:01 and I'm going to type in status
40:04 this is going to be a
40:07 static method which is coming from mock
40:10 MVC result matches I'm going to select
40:13 I'm going to import the mock MEC results
40:15 matchers dot status method and after the
40:19 status method I am going to type in is
40:21 created right so in this way mock MVC
40:25 will make a call to the API product
40:28 create product endpoint by providing the
40:31 product request string as a request
40:33 object and it will expect whether the
40:35 status is created or not
40:38 right so let's go ahead and run this
40:41 particular test whether it's working
40:42 correctly or not I'm just going to run
40:45 the product application Service Test Now
40:48 what you're going to observe your very
40:50 first time here is
40:52 the test containers is going to if it's
40:54 not already available it's going to pull
40:56 the mongodb image
40:59 from the docker container registry right
41:02 so it will take some time as it needs to
41:06 First pull the docker image and then
41:08 start running the test and here you can
41:11 see that the test has successfully
41:12 created the mongodb container and then
41:15 started running the test against this
41:17 mongodb container
41:19 right so if we go to the should create
41:20 product you can see that it has
41:22 triggered the product service product
41:24 with this particular ID is saved to the
41:26 database and let's do one verification
41:29 whether the data is really saved or not
41:32 I'm just going to Auto wire the
41:37 product Repository
41:39 right and I'm going to verify
41:46 product Repository
41:48 plot
41:50 find all
41:52 dot size is going to be 1 right because
41:56 we have inserted one product we expect
41:59 that we should have one product inside
42:02 the repository so for that I'm just
42:04 going to type in assertions
42:09 dot asset
42:12 true
42:15 and I'm going to
42:17 add this product repository find all
42:19 size method call inside the asset true
42:23 method and just I'm going to verify
42:25 whether the size
42:28 is going to be 1.
42:31 right so after making sure that the
42:35 create product endpoint is triggered
42:37 successfully we are also making another
42:40 assertion that the product repository
42:42 contains the product or not right and
42:45 here you can see that the IntelliJ is
42:48 suggesting us to we can simplify this
42:50 assertion I'm just going to use assert
42:54 equals here so in this way we can have
42:56 the expected value and the actual value
42:59 as the second parameter so now let's
43:01 make sure that we run the test again to
43:03 verify whether everything is working
43:05 fine yes indeed so the test is working
43:07 fine so in this way we can have some
43:09 automated tests in our microservice so
43:13 that we are not always relying on the
43:15 manual test we can also have some
43:17 automated tests in our microservice so
43:20 I'm going to give an exercise or a
43:21 homework to implement also the test for
43:24 the get product endpoint I have created
43:27 the test for the create product now it's
43:29 your turn to implement the integration
43:31 test for the get product as this is
43:33 going to be a micro service project a
43:36 micro service course I'm not going to go
43:38 in depth about the testing part I've
43:40 just provided you a hint on how to write
43:43 the integration test for the micro
43:45 Services now you can go ahead and
43:47 implement the required test for the
43:49 other endpoints by yourself I'm also not
43:51 going to implement the test for the
43:53 other so services like order service and
43:56 the inventory service but if you are
43:58 struck when writing the test you can
43:60 find the test inside the GitHub
44:01 repository so I'm going to give an
44:03 exercise or a homework to implement also
44:06 the test for the get product endpoint I
44:09 have created the test for the create
44:11 product now it's your turn to implement
44:13 the integration test for the get product
44:15 as this is going to be a micro service
44:17 project a micro service course I'm not
44:20 going to go in depth about the testing
44:22 part I've just provided you a hint on
44:26 how to write the integration test for
44:27 the micro Services now you can go ahead
44:29 and implement the required test for the
44:32 other endpoints by yourself I'm also not
44:34 going to implement the test for the
44:36 other services like order service and
44:38 the inventory service but if you are
44:41 struck when writing the test you can
44:43 find the test inside the GitHub
44:45 repository so you can refer whenever you
44:47 want and whenever you are stuck by
44:50 writing the integration test all right
44:52 so now it's time to create our second
44:53 microservice that is the order service
44:56 for that I've opened the website
44:58 star.spring.io and here I'm going to go
45:01 ahead and create another micro service
45:03 our order service so I'm going to type
45:06 in inside the group ID com programming
45:10 techie and artifact is going to be
45:12 ordered service and the name is also
45:14 going to be order service and I am going
45:16 to select Java 17 15 as the latest
45:19 stable Java version and coming to the
45:21 dependencies I am going to add the
45:24 dependency for lombok
45:26 and also for spring web and as we are
45:30 going to use mySQL database for the
45:33 order service I am going to scroll or
45:36 scroll down and add the dependency for
45:39 spring data jpa
45:41 and lastly I'm also going to add the
45:44 dependency for
45:46 MySQL jdbc driver
45:48 once you have all these dependencies you
45:51 can just click on the generate Button as
45:54 we are going to create more and more
45:56 services we have to maintain these
45:58 Services we have to run this Services
46:00 parallely by opening multiple triple ID
46:03 windows so that will create a lot of
46:05 load on your machine right like so to
46:09 get around that what we can do is we can
46:11 maintain all these Services inside one
46:14 inside one project so for that I'm just
46:16 going to create a new folder or less
46:19 micro Services parent something like
46:22 this and I'm going to move this product
46:24 service and as well as the order service
46:27 into the parent project so in this way I
46:31 just need to open the micro Services
46:33 project
46:35 microservices parent folder inside
46:37 IntelliJ and I will have both these
46:40 projects inside one ID all right so now
46:42 I opened IntelliJ and I'm going to click
46:44 on open and
46:46 select the microservices parent folder
46:48 which I've created before click on OK
46:51 and now you can see that we
46:54 are able to access both the product
46:56 service and as well as the order service
46:59 inside one folder so now let's open the
47:02 order service and let's have a look at
47:04 the palm.xml we have the spring dub wood
47:06 starter data jpa and spring boot starter
47:09 web and two dependencies for the and the
47:12 dependency for the MySQL jdbc driver
47:15 long walk and the starter test
47:18 dependency right so now first what I'm
47:21 going to do is I'm going to create a
47:23 package called as controller
47:27 and I'm also going to create some other
47:30 packages so that will have this
47:33 structure called as controller
47:35 service
47:37 repository and the dto right like what
47:39 we how we did for the product service in
47:43 this way right so let me create the
47:45 package for
47:48 Repository
47:51 another package for
47:53 dto
47:56 and lastly we need another package
47:60 for model
48:02 right and for the order service the
48:06 model we are going to maintain in our
48:08 database now mySQL database is going to
48:11 be to store the orders so for that I'm
48:14 just going to create a new class inside
48:16 model package called as order and this
48:21 class is going to be a jpa entity so for
48:24 that I'm just going to add the entity
48:26 annotation from java X persistence
48:28 package and I'm also going to name this
48:32 uh table as
48:36 T underscore orders
48:38 and I'm going to add a getter
48:41 and Setter annotation followed by no Ox
48:45 Constructor and Alex Constructor for the
48:48 order class and coming to the fields
48:50 inside the order class I'm going to add
48:53 a primary key as ID with data type as
48:58 long and I'm going to add the ID
49:01 annotation from jpa and also the
49:04 generated
49:06 value annotation with strategy as
49:09 generation type Dot
49:12 Auto
49:14 and I'm going to also create a string
49:18 variable and I'm also going to create a
49:21 field called as order number
49:25 and followed by I'm going to create
49:29 a list of order line items right so each
49:32 order contains some items inside the
49:36 order so each each order contains some
49:38 Adder line items so I'm going to create
49:40 the order line items list and we have to
49:44 also create this order line items object
49:46 class we have to create this order line
49:49 items class
49:50 inside the model package so I'm just
49:52 going to create this class inside the
49:56 model package
50:04 and this order line items is going to
50:07 again have an entity
50:10 annotation from jpa this is going to
50:12 also be a table so I'm just going to add
50:15 the table name as order
50:19 T underscore order underscore line
50:21 underscore items so similarly to order
50:25 class I'm just going to add the getter
50:28 setter
50:30 hollocks and the no Ox Constructor from
50:33 lombok
50:34 and I'm going to also add the ID of type
50:38 long and I'm going to add the ID
50:41 annotation from jpa followed by the
50:44 generated
50:46 generated value annotation with
50:50 strategy as generation type identity and
50:53 the next field I'm going to create is
50:56 going to be order number of type string
50:59 and after that we are going to have a
51:02 list of order items inside the order so
51:04 for that I'm just going to create a list
51:07 of order line items
51:11 I'm going to call this as order line
51:12 items list
51:14 I'm just going to import this list class
51:17 from
51:19 java.utilpackage and we have this order
51:21 line items
51:25 and let me create this order line items
51:27 class inside the model package so this
51:30 order line items class is again going to
51:32 have the The Entity annotation from jpa
51:36 the table annotation and I'm going to
51:40 name this table as T underscore order
51:44 underscore line underscore items and I'm
51:47 going to add the getter annotation and
51:50 Setter annotation from one book followed
51:52 by Alex Constructor and the novox
51:55 Constructor so inside the order line
51:57 items I'm going to again have our
51:59 primary key called ID which is of type
52:02 long
52:04 and I'm going to add the ID annotation
52:07 from jpa as this is a primary key and
52:10 I'm going to add the generated value
52:12 annotation with the strategy as
52:15 generation type dot identity because
52:18 then it will be as this will be Auto
52:20 incremented the primary key for each
52:24 valve for each new record which we are
52:26 saving to the database and next in the
52:28 order line items we are going to have we
52:30 are going to maintain the skew code so
52:33 I'm going to type in the field of type
52:36 string called skew code
52:39 followed by the
52:42 field price which is of type big decimal
52:45 and lastly we will maintain the field
52:48 called as quantity
52:51 which is of type integer we have our
52:54 basic model created but we are we did
52:57 not defined any relationship between the
52:60 order table and the order entity and the
53:02 order line items entity as this is going
53:05 to be as the order and the order line
53:08 items is going to have a one-to-many
53:11 relationships I'm just going to add 1 to
53:14 many annotation which Cascade as Cascade
53:18 type or all so now we are done with the
53:21 model part so let's go ahead and create
53:23 the controller and the end point which
53:27 will help us to take create orders
53:29 inside the order service so for that I'm
53:31 just going to create a class called as
53:33 order controller
53:36 I am going to add the rest controller
53:38 annotation on top of this class and
53:41 inside the request mapping this is going
53:43 to take the value slash API
53:45 slash order and inside the class I'm
53:49 going to create a post mapping and I'm
53:52 going to call the method as
53:55 calls public string
53:58 place order and this method is going to
54:02 just return a string called as order
54:05 placed successfully by default and this
54:09 place order method is going to take a
54:11 request body
54:13 of type order request
54:17 and we don't have this order request
54:20 class created yet so I'm just going to
54:22 create this class called as order
54:24 request inside the dto package
54:27 and here I'm going to add a
54:31 data annotation so that we will have the
54:34 Getters insiders and all the
54:37 default boilerplate methods for the
54:41 class and I'm also going to add the
54:43 allux Constructor followed by the no Ox
54:46 Constructor and this order request is
54:49 going to have a list of order line items
54:54 dto class so this is going to be order
54:57 line items dto and this order line items
55:01 dto is still not existing in our we have
55:04 to create this order line item CTO so
55:06 let's go ahead and create this order
55:08 line items CTO class quickly inside the
55:10 dto package and I'm going to just
55:12 quickly copy and paste the whole
55:15 annotations from the order request class
55:18 and paste it on top of the order line
55:20 item Studio class and for the order line
55:23 items dtbo I'm just going to open the
55:26 order line items class and just I'm
55:28 going to copy everything there is inside
55:30 the orderline items class and paste it
55:34 inside the order line item Studio
55:36 because we'll basically have the same
55:38 Fields inside the online items that you
55:41 also and coming back to the order
55:43 request I have to fix the compilation
55:46 Errors By first importing the list class
55:48 from java util package
55:51 into the order request class and now the
55:53 next thing is to create a service so
55:57 that it will create the orders inside
55:59 the database so for that I'm just going
56:02 to create a new class inside the service
56:04 called as order service
56:06 and this order service is going to have
56:09 a service annotation on top of the class
56:12 and in this class I'm going to first
56:15 create I'm going to create a method
56:18 called as place order
56:21 and this place order method is going to
56:24 take a take the order request which is
56:27 coming in from the from the controller
56:30 and from this order request I now have
56:32 to create an object of type order so for
56:35 that I'm just going to create new order
56:38 object of type order and I'm going to
56:40 say order Dot set order number
56:45 as uuid dot random uuid I'm going to
56:48 provide some random uuid as the order
56:50 number and now I have to map the order
56:53 line items which is coming in from the
56:56 order request as orderline items tto to
56:59 the order line items inside the right so
57:02 for that I'm just going to say order
57:04 request dot get orderline items dot
57:08 stream and I'm going to call the map
57:12 function so that I can map the object of
57:15 order line item cto2 order line items so
57:18 I am going to type in here order line
57:21 items dto and I'm going to map to create
57:26 a method called as map to dto and
57:28 provide the order line item CTO as the
57:31 method parameter and I'm going to
57:32 quickly create this method called as map
57:35 to dto inside the order service and
57:37 instead of returning an object from this
57:40 method I am going to return I am going
57:42 to return an object of type order line I
57:45 items and inside this method I'm going
57:47 to create an object of type order line
57:50 items
57:51 and I'm going to just map all the fields
57:53 which we have
58:06 right and finally I'm just going to
58:08 return the order line items back to the
58:13 place order class and inside this map
58:17 method you can see that we can replace
58:19 this Lambda with a method reference so
58:22 I'm just going to replace it with a
58:23 method reference and I'm going to
58:25 convert this to a list of order line
58:28 items by using the dot to list terminal
58:32 operator I am going to introduce a new
58:35 local variable called as order line
58:36 items and finally I am going to set this
58:40 order line items to the order by typing
58:42 order dot set order line items
58:44 and providing outline items and now we
58:48 have constructed an object of type order
58:51 what we have to do is to save this order
58:54 inside the database so to save this
58:56 order to the database we need to create
58:58 a repository for that I'm going to right
59:00 click and go to new class and I'm going
59:03 to create a new interface called as
59:06 order repository and this order
59:08 repository is going to extend
59:11 jpa Repository
59:15 of type order and the primary key is
59:19 going to be of type wrong
59:22 right so now once we have the order
59:25 repository I can just inject this order
59:28 repository into into the order service
59:30 so I'm just going to type in private
59:33 final order Repository
59:37 and as this is going to be a Constructor
59:40 injection I have to create a Constructor
59:42 parameter for that instead of creating
59:45 the Constructor manually I can add the
59:48 required args Constructor annotation
59:51 from lombok so then it will create a
59:54 Constructor automatically based on the
59:57 parameters during the compile time and
59:59 I'm going to use this Auto repository
60:01 inside the place order method and I'm
60:04 going to type order repository dot save
60:06 right so in this way we are receiving
60:09 the order request from the from the
60:13 client to the controller and the
60:14 controller is passing the order request
60:16 to the order service and inside the
60:18 order service we are mapping this order
60:21 request to an order object and finally
60:24 saving this order into the order
60:26 repository so now let's go back to the
60:28 order controller class and now we have
60:30 to call this order service from the
60:31 order controller right so for that I'm
60:34 just going to add the inject this
60:36 Hardware service by typing private final
60:39 order service
60:42 and I'm going to also add the required
60:45 RX Constructor so that will have a
60:47 Constructor parameter for order service
60:50 and inside the place order method of the
60:53 controller I'm going to type in order
60:54 service dot place order
60:56 order request
60:58 right so whenever the user places the
61:01 order it will be saved to the database
61:03 and it will return the text called as
61:06 order placed successfully and I forgot
61:09 to add the response
61:11 status and the response status is going
61:14 to be
61:15 HTTP status dot created so let's test
61:20 whether this is working or not so I'm
61:22 just going to open order service
61:23 application and I'm going to run this
61:27 order service application and you can
61:29 see that we are
61:32 yeah we are getting an error because of
61:34 course we did not Define the data source
61:37 attributes inside the
61:38 application.properties we did not Define
61:40 the database URL and all the related
61:43 properties inside the inside the
61:45 application.properties file so I'm going
61:48 to do that now so inside the
61:50 application.properties file I'm just
61:52 going to type in Spring data source dot
61:55 driver class name this is going to be
61:58 com dot MySQL
62:00 dot jdbc dot driver right and
62:06 spring.datorsource Dot URL is going to
62:09 be my jdbc URL so just I'm going to type
62:11 in jdbc MySQL
62:14 localhost
62:16 3306 this is going to be my port on
62:19 which local MySQL server is running and
62:22 I'm going to
62:23 call the schema as order service right
62:27 and I'm going to type in
62:29 spraying.datorsource dot username as
62:32 root
62:34 displaying.datorsource dot password as
62:37 MySQL
62:39 jpa dot hibernate.ddl Auto is going to
62:44 be update
62:46 and Screen dot data source and lastly
62:49 I'm also going to add the server.port as
62:53 8081 as on 8080 we are running the
62:57 product service we want to run the order
62:59 service on Port 8081 right so let's go
63:03 ahead and start the application now and
63:05 you can see that the application is
63:06 started successfully but we see a
63:09 warning that the class MySQL jdbc driver
63:13 is deprecated and the new class is com
63:15 MySQL CJ jdbc driver so I'm going to use
63:19 this driver instead of the other one so
63:23 I'm just going to paste this inside the
63:26 application properties and I'm just
63:28 going to restart the application one
63:30 more time and now you can see that the
63:32 application is started on Port 88081 so
63:37 now it's time to test whether the
63:40 test whether the end point is working
63:42 correctly or not so I'm just going to
63:44 open the postman client I'm going to
63:47 open the postman client and then now I'm
63:49 going to type http
63:51 localhost
63:53 8081 slash API slash order and I'm going
63:58 to send a request body of type Json and
64:02 in this request body I am going to type
64:04 the and in this request body I am going
64:08 to
64:09 send the fields for the order request so
64:11 it's going to be an array of order line
64:14 items dto right so I'm just going to
64:17 create
64:18 an order line
64:21 items DTU array and inside that array
64:25 I'm going to first
64:27 send the skew code price and quantity
64:32 right and instead of Auto line items DDO
64:35 it should be order line items dto list
64:37 the name of the key value of the Json
64:41 it's going to be outline items list and
64:43 inside the object I am going to create
64:46 SQ code price and quantity right so I'm
64:50 just going to create an object for skew
64:53 code
64:55 and an object for price
65:01 and it's going to be 1200
65:04 and
65:06 quantity is going to be 1 right and for
65:11 the skew code and for this Q code I'm
65:13 just going to type in iPhone underscore
65:15 13 and I'm going to just click on
65:20 13.
65:21 and instead of and I've made a mistake
65:24 here it should be localhost 8081 but not
65:28 8082 and also I have to send a post
65:32 request to the order service so now with
65:36 this request body if I click on send I
65:39 should receive a success response order
65:42 placed successfully with status as 201
65:44 created that's perfect now the next
65:48 thing we have to do is to create the
65:50 inventory service which will check
65:51 whether the products are in invent are
65:54 whether the products are in stock or not
65:57 right so let's go ahead and do that all
65:59 right so now I'm in the star.spring.io
66:02 website and it's similar to the other
66:05 two Services I'm just going to leave the
66:07 defaults for the spring boot options and
66:10 also the packaging and the Java version
66:12 I'm going to add it as 17. the only
66:15 change I'm going to do is to change the
66:17 artifact name to inventory service
66:21 and I'm going to add the dependencies
66:23 similarly to order service I'm going to
66:26 add the dependency for lombok dependency
66:28 for spring web
66:30 dependency for spring data GPA I'm just
66:34 going to scroll down until I find Spring
66:36 data jpa
66:38 and the dependency for MySQL you know
66:41 what I am scrolling down and you can
66:43 just search for my SQL driver here so I
66:46 can just select my SQL driver and once
66:49 everything is done I'm going to click on
66:50 generate all right all right so open the
66:53 inventory service project inside the
66:55 your favorite IDE and after after that
66:59 I'm just going to open source main
67:01 resources and the first thing I'm going
67:03 to do is to configure the mySQL database
67:06 properties inside the
67:08 application.properties file so for that
67:11 I'm just going to leverage on the
67:13 changes we did for the order service so
67:15 I'm just going to open the order service
67:17 and just copy every all the properties
67:20 which you have defined inside the
67:22 application.properties
67:24 and I'm going to paste it inside the
67:28 application.properties of the inventory
67:30 service so one question you may have is
67:32 why did I not add the inventory service
67:35 already inside the micro services
67:38 you know microservices parent right
67:40 because at this moment I am facing some
67:42 issue to add the inventory service to
67:45 the micro Services parent but we are
67:47 going to add them to one project by
67:51 creating Maven multi modules further
67:53 down the project so you can just
67:55 continue by opening the inventory
67:58 service in a new IDE and we'll do the
68:01 refactoring later right and the next
68:04 thing I'm going to do is to change the
68:06 server port to 8082 because 8080 is
68:10 taken by the product service 8081 by the
68:13 inventories by the order service and
68:15 8082 will be assigned to the inventory
68:18 service
68:20 so now the next thing I'm going to do is
68:23 to create a package inside the root
68:26 package called as model and inside this
68:29 package I am going to create a new class
68:32 called as inventory
68:35 and this inventory class will acts as
68:37 the the model where the actual inventory
68:40 which we are storing in the database so
68:42 for this reason we are going to add the
68:44 entity annotation from jpa
68:47 and we're going to add the table
68:49 annotation also with the name as T
68:52 underscore inventory and we're going to
68:54 add the getter annotation
68:56 the setter annotation locks Constructor
68:60 no aux Constructor annotation
69:03 and
69:05 inside the inventory class I'm going to
69:08 add
69:10 a primary key called ID with ID
69:13 annotation from jpa and generated value
69:16 annotation with strategy as generated
69:19 value type dot identity
69:22 and after that I'm going to add the
69:25 field called as skew code which
69:28 represent the product and lastly I'm
69:31 going to add another field called
69:34 quantity
69:36 which will provide the number of items
69:39 which are in stock for this particular
69:40 skip code
69:42 right so the next thing I'm going to do
69:44 is to create the repository for this
69:46 corresponding
69:48 jpa entity so for that I'm just going to
69:51 create the repository package and create
69:54 an interface
69:56 called as inventory
69:58 repository and this inventory repository
70:01 is going to extend jpa
70:05 repository because as we are using
70:08 spring data jpa and spring data jpa to
70:12 store for the persistence and for the
70:16 JPI repository we have to provide the
70:18 inventory as the first generic argument
70:21 as the jpa repository is going to take
70:23 two generic arguments the first one is
70:26 going to be the type of object which
70:27 you're going to store and the second one
70:29 is the type of the primary key and here
70:33 the type of the primary key is going to
70:34 be long so I'm going to add the type as
70:38 long so the next thing is to create the
70:41 controller where we can
70:45 create our endpoints right so I'm just
70:48 going to create the controller package
70:49 and create the inventory
70:53 controller class
70:55 and similar to every similar to all the
70:58 controllers we have created I'm going to
70:60 add the rest controller annotation
71:03 and request mapping annotation with
71:05 value AS Slash API slash inventory
71:10 and in here in this inventory controller
71:13 class I am going to have only one method
71:16 and this method is going to be a get
71:19 mapping
71:20 so we are going to make a get call to
71:23 get the inventory status and this
71:26 response status is going to be
71:28 http
71:30 status
71:33 dot okay
71:36 and the method
71:39 and the method name is going to be
71:41 public
71:43 Boolean
71:44 is in stock
71:47 right so what this particular end bond
71:50 is going to do is it's going to take in
71:51 a skew code and it's going to verify
71:54 whether the skew code or against the
71:56 product is in stock or not right so for
71:59 that we have to take in the skew code as
72:02 a request parameter or maybe a path
72:04 variable right so for that first have to
72:06 what you have to do is I have to add the
72:08 path variable to the get mapping
72:10 annotation so I'm just going to type in
72:12 slash
72:14 and
72:15 type skew code
72:17 and I'm also going to add the path
72:19 variable annotation here
72:21 it's going to be string skew code and as
72:25 this is not going to be as part of this
72:27 Nami convention so I'm just going to add
72:29 the path variable as skew code so so now
72:32 inside the agent stock method I have to
72:34 query the database to check the
72:36 inventory so far we are not going to
72:38 access the inventory repository directly
72:40 we are going to create a service class
72:42 so for that I'm going to create a
72:44 package called as service and inside the
72:47 service I'm just going to create class
72:49 called as inventory service
72:53 and this inventory service is going to
72:55 have an annotation called as service
72:57 because this is representing the spring
72:59 boot service right so inside this
73:01 inventory service class I'm going to
73:03 create a method called as public Boolean
73:06 is in stock
73:09 and this is going to take in the string
73:12 SKU code
73:14 as an input parameter and inside this is
73:18 Institute code is in stock method I'm
73:20 going to query the inventory repository
73:22 right so for that I need to inject the
73:24 inventory repository into the inventory
73:26 service where private I'm going to type
73:29 in
73:30 private final inventory Repository
73:34 and I'm going to
73:36 add the Constructor parameter by typing
73:38 the required arcs Constructor annotation
73:40 from Chromebook and inside the ease in
73:43 stock method I'm going to type in
73:45 inventory repository dot find by and I
73:50 don't have any method which defined by
73:52 Extinction so I'm just going to type in
73:54 find by capital S skew and capital c
73:57 code
73:59 so this will going to be an extension
74:01 method from Spring data jpa so I'm going
74:04 to create this method when the inventory
74:07 repository so spring data jpa is going
74:09 to automatically implement the logic for
74:13 this find by skew method so at the time
74:15 of at the runtime spring data jpa will
74:18 to generate the query to retrieve the
74:21 inventory by this Q code right so this
74:24 find best Q code method should not
74:26 return a type of void but it should
74:28 return an optional
74:30 of type
74:32 inventory
74:33 right so I'm just going to import the
74:35 optional class from java util package
74:38 and let's go back to the inventory
74:40 service
74:42 and here I'm just going to return
74:46 this
74:48 optional Dot
74:51 is present value right as the find base
74:55 Q code method is returning an optional I
74:57 can just type in dot is present to check
74:59 whether the object is present inside the
75:02 optional or not and I can just return
75:03 the value from the ease in stock method
75:07 directly and lastly I'm going to add The
75:10 transactional annotation
75:12 on top of this method and this
75:15 transactional is going to be read only
75:18 transactional transactional annotation
75:20 from Spring framework and I'm going to
75:23 add the read-only attribute as true well
75:26 I think we have forgot to add the
75:29 transaction annotation on the order
75:31 service so let me check once so I'm
75:34 going to open source main Java service
75:37 order service and yeah we didn't add The
75:41 transactional annotation right so I'm
75:43 just going to add The transactional
75:44 annotation on top of the class
75:47 by type in transactional from Spring
75:50 framework transaction dot annotation
75:52 package right so in This Way Spring
75:54 framework will automatically create and
75:56 commit the transactions once this is
75:58 done I'm going to go open the inventory
76:01 controller class and I'm going to now
76:03 call the inventory service dot is in
76:06 stock method from the inventory
76:08 controller so for that I'm just going to
76:10 type in inside the inventory controller
76:12 private final inventory service
76:18 and I'm going to add the required hugs
76:21 Constructor from lombok and here I'm
76:25 going to type in return
76:28 inventoryservice.he is in stock and I'm
76:30 going to pass in the skew code so in
76:32 this way we have created an endpoint
76:34 which takes in SQ code as the path
76:36 variable and
76:38 queries the database based on this Q
76:40 code and will return the response
76:42 whether the the inventory is whether the
76:46 product is in stock and one last thing
76:48 we have to do is to we have to create
76:50 the data inside the database right so if
76:52 we try to run the application and then
76:54 if we try to query this particular
76:57 endpoint of course it will return false
77:00 by default because there is no data
77:02 inside the database right so for that
77:04 what I can do is I can create
77:06 a bin inside the inventory service
77:09 application class so so what this will
77:13 be and we'll do is it will try to load
77:15 the data at the time of application
77:17 startup so for that I'm just going to
77:20 type in uh Bean definition so I'm just
77:23 going to add the green annotation
77:25 followed by the method public command
77:28 line Runner load data I'm going to name
77:31 this method as load data and this being
77:34 is going to take the inventory
77:36 repository as a method method parameter
77:39 so this will be injected through the
77:42 method injection and inside this load
77:45 data method we are going to create a
77:48 supplier and inside the supplier I am
77:51 going to first create an object for the
77:54 inventory
77:56 class so I'm going to first set the
77:58 inventories Q code AS iPhone underscore
78:00 13 and the quantity as 100 so I'm going
78:04 to now create another inventory object
78:07 called as inventory one so here the skew
78:11 code is going to be iPhone 13 red and
78:14 for this inventory I'm going to set the
78:15 quantity as 0. right so after defining
78:19 these two objects I am going to save
78:21 this two objects into the database so
78:24 for that I am going to type in inventory
78:26 repository dot save inventory and the
78:30 for the inventory one object I'm also
78:32 going to this the same thing inventory
78:34 repository dot save inventory one so at
78:37 the time of application startup this pin
78:39 will be created and as part of this bin
78:41 we are going to create both these
78:44 objects and save to the database so
78:47 before we test whether this is working
78:48 correctly or not I am just going to open
78:50 MySQL workbench and I'm going to first
78:54 create
78:55 a schema called as
78:58 inventory
78:59 service I'm going to click on apply
79:03 and also apply this will create the
79:05 schema inventory service and now let's
79:08 go to the inventory service project
79:11 inside IntelliJ and start our inventory
79:15 service application so you can open
79:16 inventory service application and just
79:18 click on this debug option
79:21 and at the time of startup it should
79:24 execute this particular piece of code
79:26 where it will
79:28 save the inventories to the table to the
79:30 repository inventory objects to the
79:33 repository so let's check whether this
79:35 is executed correctly or not yes it does
79:38 so we have two objects iPhone 13 and
79:42 iPhone 13 red inside the database all
79:45 right so before I've said that instead
79:47 of maintaining these two projects
79:49 inventory service and micro Services
79:51 parent
79:53 we're going to Club this everything into
79:55 one project we are going to create one
79:58 parent project and we are going to
79:59 maintain all our services into that main
80:01 to that parent project right so let's
80:05 try to do that now
80:07 so what I'm going to do now is I'm going
80:09 to create a new moving project
80:11 completely new Maven project inside
80:14 IntelliJ so for that I'm going to go to
80:17 file new project
80:20 and I'm going to select the option Maven
80:22 and make sure to select the version you
80:24 like I'm going to select jdk 17 here and
80:28 I'm going to click on next
80:30 and here I am going to provide a name
80:34 called as micro Services New so this is
80:39 the project name I'm going to give it's
80:41 like a new parent and in the artifact
80:43 coordinates I am going to name the group
80:46 ID as
80:49 com dot programming techie
80:53 and the artifact ID I'm going to leave
80:55 it as micro Services New and I'm going
80:57 to click on finish and I will going to
81:01 click on new window it will ask whether
81:04 to open the project in this window or in
81:06 a new window I will click on new window
81:10 and now you can see that it created an
81:13 empty Maven project
81:15 which contains just an empty Source main
81:18 Java folders resources and the test
81:20 resource folder and the test folder
81:23 and it's just a complete empty Maven
81:26 project right so what I'm going to do
81:29 now is to I am going to create three
81:31 modules one for the product service one
81:33 for order service and the other one for
81:35 the inventory service so inside this
81:37 main project I'm going to create a maven
81:39 module so for that I'm going to right
81:41 click and go to new Maven module and
81:45 again select the SDK as jdk17 click on
81:49 next
81:50 and I'm going to name this module as
81:52 product service and I'm going to check
81:54 whether the artifact coordinates are
81:56 similar to the group ID we have provided
81:59 for the root project
82:00 s and the artifact ID is going to be
82:03 product service click on finish and this
82:07 product service module is now created
82:09 again with the pom.xml file you can see
82:12 that the artifact ID is product service
82:15 and if I open the form XML for
82:18 microservices new root project it's also
82:21 added inside the module section the
82:23 product service
82:24 so similarly I'm going to create a
82:27 module also for the order service and
82:29 the inventory service
82:43 all right so now we have all these three
82:46 modules inside our root project so as we
82:49 have these modules we don't need the
82:51 source project source folder anymore so
82:54 I'm just going to delete this
82:56 Source folder
82:58 and now what I'm going to do is and now
83:00 I'm going to migrate every all these
83:02 three projects we have before inside
83:05 this microservice parent and in the
83:07 inventory service I'm going to migrate
83:09 this into the micro Services new project
83:12 first thing I'm going to do is to open
83:15 the is to open the pom.xml of the
83:20 product service so inside the micro
83:22 Services parent I'm going to open the
83:25 form.xml of the
83:27 product service and I'm going to copy
83:30 everything
83:32 copy everything under the dependencies
83:35 section so I'm going to copy everything
83:38 which is inside the dependence section
83:40 so all the way down here I'm going to
83:43 copy the dependencies tag and I'm going
83:46 to paste it inside the form XML of the
83:49 product service so just beside the
83:51 properties just below the properties I
83:53 am going to type in the I'm going to
83:55 paste in the dependencies section
83:57 right
83:59 so after adding this dependencies
84:01 section so now what I'm going to do is
84:04 I'm going to do the similar thing also
84:06 for the order service so go back to the
84:09 micro Services parent
84:10 open the pom XML of the order service
84:14 and again copy the dependencies
84:20 inside the order service and go back to
84:24 the order service inside the micro
84:26 Services New
84:27 and paste them just below the property
84:30 stack
84:32 all right so now we have the
84:34 dependencies for the order service let's
84:36 do the same thing also for the inventory
84:38 service I'm going to open the inventory
84:40 service
84:41 form.xml and copy everything inside the
84:46 dependencies right so copy it and open
84:50 the inventory service inside the
84:52 microservices new folder and again
84:54 similar to the other two projects I'm
84:57 going to paste in the dependencies just
84:60 below the property stack so now we have
85:02 the dependencies in all three projects
85:05 now what we need is the inside the
85:08 product service
85:10 of the micro Services parent I'm going
85:13 back right
85:15 so I'm switching a lot between these two
85:17 projects I hope you are understanding so
85:19 I'm now inside the product service of
85:21 the micro Services parent
85:23 and in here we have the dependency
85:25 management tag right so this is where we
85:28 are defining the version of the
85:30 dependencies for the test containers so
85:33 I'm going to copy this dependency
85:34 management tag
85:36 and I am going to add it inside the root
85:39 folder I'm not going to add it inside
85:41 the product service
85:43 but I'm going to add it inside the root
85:46 of microservices new project
85:49 so you may get the doubt why I'm adding
85:52 it in the root because if I add it in
85:54 the root and if you want to have the
85:57 test container dependencies inside the
85:59 order service and as well as the
86:01 inventory service you don't need to
86:03 repeat yourself right inside the product
86:05 service before we have to we are adding
86:08 the dependency management tag and
86:10 instead of repeating them one in on each
86:12 module I can just Define this inside the
86:16 parent module under the dependency
86:17 management tag and this dependency will
86:20 be
86:22 like propagated to all the child's
86:24 services like order inventory and the
86:26 product service
86:28 so that's the main reason we are adding
86:29 it in the parent
86:31 and another important thing we have to
86:33 do is to open the microservices parent
86:36 like you can open any uh any of the
86:40 form.xml and you can see that the parent
86:43 is defined as spring boot starter parent
86:46 for each of the services like product
86:48 service order service and the inventory
86:51 service in the old setup right so what
86:54 I'm going to do is I'm going to copy the
86:55 parent
86:56 and if I open for example product
86:59 service you can see if I scroll up you
87:03 can see that the parent is
87:05 microservices new project right but to
87:08 be able to download all these
87:09 dependencies to our project we have to
87:12 define the parent as
87:14 springboard starter parent so for that
87:16 what I can do is I can go to
87:18 microservices new project
87:20 and I can Define the Springwood starter
87:24 parent
87:26 as the parent of the micro Services new
87:29 project right so in this way
87:32 if microservices new project is the
87:35 parent for order service and the
87:37 inventory service so this springboard
87:40 starter parent will be the grand parent
87:42 of order service inventory service and
87:45 the product service so in this way all
87:48 the version will be downloaded without
87:50 any problems to our to our project so if
87:54 I click on the
87:55 load Maven changes icon all the
87:58 dependencies will be downloaded without
87:60 any problems right and one last thing I
88:04 have to do is now to add the spring boot
88:08 Maven plugin right if I open the one of
88:11 the old setup so if I go to product
88:13 service we have this plugin information
88:17 here right like the spring boot Maven
88:19 plugin and also in the order service we
88:21 have the spring boot Maven plugin right
88:23 so I'm just going to copy this whole
88:25 build tag where we have the compiler
88:28 plugin and the maven plugin
88:31 and similar to dependency management I
88:34 am going to add it inside the root
88:35 project inside the micro Services New
88:38 form.xml
88:39 so in this way the spring boot Maven
88:42 plugin and the compiler plugin will also
88:45 be added to the all the child projects
88:49 right so again I'm going to load the
88:51 maven changes to make sure that there
88:54 are no problems
88:55 in the setup
88:57 so to be able to test whether everything
88:59 is working correctly or not we have to
89:02 also copy the source code right so now
89:04 if you check the services we don't have
89:06 any code inside we've already developed
89:09 the code for this one so what I'm going
89:11 to do is I'm going to first delete this
89:13 Source folder delete the source folder
89:17 inside the product service
89:18 I'm going to do the same thing also for
89:21 the order service
89:23 and the
89:27 inventory service right so what I'm
89:30 going to do now is I'm going to copy the
89:32 source code from the old projects so I'm
89:35 going back to the micro services
89:38 parent project and I'm going to copy the
89:41 search folder from the product service
89:44 just right click copy
89:47 and just paste it under the product
89:49 service right click paste new name will
89:53 be Source click on OK
89:55 and everything will be copied without
89:58 any problems
89:60 similarly I am going to do the same
90:01 thing for the order service so go back
90:04 to the old project microservices parent
90:07 copy The Source folder
90:12 paste it inside the
90:14 order service
90:17 it's fine and now open the inventory
90:21 service
90:22 copy The Source folder
90:26 and
90:27 paste it inside the inventory service
90:29 click on OK
90:31 and everything we should work
90:34 without any problems now just to verify
90:37 that everything is working fine what I
90:40 can do is I can run a command called as
90:43 mavenclean verify so I'm going to open
90:47 the maven tab to the right side of the
90:49 IntelliJ IDEA and I'm going to click on
90:51 this m icon to execute a maven goal and
90:54 here I'm going to type maven clean
90:57 verify
90:58 right so what it will do is it will just
91:00 run the compile so it will try to build
91:03 the project without trying to install
91:05 the everything and finally you can see
91:09 that the build is success and it took
91:12 total of 29 seconds on my machine to
91:14 execute this and lastly let's check
91:17 whether the applications are also
91:19 working correctly or not like are also
91:21 able to start up correctly or not so I'm
91:24 going to open source main Java
91:26 product service application and I'm
91:29 going to run the product service
91:30 application so you can see that the
91:32 application is startup is started
91:34 without any problems the product service
91:36 application
91:38 and
91:40 order service application let's start
91:43 this one more time and it looks and it
91:47 started successfully on Port 8081
91:51 lastly the inventory service
91:55 let's start this application and also
91:58 you can see that it started successfully
92:00 on Port
92:02 8082 right if you are facing some
92:05 problems if you are not able to
92:07 replicate the same result as I am you
92:10 can download this whole particular whole
92:13 project from GitHub I will upload this
92:15 as a starter project for this particular
92:18 Series in GitHub you can download this
92:21 project and you can start working from
92:23 that point right so again you can check
92:26 the initial setup branch
92:28 inside the spring boot micro Services
92:30 New repository and there you will find
92:32 the complete setup the initial setup
92:35 which we have just did before right so
92:38 just download this project if you are
92:40 facing any problems and then you are and
92:43 you can start coding along with me from
92:45 now on all right so before we go on
92:47 there is a small bug inside the
92:49 inventory service
92:51 inside the inventory service application
92:53 class so we are running this load data
92:56 method multiple times whenever like
92:59 whenever so we are running this load
93:01 data method whenever we are starting up
93:04 the application right so that means
93:06 whenever we are starting up the
93:08 application we are inserting this two
93:12 inventory
93:13 objects into the database so that means
93:16 for sure we have some duplicates inside
93:19 our database let's go open our MySQL
93:22 workbench and just I'm going to run the
93:26 select query and you can see that this
93:28 particular iPhone 13 and iPhone 13 red
93:31 skew code inventory objects are
93:35 are duplicated so to prevent this for
93:39 now what I can do is I can just select
93:42 all these
93:45 duplicated records and I can just try to
93:47 delete them manually
93:49 delete and click on apply
93:52 and this will delete all the records
93:55 from our
93:57 from our table
93:59 right and to make sure that it won't
94:02 occur one more time what I can do is I
94:05 can go to resources
94:06 application.properties and as this is
94:10 not a production application I'm just
94:12 going to add it as create drop the ddl
94:15 auto as create drop if you are using it
94:18 in a production setup please don't use
94:20 create drop you have to use ddl Auto as
94:24 none
94:25 and you have to use a database migration
94:29 Library like liquibase or or Flyway to
94:33 do this kind of migrations so please
94:36 don't use create drop this is just used
94:39 as a demo purpose and please don't take
94:42 this as a recommendation to add this
94:44 inside your production code right so
94:47 having said that let's restart the
94:49 inventory service application and now if
94:52 you open the inventory service
94:55 click on select you can see that we
94:57 still have the two records we are not
94:60 duplicating it because it's creating and
95:03 first it's first dropping and then
95:04 creating the schema again so in the
95:06 previous part we have developed all the
95:08 required services for our project so we
95:11 have developed the product service the
95:13 order service and the inventory service
95:15 but what is still missing is like we
95:17 don't have the services communicating
95:20 with each other in our project right
95:22 usually in a micro service architecture
95:25 if I open the architecture diagram again
95:27 we want our services communicating with
95:30 with each other right so this kind of
95:32 communication can be of two types as
95:34 I've already mentioned before it can be
95:36 of type A synchronous communication or a
95:39 asynchronous communication
95:40 so let's have a look at this kind of
95:43 communication in detail so we have our
95:45 order service and imagine that we want
95:48 to make a call to the inventory service
95:50 whenever a customer places an order we
95:52 want to check whether the product is in
95:54 stock or not right so for that our order
95:57 service will make a call to the
95:59 inventory service it will make a HTTP
96:01 request and if the inventory service
96:04 will respond status of the product
96:05 whether the product is in stock or Not
96:08 So based on this response the order
96:11 service will either place the order
96:13 successfully or it will throw an
96:14 exception or maybe or show an error
96:17 message that the product is out of stock
96:19 to the customer
96:20 right so this kind of communication
96:22 where the service a is talking to
96:25 service B and is waiting for the
96:27 response from service B so this kind of
96:29 communication is called as a synchronous
96:31 communication so the exact opposite
96:33 where order service will make a request
96:35 to the inventory service and it doesn't
96:37 care about the response from the
96:39 inventory service then this kind of
96:41 communication is called as asynchronous
96:43 communication so it's kind of a fire and
96:46 forget pattern where the service a
96:49 files a request to for service B and it
96:52 won't care about the response from
96:54 service P so this kind of request like
96:57 the synchronous communication is usually
96:59 done through the HTTP clients right so
97:02 the HTTP clients we have in our
97:03 springbootex system are the rest
97:06 template class which comes by default
97:10 from the spring boot framework and we
97:12 also have another alternative called as
97:14 web client so this web client class is
97:17 coming from Spring web flux project we
97:20 can either use rest template or web
97:21 client but if you open the documentation
97:24 of rest template you can see that it is
97:27 as of 5.0 spring framework 5.0 the rest
97:31 template class is in maintenance mode so
97:33 spring boot is recommending to use web
97:35 client as an alternative because it has
97:37 a more modern API and it also supports
97:40 synchronous asynchronous and streaming
97:42 scenarios so for this reason we will go
97:45 ahead and use web client to make a
97:48 synchronous request from order service
97:50 to the inventory service so let's go
97:52 ahead and implement this in our project
97:54 now so I'm inside the project so let's
97:57 make a so to be able to call the
97:60 inventory service from the order service
98:02 so first let's open the
98:06 order service class and here while
98:09 placing the order
98:11 we want to make a call to the inventory
98:13 service so let's
98:15 let me
98:17 add a comment here
98:20 call inventory service
98:23 and place order
98:26 if product
98:27 is in
98:29 the stock
98:31 right so we want to implement this
98:34 functionality now so for that as we have
98:36 said before we're going to we are going
98:38 to use web client so for that let's
98:40 first create a new package inside the
98:43 order service package
98:45 called as config
98:48 and in here I'm going to create a new
98:50 class called as web
98:53 client config
98:56 right let me add this class to my git
98:59 Repository
99:00 and I am going to add a configuration
99:04 annotation on top of this class because
99:06 as we are going to maintain the
99:07 configuration about the web client class
99:10 so that's why I created a new class and
99:13 inside this web client config class I'm
99:15 going to define a bean of type web
99:18 client right so for that I am going to
99:20 add the bean annotation
99:24 and type in public your client
99:28 and you can see that we don't find the
99:31 web client class because this is not
99:35 part of the spring MVC project but this
99:37 is part of the spring web flux project
99:39 right so to be able to use spring web
99:42 client in our project we have to use add
99:45 the spring web flux dependency to our
99:47 order service so let's go ahead and do
99:49 that I'm going to open the form.xml file
99:52 and right below the spring boot starter
99:56 web I'm going to copy this dependency
99:58 information I'm just going to press
100:01 enter and paste in the
100:03 text I've copied so instead of typing
100:06 springboard starter web I am going to
100:08 say spring boot starter web flux right
100:12 if you type in IntelliJ will already
100:15 show us the recommendation I'm going to
100:17 just select the spring boot starter web
100:20 flux dependency and I'm going to select
100:22 on and click on this load maybe changes
100:24 icon so that IntelliJ will download this
100:27 dependency to our project so once the
100:29 dependency is downloaded you can see
100:31 that the red text around the spring boot
100:33 starter web flux is gone that means the
100:35 dependency is downloaded successfully to
100:38 a project
100:39 and now let's go ahead to the web client
100:41 config class and continue what we were
100:43 doing before so let's try to import this
100:46 web client class from Spring framework
100:49 web reactive function client package so
100:52 you can see that this client web client
100:54 class is now available in our class path
100:57 and I am going to call this method name
100:59 as web client
101:01 and in here I'm going to type in return
101:05 web client dot Builder dot build
101:09 right so what this will do is it will
101:12 create a bean of type web client and you
101:15 are going to Define that bin with the
101:17 name of web client right so whenever we
101:19 add a bean annotation this mean will be
101:22 created with the name of the message
101:23 method name so whatever method name you
101:26 have given it will create the Bain with
101:28 this particular name right so if you
101:30 want to use this particular web client
101:32 being inside the order service we have
101:35 to Define this web client bin with the
101:36 same name so I'm just going to copy this
101:38 with client name and go back to the
101:41 order service
101:42 and now I'm going to inject this web
101:45 client into our order service class by
101:48 typing private final web client
101:52 and just paste in the web client
101:54 reference variable name
101:57 and now inside the place order method
102:00 I'm just going to type in just below the
102:03 comment I'm going to type in red client
102:06 dot get because the inventory controller
102:10 if you open the end point inside the
102:11 inventory controller
102:14 you can see that we are exposing a get
102:16 endpoint which will take in a skew code
102:19 as the as the path variable and it will
102:22 return a Boolean variable as a response
102:24 so for that we have to make a get call
102:26 so I'm going to type the get method and
102:30 in here I have to pass in the URL of the
102:33 inventory service right so as we already
102:35 know inventory service is running on
102:38 Port 8082 that means I have to add the
102:42 URL as http
102:44 localhost
102:46 8082 slash API slash inventory
102:51 right so we're going to make a get call
102:54 and to be able to retrieve the response
102:57 I am going to type in the method name
102:60 dot retrieve and for the web client we
103:03 also need to add uh Define what is the
103:06 type of response we are returning from
103:08 the inventory service right so for that
103:10 I can type in body to
103:13 so mono is nothing but a data type in
103:17 the reactive framework I'm not going to
103:18 go into the details of what is mono to
103:20 be able to so just you have to know that
103:23 to be able to read the data from the
103:25 client response you have to add this
103:28 body to Mono method and inside this body
103:31 to Mono method we have to provide the
103:33 type of the response right so we are
103:36 returning a Boolean value as a response
103:39 so for that I'm just going to type in
103:41 Boolean Dot
103:44 class and finally by default the client
103:48 will make asynchronous request right but
103:50 we have to make a synchronous request so
103:52 to be able to make a synchronous request
103:54 with web client we have to add one more
103:57 Port we have one more line of code
103:58 called as
103:60 dot block right so if we add this piece
104:05 of code then web client will make a
104:07 synchronous request to http localhost
104:10 8082 to the inventory endpoint right
104:13 because I think I've got the syntax
104:15 wrong you should not provide the URL
104:18 inside the get method but we have to add
104:21 another method called as dot URL
104:26 dot URI and then we have to provide the
104:29 actual URL we want to call right so it
104:32 should be like webclient.get URI and
104:35 then retrieve the response so now we
104:37 have to save this response into a local
104:39 variable so I'm just going to introduce
104:41 a new local variable called as Boolean
104:43 and I'm going to call it as result
104:46 right and if this pool if this result is
104:49 true that means if the product is in
104:53 um is in stock then we have to place the
104:55 order or else we have to throw an
104:57 exception right so for that I'm just
104:59 going to type in if result
105:02 then order repository dot save order
105:07 else
105:09 through illegal argument exception
105:14 and I'm going to type in
105:17 product is not in stock
105:20 please try again later all right so we
105:24 are not yet done with the implementation
105:25 you can observe that our inventory our
105:29 inventory controller will is taking is
105:33 expecting a skew code as a path variable
105:35 right so for whatever product we want to
105:37 make a inventory check we have to pass
105:40 in the skew code also as part of the get
105:42 request but here is one tricky thing so
105:46 if You observe the order Pro order
105:48 object you can see that each order can
105:51 have multiple order line items right and
105:53 each order line item contains a skew
105:55 code so for example if a customer orders
105:58 100 items as part of the order and it
106:02 contains hundreds Q codes should we make
106:04 100 calls to the inventory controller
106:07 does it make sense no right because
106:09 making 100
106:11 HTTP calls is very time consuming and
106:14 the request will also take a lot of time
106:17 for the customer the request processing
106:19 will also take lots of time right so for
106:23 that what we can do is we can collect
106:25 all the skew codes which are present as
106:27 part of the order like take it as
106:30 collect these SQ codes as a list and
106:33 then we can provide this list to the
106:35 inventory service API right so we have
106:38 to make the changes in the inventory
106:40 controller so instead of taking one
106:42 string once Q code as the parameter we
106:45 have to take multiple skew quotes as a
106:47 parameters right and this is how the
106:50 usual HTTP request looks like if you are
106:52 using a so this is how it looks like the
106:54 HTTP localhost 8082 slash API slash
106:58 inventory and iPhone 13 would be this Q
107:01 code and what happens if you send
107:03 multiple values as part of the path
107:06 variable so this looks like something
107:07 like this right iPhone 13 comma iPhone
107:10 13 red
107:11 right this is how the path variable
107:14 format looks like and what happens if we
107:17 want to display it like a request
107:20 parameter so instead of instead of path
107:22 variable you can also choose to Define
107:24 this as a request parameter right so in
107:27 this case the request parameter looks
107:29 something like this right
107:31 the skew code equals iPhone 13 and skew
107:34 code equals iPhone 13 red so for each
107:36 element inside the list it will create a
107:39 separate request parameter and it will
107:41 append it to our URL all right I would
107:44 like to go with this format because this
107:46 is a more readable format right you can
107:49 see that we have two different request
107:51 parameters and instead of the first one
107:54 where we are just appending all the all
107:56 the values with a comma right so for
107:59 this reason what I'm going to do is I'm
108:01 going to remove the path variable so
108:03 this I'm just going to remove everything
108:06 inside the get mapping definition and
108:08 I'm also going to remove the path
108:11 variable and add the request
108:15 param
108:16 annotation and I'm going to change the
108:19 string to a list of string
108:23 right and I'm also going to
108:25 import this list from java util package
108:28 and of course
108:30 the inventory service will complain that
108:33 this skew code should be a string
108:35 instead of a list of string right so for
108:37 that I am going to also change the ease
108:39 in stock method inside the inventory
108:42 service from string to a
108:44 list of string and I'm also going to
108:47 import the class list from java util
108:50 package and similarly I'm also going to
108:52 do a change for the find by skew code
108:54 method right and in the spring data jpa
108:58 we want to find the inventory which we
109:02 have to find all the inventories with
109:04 this given skew code right so instead of
109:06 find by skew code I want to type find by
109:09 skew code in and I'm going to provide
109:12 all the list of skew codes of course
109:14 this method does not exist yet so I'm
109:16 just going to create this find bias Q
109:18 code
109:19 in method and this find by skew code in
109:22 method should return us a list of
109:27 inventory
109:28 objects
109:30 right so we are providing a list of skew
109:33 codes and spring data jpa will return us
109:36 all the inventory objects which match
109:38 this skew code right so I can remove the
109:42 first find by skew code method as we are
109:46 not using this anymore
109:48 right hand I'm going to go back to the
109:50 inventory service and now the East
109:53 present method uh doesn't make any sense
109:56 because we are returning a list from
109:58 this find bias Q code in method right
110:01 now what we have to do is
110:03 we have to also change the written type
110:06 of this in series in stock method right
110:08 before we are just returning a Boolean
110:10 but now we have to return the
110:13 stock we have to return the stock
110:15 information for each skew code right so
110:18 for that what I will do is I am just
110:20 going to remove the yeast in pressure is
110:22 present method and I'm going to type in
110:24 dot stream and now I'm going to map this
110:27 list of inventory objects which are
110:29 coming as a response to a different uh
110:32 different type right I'm going to create
110:34 a dto called as inventory response so
110:38 first I'm going to create a map method
110:40 and in here for each inventory I am
110:43 going to create a inventory response
110:46 object so for that first I have to
110:48 create the inventory response class
110:50 inside the inventory service inside the
110:53 inventory service project so I'm going
110:55 to create a package called as dto and
110:58 inside the dto I'm going to create a
110:59 class called as
111:02 inventory response
111:05 right I am going to add the data
111:07 annotation from lombok all aux
111:09 Constructor Knox Constructor I am also
111:12 going to add the Builder annotation
111:15 and inside the inventory response I'm
111:17 going to have two Fields private
111:20 string skew code
111:22 and private
111:24 Boolean is in
111:27 stock
111:28 right we are going to store the
111:29 information about the skew code and
111:31 whether the skew code is in stock or not
111:33 right go back to the inventory service
111:36 and inside the map
111:40 inside the map method
111:41 I'm going to type in
111:44 inventoryresponse Dot Builder
111:48 dot skew code and I'm going to type in
111:51 inventory dot gets keyboard
111:55 right and coming to the is in stock
111:58 Boolean variable I want to check whether
112:01 quantity of your inventory right so I'm
112:03 going to type in inventory dot get
112:05 quantity and if this quantity is greater
112:07 than 0 then we can say that the product
112:10 is in stock right so I'm just going to
112:12 add greater than 0 then it will return a
112:15 Boolean variable whether the quantity is
112:17 in stock or not and finally I'm going to
112:20 type in the build method so in this way
112:23 we have created the inventory response
112:26 object and we want to send this as a
112:29 response send this as a return variable
112:31 as a written variable from our is in
112:34 stock method right so for that I'm just
112:36 going to type in dot to list
112:38 and uh
112:41 I'm going to remove the
112:44 brackets here
112:49 and also remove the semicolon
112:52 so in this way
112:54 we will return the list of inventory
112:56 response from our method and instead of
112:59 Boolean we have to change the written
113:01 type as let's talk
113:04 inventory response
113:07 right now you don't have any compilation
113:10 errors and what we have to do is now we
113:13 have to adapt also our inventory
113:14 controller reason stock method so I'm
113:17 going to open the inventory controller
113:20 and change the written type also here
113:22 from Boolean to list of
113:25 inventory
113:27 response
113:29 so we are taking the list of string as
113:32 request parameters and we are passing
113:35 this Q code list of string into the
113:37 easing stock method and acquiring the
113:40 repository to find out all the inventory
113:42 objects for this given's Q code and then
113:45 mapping the inventory objects to the
113:47 inventory response object and then
113:50 finally we are sending this list of
113:52 inventory response object as the
113:54 response right I hope this is clear now
113:57 the next part is we have to call this
113:60 new enhanced get endpoint from our order
114:03 service right now inside the order
114:05 service and now what I want to do is I
114:08 want to First collect all those Q codes
114:11 from the order object right so for that
114:14 what I can do is I can type in order dot
114:18 get orderline items list
114:21 dot stream
114:23 dot map
114:24 and for each order line item
114:30 I want to map orderline item dot getsq
114:33 code
114:33 and I can replace this Lambda with a
114:36 method reference so I'm just going to
114:38 replace Lambda with method reference and
114:41 what I'm going to do is I'm going to
114:42 collect this to a list
114:44 now what I have is file try to store
114:50 this into a local variable I will get a
114:52 list of strings right so I'm going to
114:54 call it as
114:55 skew codes
114:56 now I have a list of skew codes with me
114:59 what I have to do is I have to enhance
115:01 this web client API call to include this
115:05 request parameters skew codes as the
115:07 request parameters right so for that
115:09 what I can do is right after the URI I
115:13 can type in URI Builder
115:16 and I can type in URI Builder dot query
115:19 Para
115:21 let's add this in a new line so that
115:24 it's visible so as a query pattern I am
115:27 going to provide the skew code right so
115:30 call it as skill code and I'm going to
115:33 provide the list of skew codes as the
115:35 value right and I'm going to finally
115:38 call build method so that the web client
115:42 will fill the URI with the query
115:44 parameters as skew code so in this way
115:47 as you have said before web client will
115:49 construct the URI in this format so it's
115:53 not SQ code with hyphen but it's going
115:56 to construct it as skew code with
115:59 capital c because as we have provided
116:01 the variable name as capital c so
116:06 this is going to construct the request
116:08 in this format so
116:10 now
116:11 once we have made the call we have to
116:13 also change the return type of this get
116:16 request right we are not returning
116:18 Boolean as a variable anymore we are
116:20 returning an inventory response right so
116:23 to show this inventory response inside
116:25 the order service we also have to create
116:27 the inventory response class inside the
116:30 order service right so let's go ahead
116:33 and do that I'm going to open order
116:34 service and inside the dto I'm just
116:37 going to copy this inventory response
116:40 and paste this inside the
116:42 dto package
116:44 so if you are confused why I am creating
116:46 this inventory response class one more
116:48 time because this inventory response we
116:50 have created it inside the inventory
116:52 service right the class inside the
116:54 inventory service we cannot access it
116:56 inside the order service so that's why
116:58 I'm duplicating this class inside I'm
116:60 also creating this class inside the
117:02 order service so now let's go back to
117:04 the order service class instead of
117:05 borrow it instead of Boolean I'm
117:07 expecting a list of inventory response
117:09 object right so I can also Define this
117:11 inventory response list as an inventory
117:14 response array so for that I am going to
117:16 type in inventory response
117:19 array
117:21 dot class so in this way web client will
117:24 parse the response to an array of
117:27 inventory response objects and will
117:29 provide it as a result right and we are
117:31 getting a compilation error because we
117:33 are trying to assign the result of this
117:36 web client call to a Boolean variable so
117:39 instead of Boolean I am going to call it
117:41 as
117:42 inventory response
117:49 array so let's also rename this variable
117:52 as
117:53 inventory responses
117:55 or maybe inventory response array
117:59 and now we have to verify whether the
118:02 products are in stock inside the each
118:05 inventory response or not right so for
118:07 that I am going to
118:10 say inventory response
118:12 Dot
118:14 stream
118:15 and what it will do is it will create a
118:17 stream from the array right using the
118:20 arrays.rs.stream method it will create a
118:23 Java 8 stream from the array so that we
118:26 can easily call the all match method
118:30 right and I'm going to type in inventory
118:33 response Dot
118:35 is in stock
118:37 and I'm going to
118:41 replace this Lambda with a method
118:43 reference and also I'm going to store
118:46 this inside a new local variable
118:49 and I'm going to call this local
118:51 variable as all products
118:53 in stock
118:55 right
118:56 let's step back and see what we are
118:59 doing here so we got this list of
119:01 inventory response array we are
119:03 converting it to a stream and we are
119:05 calling the all match method from java
119:07 height it will check whether the is in
119:10 stock variable is true inside the array
119:13 or not if all the elements inside the
119:16 inventory response list contains is in
119:19 stock as true then it will return as
119:21 true even if one of them is false we
119:24 will get the all products in stock as
119:26 false so what I'm going to do is I'm
119:27 going to copy this all products in stock
119:29 and replace it with the result
119:31 and if all products are in stock then we
119:33 are going to save the order to the
119:35 database or else we are going to throw a
119:37 exception I hope this is clear so now
119:40 what I'm going to do is I'm going to
119:42 restart the order service
119:45 so open the order service and click on
119:47 this Free Run order service button
119:50 and also for the inventory service I am
119:52 going to open the inventory service go
119:54 to console and
119:58 click on the button again rerun
119:60 inventory service application so once
120:02 both the inventory Service as well as
120:04 the order service is up and running I'm
120:06 going to open Postman and I've already
120:08 created a request here so I'm going to
120:11 make a call to localhost 8081 API order
120:14 this is going to be a post request and
120:17 inside the body I am going to provide
120:19 two items right inside the order line
120:21 items one is going to be iPhone 13 and
120:23 one is going to be iPhone 13 red right
120:26 so iPhone 13 red is we have set the
120:30 quantity has zero if I open the
120:32 inventory application
120:34 and
120:36 here you can see that for iPhone 13 red
120:38 you have set the quantity as zero and
120:40 for iPhone 13 we have quantity set the
120:42 quantity as 100 right so if you try to
120:44 place this order we should get an
120:46 exception right so let's try to see
120:48 whether we are getting an exception or
120:49 not I am going to click on send
120:53 and yes we did get the X action let's go
120:56 to the console and see whether the
120:59 exception
120:60 is correct or not so you see that we are
121:03 getting an S response like product is in
121:05 stock please try again later product is
121:08 not in stock please try again later
121:10 right so let's try again with a happy
121:13 path so I'm just going to clear this
121:15 console
121:16 go back to the postman client and now
121:19 I'm going to remove this
121:21 uh iPhone 13 red object right so let's
121:24 check whether we are able to place the
121:25 order successfully or not
121:28 I'm going to click on send again
121:30 and now you can see that order is placed
121:32 successfully that means our order
121:34 service is able to successfully
121:36 communicate with the inventory service
121:38 and we are making it happen only with
121:40 the one rest API call right we are not
121:43 making multiple eBay calls we are making
121:45 only one recipe call so with this we
121:48 have completed the development of our
121:49 services in the next part we will have a
121:52 look at the spring Cloud ecosystem and
121:54 how we can introduce the services in
121:56 Spring Cloud to our project right so
121:59 I'll see you in the next video Until
122:00 Then happy coding techies so in the
122:02 previous part we had a look at what is
122:04 inter process communication and how this
122:06 works in our micro Services project so
122:09 we have our order service calling the
122:11 inventory service through the web client
122:14 class and we are making a call to the
122:17 inventory service at the location at the
122:19 URL localhost 8082 slash API inventory
122:22 right so let's also look at this here so
122:25 we are making a call directly to the
122:27 inventory service by hard coding the URL
122:29 so the problem here now is this works as
122:32 we are running is in our local machines
122:34 right but usually in the micro Services
122:37 environment everything will be deployed
122:39 on a cloud environment and in a cloud
122:41 environment we cannot have dedicated IP
122:44 addresses right everything will be
122:45 dynamic we will have Dynamic IP
122:47 addresses and we can it will run on
122:50 maybe different ports usually if the
122:52 microservices runs on port 8080 but it
122:54 can run on different ports right you
122:56 cannot make any assumption that
122:58 inventory service will be available in
123:00 this in this IP address right
123:02 uh in addition to that we can have
123:05 multiple instances of the given
123:07 microservice like in this diagram we can
123:09 have multiple instances of the inventory
123:11 service and each instance can have a
123:14 dynamic IP address for example you can
123:16 have
123:17 10.120.23.1 23.2 and 23.3 now in this
123:22 instance how our order service will know
123:24 which instance of the inventory service
123:27 we have to call right it cannot
123:29 understand which instance we have to
123:31 call even though we try to hard code one
123:34 instance IP address of the one instance
123:36 it can this instance can go down anytime
123:39 right it can it can be um
123:41 stopped and shut down anytime and we
123:45 have to make a call to the other
123:46 instances so how do our order service
123:48 understand which inventory service you
123:51 have to call right so for this reason
123:53 there is a pattern called as service
123:55 Discovery pattern and we are going to
123:57 discuss this in detail in this video so
123:59 the service Discovery pattern is nothing
124:01 but creating a server something called
124:03 as a discovery server which will store
124:06 all the information about the services
124:07 right the service name and as well as
124:10 the IP addresses like you see in this
124:12 particular table right a discovery
124:15 server is nothing but a place where it
124:17 stores all the information about the
124:18 services when we are using the discovery
124:20 server what our micro services will do
124:23 is it will they will first try to
124:24 register at the time of starting up the
124:26 application they will try to register
124:28 with the discovery server by making a
124:30 request right whenever the services are
124:33 making the request it will add the
124:35 discovery server will add the entries of
124:37 these Services into its into its local
124:40 copy we call it as a registry so that's
124:42 why it's also called as a service
124:43 registry once all this information is is
124:46 present in the discovery server when our
124:48 order service wants to call the
124:50 inventory service so let's see how the
124:52 communication will happen here in this
124:54 diagram when the order service wants to
124:56 call the inventory service the order
124:58 service will first make a call to the
124:60 Discovery server asking where I can find
125:02 the inventory service and then the
125:04 discovery server will respond with like
125:06 particular IP address to call the
125:08 inventory service in this particular IP
125:11 address
125:11 and then the order service will make the
125:14 call to the inventory service in this
125:16 way we can avoid hard coding the URL of
125:19 the inventory service by making use of
125:21 the discovery server how we can do that
125:23 exactly we will have a look at it later
125:25 but this is what you need to know to
125:29 understand the service Discovery process
125:30 right and another thing which you have
125:34 to know is when making initial call to
125:37 the Discovery server what the discovery
125:38 server will do is
125:40 it will also send its registry as the
125:43 response to the client right so what the
125:46 client will do is it will store also the
125:48 local copy of the discovery server in a
125:50 separate location so if for some reason
125:52 the discovery server is not available it
125:55 will first check the local copy right
125:57 because as it already has information
125:58 about the inventory service it will
126:00 check the local copy and we will make a
126:02 call to
126:03 10.12.123.1 which is the inventory
126:05 service so what happens if this 10.12.
126:08 list is the first instance of the
126:10 inventory service is not available right
126:12 it will try to check the next entry in
126:14 the local copy so it will make a call to
126:18 10.12.123.2 so likewise it will go
126:21 through all the entries of its own
126:23 registry and if all the instances are
126:25 down then it will fail the communication
126:27 saying that the inventory service is not
126:30 available
126:31 so we are going to see all this theory
126:32 in practice so now let's go ahead and
126:35 create the discovery server all right so
126:37 the first thing I'm going to do inside
126:39 our project is to create a new Maven
126:41 module called as Discovery server so for
126:45 that I'm just going to right click on
126:46 our root project and going to new module
126:51 and in here I'm going to select the
126:53 maven module and just click on next
126:57 and I'm going to view the name as
126:59 discovery
127:01 server
127:03 right and I'm going to click on finish
127:05 and let's just check the artifact
127:07 coordinates we have the group ID as com
127:10 programming techie artifact IDs
127:12 Discovery server right and let's click
127:14 on finish
127:15 click on ADD
127:17 and you can see that a new module Maven
127:20 module is added Discovery server and if
127:23 I check the root Palm XML file
127:26 the discovery server is also added as
127:29 one of the modules of the root bomb XML
127:32 so the next thing we have to do is we
127:34 have to add the necessary dependencies
127:35 to make this a discovery server and to
127:38 be able to create the discovery server
127:40 we are going to make use of the library
127:42 Netflix Eureka so let me open the spring
127:45 Cloud documentation and in here if I
127:48 scroll down until I find the spring
127:50 Cloud spring Cloud Netflix project you
127:53 can see that we have a module for
127:55 service Discovery called as a Eureka
127:57 server so we can make use of this spring
127:59 Cloud Netflix Eureka server in our
128:01 project to enable the discovery server
128:03 capabilities so for that I am going to
128:05 so for that I need to add some necessary
128:08 dependencies in our Discovery server
128:11 project so for that time I'm going to
128:13 make use of star.spring.io website here
128:16 we have the good option to explore the
128:19 repositories after we added the
128:20 dependency and we can just make use of
128:22 the dependency information which is
128:24 displayed here so let me close this and
128:26 first fall what I'm going to do is I'm
128:28 going to add a dependency
128:30 called as
128:31 yurica server so this would be the
128:33 spring Cloud Netflix Eureka server I am
128:35 going to add this dependency and I'm
128:37 going to click on explore and I'm going
128:40 to scroll down and you can see that we
128:42 have the dependency spring Cloud starter
128:44 Netflix Eureka server right I'm going to
128:48 copy this particular dependency
128:51 and open my IDE and under the palm.xml
128:55 of the discovery server
128:57 I am going to add the dependencies tag
129:00 and I'm going to paste in the spring
129:03 Cloud starter networks you like a server
129:05 dependency right the one thing you can
129:07 observe here is this is coming from
129:09 springframework dot Cloud but not
129:12 auxpringframework DOT boot right so for
129:15 this reason as this is coming from a
129:18 different group ID we have to add some
129:20 additional information to our palm.xml
129:22 so if I go back to the start.spring.io
129:25 website and just scroll down until I
129:28 find the dependency management section
129:29 here we have the dependency management
129:32 section the bomb version of the spring
129:34 Cloud dependencies right so I'm going to
129:36 copy this information the dependency
129:39 information I'm not going to copy the
129:40 dependencies I'm just going to copy the
129:42 dependency information
129:45 and I'm going to open my pom.xml here
129:49 the root prompt.xml because I'm already
129:51 maintaining the necessary information
129:53 for test containers inside this
129:55 dependency management I'm also going to
129:57 add spring Cloud dependencies to the
129:59 dependency management section and here I
130:02 also need to add the spring Cloud
130:03 version so for the spring Cloud version
130:06 I'm going to add a property here under
130:08 the property stack
130:11 called as explain Cloud version
130:13 and the spring Cloud version this is
130:16 going to be
130:18 2021.0.2 as of creating this tutorial
130:22 so I'm going to use the same version
130:24 here
130:25 and I'm going to click on this Maven
130:29 icon so that the necessary dependencies
130:32 will be downloaded to my local machine
130:34 and you can see that the red text is
130:36 gone and that means the dependency is
130:39 downloaded successfully to our machine
130:41 and the next thing I'm going to do is
130:43 I'm going to open source main Java and
130:46 I'm going to right click and create a
130:49 package called as com
130:52 programming dot techie
130:55 Discovery server and make sure you don't
130:58 add any
130:59 Dash or hyphen here inside the package
131:02 name or else the IntelliJ won't
131:04 recognize this as a Java package
131:07 so after that I'm going to create a new
131:09 class called as Discovery server
131:14 application
131:16 and this is going to be a normal spring
131:19 boot application so I'm going to add at
131:21 springboot application annotation
131:24 and we're going to have a public static
131:27 void main method so I'm going to create
131:30 this method here and inside this method
131:35 I'm going to copy the existing text
131:39 which is spring application.run
131:42 and here instead of the inventory
131:44 service application I'm going to call it
131:46 as Discovery server application.class
131:49 and I'm going to pass in the command
131:51 line arguments to the run method
131:53 all right so now we have the basic
131:56 Discovery server application to make it
131:59 a Eureka server Eureka Discovery server
132:01 all we have to do is we have to add
132:04 enable
132:07 Eureka server annotation on top of this
132:10 class and that's it that's all you need
132:12 to do to create the discovery server
132:15 apart from this you also need to add the
132:17 application.properties in the
132:19 application with properties you need to
132:21 add some couple of properties so for
132:23 that I'm going to go to the resources
132:25 folder click on new
132:27 file
132:29 application Dot
132:31 properties
132:34 and the first thing I'm going to do is
132:35 I'm going to Define Eureka dot instance
132:39 dot hostname this is going to be
132:42 localhost
132:44 and I'm going to type in Eureka Dot
132:49 screen Eureka dot client
132:52 dot register with Eureka as false
132:57 because as this is a discovery server we
133:01 don't want to register with Eureka right
133:03 because this is the Discovery server
133:04 itself we don't want the server to
133:06 register itself as a client right as
133:09 this is the server we don't need to
133:11 register as a client and that's why we
133:14 set it as false and the next thing is
133:16 Eureka Dot client dot fetch registry is
133:21 also going to be false again because as
133:24 this is a server we don't need to fetch
133:26 the registry of the server because it's
133:27 already maintains it registry locally
133:30 so whenever the client tries to register
133:32 to the Discovery server the discovery
133:34 server will send the local copy of its
133:37 office registry right the client will
133:39 store the registry in memory in its own
133:42 storage so usually when we are defining
133:44 the client we will set the Eureka client
133:47 fetch registry as true by default as we
133:50 are configuring a server we have to set
133:52 it by for as false manually and the last
133:56 thing we have to do is we have to define
133:58 the port which on which the discovery
134:02 server is running so we are going to
134:03 Define it as
134:05 server.port is equals to 8 7
134:08 X1 right so this is the default Port
134:12 where the Eureka server will run so
134:14 that's why I have provided the port as
134:16 8761 so now let's go back to the
134:19 Discovery server application and try to
134:21 run this discovery server
134:24 and as let's see if it's able to start
134:27 up without any errors
134:30 and you can see that in the logs
134:33 that
134:34 the
134:37 we have started the Eureka server and
134:39 the discovery server application is up
134:41 and running in three seconds right so
134:44 the next thing we have to do is we have
134:45 to define the Eureka clients so we have
134:48 the discovery server we have to define
134:51 the Eureka client so we have three
134:53 applications the inventory service the
134:55 order service and the product service
134:57 right so I'm going to open each one of
135:00 these Services one by one and I'm going
135:02 to add the dependency for the Eureka
135:05 clients starter right so for that I'm
135:08 just going to go to the palm.xml and
135:10 just copy this dependency because I just
135:14 wanted to don't want to type this one
135:16 more time
135:17 I am going to paste this dependency
135:19 inside the form.xml and I'm going to
135:22 replace the text Eureka server with
135:26 Eureka
135:27 client
135:29 right
135:30 and the version we are going to put it
135:34 directly from the parent form so all I
135:37 have to do is load the maven changes and
135:40 it's trying now it's trying to download
135:42 the dependencies from the maven
135:45 repository and you can see that the red
135:49 text turned to White that means Maven is
135:52 able to download the Eureka client
135:54 starter dependency successfully now I
135:57 can copy this dependency one more time
135:59 and paste it inside the order service
136:00 and as well as the product service so
136:03 let's do that
136:05 pasted it inside the order service
136:09 and let's open the pro product service
136:12 now
136:13 and I am going to again paste this
136:16 information here
136:20 so once the dependency is added we have
136:22 to add an annotation on top of each
136:25 application class for clients so I'm
136:29 going to open the main application class
136:31 where we have the spring boot
136:32 application annotation just below this
136:34 annotation I am going to add
136:36 enable
136:38 Eureka client annotation
136:41 similarly I am also going to add this
136:43 annotation also for the
136:46 order service application class
136:50 enable Eureka client
136:52 let's also open the
136:55 product service application
136:59 and I'm going to add the enable Eureka
137:02 client annotation also here and now we
137:04 need to add some configuration in our
137:06 application.properties file regarding
137:09 the Eureka client right because as these
137:11 are Eureka clients we have to add the
137:14 properties of the urea server we have to
137:16 provide the information where we can
137:18 find the Eureka server so inside the
137:20 application.properties of the inventory
137:22 service I am going to just add this Rica
137:26 dot client dot service URL dot default
137:31 Zone
137:32 equals http
137:35 localhost
137:39 878761 slash
137:41 Eureka right so this is going to be the
137:44 URL of the Eureka server so we have to
137:47 provide it as the default service URL
137:50 default zone so that our clients can
137:52 find the Eureka server right I'm going
137:55 to copy this property again to the order
137:59 service
138:00 application.properties file
138:03 and also inside the
138:05 product service
138:06 application.properties file so once this
138:09 is done I am going to restart all the
138:12 services so I'm going to
138:15 open the inventory service application
138:17 I'm going to restart application one and
138:20 also for the order service application I
138:22 am going to restart it and finally I'm
138:24 going to open the product service
138:26 application
138:27 and I'm also going to start the product
138:30 service application so meanwhile let's
138:32 see whether the inventory service
138:33 application is up and running or not
138:36 and we can see that our application is
138:39 up and running at Port 8082 and in here
138:41 you can see that our inventory service
138:44 application is acting as a discovery
138:46 client and it's trying to call the
138:49 discovery server right so you can see
138:51 the log getting all instance registry
138:53 entry information from the Eureka server
138:55 the response status is 200 that means
138:58 our our Discovery our client that's
139:01 inventory service application made
139:03 request to the Discovery server and
139:05 fetch all the registry at the time of
139:07 startup right and let's see also it's
139:11 the same for order service application
139:13 yes it's the same and also for the
139:17 product service application
139:18 it's not yet done because we just
139:21 restarted it because we just started the
139:23 application but in some time it should
139:26 also started to it should also make a
139:28 call to the Discovery server and fetch
139:30 all the information about the registry
139:32 you can also check the status of our
139:34 services in a separate Eureka dashboard
139:38 8761-8761 here we can see that we are
139:41 greeted with a dashboard from Spring
139:43 Eureka here we have the environment test
139:45 and the data center as a default and we
139:48 have some metadata like the current type
139:50 and current time and how much time and
139:52 the uptime and the the renewal threshold
139:56 and the renews how many times it has
139:57 renewed and here you can see that we
140:01 have the application as unknown right
140:03 here we the Eureka server identified
140:06 three instances of application but it
140:09 has application name as unknown this is
140:12 because uh we did not Define any unique
140:15 name for our applications for our
140:17 service the inventory service and the
140:19 product service so that's why Eureka
140:22 server our Eureka server is not able to
140:24 provide any names here so to be able to
140:27 fix that we are going to open the I am
140:32 going to open the application.properties
140:33 file and I'm going to add one more
140:36 property here called as spring dot
140:39 application.name equals
140:42 product service so this is going to be
140:45 inventory service right so it should be
140:46 inventory service
140:49 and similarly I'm also I'm going to copy
140:51 this
140:52 property and open the
140:55 properties file of order service and I'm
140:58 going to name it as
140:59 order service instead of inventory
141:02 service and let's open the property file
141:04 of product service
141:06 and name this as product service so
141:09 after making these changes let's restart
141:11 the application so I'm going to restart
141:14 the product service application
141:16 I'm going to restart the order service
141:18 I'm also going to restart the inventory
141:20 service so let's see if we are able to
141:23 see the service name inside the Eureka
141:26 dashboard so all the services are
141:28 already started successfully so I'm
141:29 going to go back to the Eureka dashboard
141:31 and I'm going to refresh this one more
141:34 time and voila you can see the inventory
141:37 the application names the inventory
141:39 service ordered service and the product
141:42 service and as I have the docker
141:45 environment configured so that's why we
141:47 have the host Docker internal the
141:50 hostname is is resolved to has host
141:53 Docker internal so that's not a problem
141:55 now as we saw in the theoretical example
141:58 what I'm going to do is I'm going to
141:60 create multiple instances of the
142:02 inventory service okay so to do that I'm
142:04 going to open IntelliJ
142:06 and first of all instead of running the
142:10 inventory service on Port 8082 if I want
142:13 to run multiple instances I don't want
142:15 to hard code the port number right I
142:17 want to run the inventory service on a
142:20 random port so for this reason I'm going
142:22 to provide the service.port value as 0.
142:25 so what spring boot will do is at the
142:27 time of starting up it will pick one
142:29 random Freeport in your machine and it
142:32 will run the inventory service
142:33 application on that Port right so in
142:36 this way no matter how many services we
142:38 run it's not a problem like it will try
142:40 to take a free port and will run the
142:43 application so I've changed this to Port
142:46 0 and what I'm going to do is I'm going
142:49 to select the inventory service
142:51 application from the drop down in
142:53 IntelliJ and I'm going to click on this
142:56 inventory service application one more
142:57 time and click on edit configurations
142:59 and inside the edit configurations I am
143:02 going to select this option allow
143:04 parallel run so what this will do is it
143:07 will allow us IntelliJ will allow us to
143:09 run multiple instances of the inventory
143:11 service application right so I'm going
143:14 to click on apply click on OK and now
143:17 I'm going to go to the inventory service
143:20 application
143:21 click on the green button here and click
143:24 on debug
143:26 so now another instance of the inventory
143:30 service application will be up and
143:33 running and you can see that it is
143:36 running on Port
143:38 you can find the port somewhere here so
143:41 it's running on the port 62170 right
143:44 previously it was running on Port a28082
143:47 now this new instance is running on Port
143:50 62170 right let's go to our Eureka
143:55 dashboard and refresh
143:57 to check the two instances and there you
144:00 there you can see inventory service to
144:03 availability zones the first one is the
144:06 service running on 8082 and other one is
144:09 running on Port 0 right in this way we
144:12 can display all the instance information
144:14 in the Eureka service dashboard all
144:17 right so now I'm going to open the order
144:19 service and here you can see that we are
144:21 still using the hard-coded version of
144:24 the inventory service details so now I'm
144:27 going to replace this localhost 8082
144:30 with the name of the inventory service
144:32 application so it's going to be
144:34 inventory hyphen service
144:38 right so I have replaced this with the
144:41 inventory service now let's restart our
144:43 order service application so I'm going
144:45 to go to the services right click and
144:47 rerun okay so the order service is
144:50 restarted successfully now let's check
144:53 whether this ordered service is
144:55 configured in Eureka or not so you can
144:58 see that the order service is still
144:60 configured in Eureka
145:02 and I'm going to open the postman client
145:05 usually you have to I suggest you to
145:08 wait for 30 seconds whenever you have
145:10 restarted the service because it takes
145:13 some time for the client to register
145:14 itself in the Eureka server so that's
145:17 the reason you I usually recommend to
145:19 wait for 30 seconds before you start to
145:22 interact with the service okay so now
145:24 I'm going to open the porceland client
145:25 and I'm going to send a request to place
145:27 an order to the order service let's
145:30 click on the send button and you can see
145:31 that we are getting a 500 internal
145:34 server error so let's open the order
145:36 service and see what is the error here
145:39 it says that fail to resolve inventory
145:42 service after nine queries this is
145:45 because
145:46 we have multiple versions of the
145:48 inventory service right so remember in
145:51 the theory session we have multiple
145:53 instances our Discovery server will
145:56 register all the instances and will
145:58 respond with this particular information
146:01 whenever our whenever order service
146:04 requests for the inventory service so in
146:06 our order service all this information
146:07 about the inventory service right it has
146:09 three instances of the inventory service
146:11 so now it's not understanding it's
146:13 confused which instance to call so in
146:16 this instance what it should do is it we
146:18 should go through this list one by one
146:20 and it should try to call the instances
146:23 one by one so to be able to enable this
146:25 feature we have to enable the
146:27 client-side load balancing in our Eureka
146:29 clients so to enable the client-side
146:31 load balancing what we have to do is we
146:33 have to add an annotation
146:35 when constructing the web client when
146:39 constructing the web client pin right so
146:41 for that I am going to open the
146:47 config package and I'm going to open the
146:50 web client config class and here instead
146:53 of creating the bin for web client what
146:55 I'm going to do is I'm going to create a
146:57 bin for webclient DOT Builder I'm going
147:00 to remove the dot build method here and
147:03 just below the bin annotation I am going
147:06 to add the load balanced annotation from
147:09 Spring Cloud client load balancer
147:11 package and what this will do is it will
147:14 add the load balancing client-side load
147:17 balancing capabilities to the web client
147:19 Builder whenever you are creating an
147:21 instance of web client using this web
147:23 client Builder it will automatically
147:25 create the it will automatically create
147:27 the client-side load balancer and it
147:29 will use this client-side load balancing
147:31 to call the inventory service so in this
147:34 way even though our order service finds
147:37 multiple instances of the inventory
147:40 service
147:41 like this example it won't be confused
147:44 and it will just try to call this
147:45 inventory service one after another
147:47 right
147:48 so let's go back to our web client
147:51 config class I'm going to change this
147:53 web client name to web client
147:56 Builder
147:58 and I'm going to open the order service
148:03 and change the Declaration of web client
148:05 to web client Builder
148:07 and the reference variable name to your
148:09 client Builder
148:11 and now I also have to adapt this web
148:14 client reference variable name to web
148:16 client Builder dot build so that we will
148:19 get instance of web client
148:21 and that's all we need to do so what I'm
148:23 going to do now is I'm going to restart
148:26 the order service application rerun in
148:29 debug mode so once the application is
148:32 restarted I'm again going to wait for
148:34 some time for 30 seconds before starting
148:36 to make the request one more time so now
148:40 I'm going to open the postman client
148:43 send the request one more time and now
148:45 you can see that the order is placed
148:47 successfully that means our order
148:50 service is able to successfully load
148:53 balance the inventory service instances
148:54 and made a call to the inventory service
148:58 in this way we can use Eureka server and
149:00 Eureka client to implement the service
149:03 Discovery pattern okay so now what we're
149:05 going to do is we are going to do some
149:07 disruptive tests like we try to break
149:09 some things and test how our system will
149:12 behave right we will try to take down
149:14 the discovery server so what we are
149:17 going to do is we are going to try to
149:19 take down the discovery server that
149:21 means I'm going to stop the discovery
149:23 server and see how our applications and
149:26 how our services are going to behave
149:28 right so I'm going to open first the
149:31 Eureka dashboard you see that it is
149:33 working fine so it's up and running and
149:36 I'm going to open IntelliJ go to
149:38 services and I'm going to stop the
149:40 discovery server application right so so
149:43 let's open the browser one more time and
149:47 restart
149:48 localhost 8761
149:51 site one portal one more time and you
149:54 can see that now the discovery server is
149:56 down right so now let's test whether our
149:59 order service is still able to find the
150:01 inventory service or not right so I'm
150:03 going to click on send one more time and
150:05 you can see that our order is placed
150:07 successfully because if you remember as
150:10 part of the theory session we discussed
150:12 that
150:13 while trying to make a call to the
150:15 Discovery server the client will store a
150:17 local copy of the registry inside the
150:19 local copy of order service we have the
150:22 inventory service information right the
150:24 inventory service is running with port
150:26 8082 and it's also running with port
150:28 62170.
150:30 so now what I'm going to do is I'm going
150:32 to stop these both instances of the
150:34 inventory service
150:36 so I have stopped this instance and I'm
150:39 also going to stop the inventory service
150:41 running on 62170 so again this instance
150:45 is stopped now I'm going to start this
150:48 instance one more time the instance of
150:51 the inventory service application
150:53 and now it should run on a different
150:55 port than 62170 yeah so it's running on
150:58 64432 but it's not able to register it's
151:01 not able to find the discovery server so
151:03 that's why we are receiving some errors
151:04 right like transport exception error so
151:07 now let's see how our order service will
151:10 behave right I'm going to send the
151:12 request one more time
151:14 and you can see that now we are getting
151:15 the 500 error so I'm going to
151:18 open the
151:21 order service application logs and you
151:24 can see that we are getting again the
151:25 transport exception because the order
151:27 service first try to check for the
151:30 inventory service in its local copy and
151:32 it's not able to reach any of these
151:35 Services right so what it did is trying
151:37 to again contact the discovery server to
151:39 fetch the registry information but it
151:41 found out that the discovery server is
151:43 not running and that's why it's throwing
151:44 an error right so now what I'm going to
151:46 do is again
151:48 go back to the Discovery server
151:50 application and I'm going to run this
151:51 discovery server one more time so now
151:54 our Discovery server application is up
151:56 and running I'm going to go back to the
151:58 portal one more time localhost 8761 I'm
152:01 going to refresh and now you can see
152:04 that the the dashboard is up and running
152:06 but still the services are not
152:09 registered in the discovery server so we
152:12 have to wait for 30 seconds until the
152:14 services send the next heartbeat to the
152:16 Eureka server
152:18 so let's wait yeah so after waiting for
152:21 30 seconds you can see that our
152:22 inventory service order service and
152:24 product service are back up one more
152:25 time and now I'm going to open the
152:27 postman client and I'm going to click on
152:30 the send button and now you can see that
152:32 the order is placed successfully
152:34 so you can see that even though we have
152:38 take down the we have taken down the
152:39 discovery server the order service is
152:42 able to call the inventory service
152:43 because it has stored the local copy of
152:46 the registry so this is it for the
152:48 inventory service so in the next video
152:50 in the next part we're going to discuss
152:52 about the next component of the spring
152:54 cloud service that is the API Gateway
152:57 right I will see you in the next video
152:59 Until Then happy coding the case so in
153:01 this part we are going to discuss about
153:03 another component in microservices
153:05 landscape called as API Gateway so this
153:08 is how our present architecture looks
153:11 like right like if the customer if the
153:13 user wants to access the product service
153:15 we are now calling some arbit report
153:17 where the product service is running on
153:20 so we are making a call to localhost
153:22 this some random Port API slash product
153:25 to access the product service similarly
153:28 to access the order service we are
153:30 calling another port and calling the
153:33 request and making a request to slash
153:35 API order this is fine for a Dev
153:37 environment but on a productive
153:38 environment this will not work right as
153:41 we have already discussed before the
153:42 micro Services environment can have
153:44 multiple instances and the application
153:46 can run on different ports also right so
153:49 for this reason we cannot rely on hard
153:51 coding these values when we want to call
153:54 the services so the solution for this is
153:56 to introduce a component at the start of
153:59 our architectural landscape called as a
154:02 API Gateway which is responsible to
154:04 Route the requests from users to the
154:06 corresponding services so let me scroll
154:09 down and yeah this is what it looks like
154:12 so we will have an API Gateway which
154:14 acts like an entry point into our system
154:17 landscape right and if the user wants to
154:20 call the product service the user just
154:23 calls the normal API Gateway with the
154:26 normal URL like it can be online
154:28 shop.com right and to be able to call
154:31 the product service all you have to do
154:33 is call online shop.com API slash
154:36 product and then in our API Gateway we
154:39 can configure the rules such that if we
154:42 receive a request with the URL parameter
154:45 AS Slash API slash product then route it
154:48 to product service and if the user wants
154:51 to call the order service based on the
154:53 request URL AS Slash API slash order we
154:56 can route this to order service and also
154:58 similarly if you want to access a
155:01 discovery server behind the API Gateway
155:03 you can also configure a rule such that
155:05 if it contains slash Eureka in the in
155:08 the URL then please route this request
155:11 to the Discovery server right so this
155:13 API Gateway will as the name suggests
155:15 acts like a gatekeeper to whatever
155:17 request which the users wants to make to
155:20 our services so not only that it can
155:22 also address some additional
155:24 cross-cutting concerns in the
155:26 microservice landscape like
155:28 authentication so if you want to make
155:30 sure that all the requests are
155:32 authenticated we can configure that in
155:34 API Gateway okay so what it can API get
155:36 we can do is it can contact the
155:38 authorization server or whatever
155:39 authentication mechanism you want to
155:41 implement so it can also take care of
155:43 authentication and security so that's
155:45 how we also going to implement in this
155:48 project we will make our API Gateway
155:50 talk to the key cloak authorization
155:52 server and will implement the security
155:54 aspects apart from that it can also act
155:56 as a load balancer so if we have
155:59 multiple instances of product service
156:00 and if the user makes a request to the
156:03 product service it will make sure which
156:05 instance of the product service to call
156:07 and it will retrieve the response from
156:09 that instance and send it back to the
156:11 user right apart from that it also tries
156:15 to implement the SSL termination so
156:18 usually if you are making a call to any
156:19 external service sure to include the
156:21 https scheme there by following the TLs
156:25 protocol so if we call the API Gateway
156:27 from the outside with the TLs scheme
156:29 with https protocol the API Gateway as
156:32 it's already part of the micro services
156:34 Network right we don't need https here
156:37 as this is completely internal
156:39 communication so we can just we can just
156:42 perform HTTP communication without https
156:44 so that's why usually the SSR connection
156:47 will be terminated at the time of at the
156:49 entry point of the request right so so
156:52 this will be terminated at the API
156:55 Gateway level this is also implemented
156:57 for us automatically by all the API
156:59 gateways so in the market we have
157:01 different kinds of API gateways we have
157:03 one popular API Gateway called as APG
157:05 and the one is a Kong API Gateway but in
157:08 this project we are going to make use of
157:10 spring spring Cloud's own implementation
157:13 of API Gateway called as Sprint Cloud
157:15 Gateway so now let's go ahead and have a
157:18 look at how we can implement the API
157:20 Gateway in our project to understand
157:22 more details about spring Cloud Gateway
157:24 I am going to open the spring Cloud code
157:26 documentation by opening the spring.io
157:29 projects spinning Cloud URL and in here
157:32 I am going to search for spring cloud
157:34 Gateway
157:35 and you can read more about what is
157:38 spring Cloud Gateway here and here these
157:40 are some features listed for the spring
157:42 Cloud Gateway it's built on Spring
157:44 framework 5 project reactor and Screen
157:47 boot 2. so this is a very important
157:49 thing it's not built on top of spring
157:51 webmdc but it's on built on top of
157:54 spring where flux which is built on top
157:56 of project reactor and we also have
157:58 different features to able to match
157:60 routes on any request attribute
158:01 predicates and filters which are
158:04 specific to Roots circuit breaker
158:06 integration you will have a look at it
158:07 in the coming parts we can also have
158:10 integration with the spring Cloud
158:12 Discovery client and we can also have
158:15 the code for request rate limiting and
158:17 URL path rewriting right we can have a
158:19 look at some of these features in this
158:21 tutorial so to get started I'm going to
158:23 open
158:25 star.spring.io website and we need to
158:27 first get some dependency information so
158:29 once we get this information we can
158:31 create the API Gateway for ourselves so
158:33 for that I'm just going to click on ADD
158:35 dependencies
158:36 and I'm going to search for Gateway
158:40 so I'm going to select this dependency
158:43 I'm going to click on explore
158:46 look for the palm.xml and here I'm going
158:50 to just copy the dependency information
158:52 here called a spring Cloud starter
158:55 Gateway just make sure to copy this and
158:57 now I'm going to open my IntelliJ ID and
159:00 what I'm going to do is I'm going to
159:02 create a new Maven module under the root
159:05 Maven project so I'm going to click on
159:08 new module
159:10 and make sure to select the option Maven
159:12 here and click on next
159:14 and I'm going to call this project this
159:16 Maven module has
159:19 API Gateway
159:20 click on finish and now inside the
159:23 form.xml of API Gateway I am going to
159:26 add a new line and I'm going to type in
159:29 dependencies and I'm going to paste in
159:31 the dependency information from the
159:34 which I copied from the start.spring.io
159:37 website I'm going to force Maven to
159:39 download this dependencies so for that
159:40 I'm just going to click on this load
159:42 Maven changes button so so IntelliJ will
159:46 take some time to download the
159:47 dependencies so once this is completed I
159:49 am going to open the API Gateway folder
159:52 so go to Source main Java and I'm going
159:56 to create first of all a new package
159:58 called as com
160:00 programming technique
160:04 dot API Gateway and I'm also going to
160:08 create a new class called as API Gateway
160:14 application so this is going to be a
160:17 spring boot application and I'm going to
160:20 again add the public static void main
160:23 method by wrapping spring application
160:25 dot run I'm going to pass in the API
160:28 Gateway application.class name and
160:31 finally I'm going to provide the
160:32 arguments here this so we have added the
160:35 dependency for API Gateway we have
160:38 configured the main application class
160:40 and the next thing we are going to do
160:42 here is to add the dependency for the
160:46 Discovery client right because as our
160:49 API Gateway is also one of the pro and
160:51 one of the component inside the
160:53 microservice landscape will also
160:54 register the API Gateway as a discovery
160:57 client right so for that I'm going to
160:59 just make go open the pom.xml of product
161:03 service and I'm just going to copy the
161:05 dependency information of the
161:07 spring Cloud starter Netflix Eureka
161:10 client and I'm going to paste it inside
161:12 the form.xml of API Gateway and make
161:16 sure to again reload the maven changes
161:19 by by clicking on the maven icon and now
161:21 I'm going back to the API Gateway
161:22 application and I am going to add enable
161:26 Eureka client annotation so that the
161:28 Lyrica client features will be will be
161:31 activated inside our API Gateway and
161:33 after that what I have to do is I'm
161:34 going to create a new
161:37 file inside the resources folder
161:40 called AS application Dot
161:45 properties
161:47 all right so in here in this
161:48 application.properties file so first of
161:50 all I'm going to define the Eureka
161:53 server URL by typing Eureka dot client
161:57 dot
161:59 serviceurl dot default Zone equals http
162:05 localhost 8761
162:09 slash Eureka right so this is the
162:12 service URL for the Eureka server so
162:15 that the API Gateway can make a call to
162:17 this particular URL and register itself
162:19 as a discovery client right and after
162:22 that I am going to type in the app
162:23 spring dot application.name property as
162:27 API Gateway
162:29 just to have some additional information
162:31 I am going to add the I am going to
162:35 increase the logging level of the API
162:36 Gateway application so I am going to add
162:39 login dot level dot root is equal to
162:47 info and I'm going to type in logging
162:50 dot level dot org
162:53 Dot
162:55 springframework.cloud Dot
162:59 gateway.rout
163:01 the drought definition locator so this
163:04 is the class which is uh responsible to
163:07 define the routes and also identify the
163:10 routes for the incoming requests so I'm
163:12 going to put this logging level
163:14 information as info
163:17 and finally I'm just going to copy all
163:19 the
163:20 information here until the cloud.gateway
163:24 and for the cloud.gateway package I am
163:27 going to define the login level as Trace
163:29 right so by adding this information we
163:31 are going to have some mode locks and we
163:33 can understand what is going on behind
163:36 the scenes whenever a request is made to
163:38 the API Gateway right and after this
163:41 configuration all we have to do now is
163:43 Define the routes
163:45 so if I open our architecture diagram
163:48 one more time whenever the user wants to
163:50 make a call to the product service we
163:52 can make sure we can understand that it
163:54 should go to the product service by
163:56 inspecting the URL right so if the URL
163:60 contains slash API product then we have
164:02 to Route it to product service similarly
164:04 if the URL contains slash API slash
164:06 order we have to dot it to the order
164:08 service right we have doing this based
164:11 on some certain rules so we can Define
164:13 these rules inside uh API Gateway as
164:16 routes right if I open the documentation
164:19 of spring Cloud Gateway on the learn and
164:22 by opening the reference documentation
164:24 you can read about spring Cloud Gateway
164:27 in much more detail about what I can
164:29 cover in this tutorial the glossary
164:31 section here you can see how everything
164:33 works we have routes which is like a
164:36 building block of Gateway so each URL we
164:39 want to route through the API Gateway we
164:41 will create a corresponding route and we
164:44 can have a predicate as well as a filter
164:46 like a predicated lets you define the
164:49 matching criterias for all the HTTP
164:51 requests and lastly we also have filters
164:54 where you can modify the requests which
164:56 are coming into our real chart coming
164:59 into the API Gateway so we can we can we
165:02 can modify the request as well as the
165:04 response that means you can add
165:05 additional URL parameters and you can
165:08 also add some additional headers to the
165:11 response when we are trying to process
165:13 the request and response information so
165:16 now what we are going to do is we are
165:17 going to define a route inside our
165:20 project so I can define a route so here
165:23 what I'm going to do is I want to define
165:26 a route for the product service so I'm
165:27 going to type a comment
165:29 called as product service route and to
165:32 define the route I'm going to type in
165:34 Spring dot Cloud dot Gateway dot routes
165:39 we can Define some some list of routes
165:41 here so I'm going to create a number of
165:43 multiple routes so for that I'm just
165:45 going to add the index as 0 and for each
165:48 route I can define an ID right so an ID
165:51 is going to be product service usually
165:54 it would be the name of the application
165:56 name of the service which you are
165:57 defining and after the ID we can also
166:00 Define the URI which which we want to
166:02 map to this particular service so this
166:05 is going to be http
166:07 product service API Gateway also acts
166:11 like a load balancer you can also ask
166:13 IPA gateway to load balance this request
166:16 by typing lb so in this way the API
166:19 Gateway will also do some client-side
166:21 load balancing and it will first and it
166:24 will try to access the available
166:25 instance of the product service right
166:28 and after this lastly we are going to
166:31 define the URL parameter which should
166:34 match this particular service so for
166:36 that I am going to type in Spring Cloud
166:38 Gateway dot routes of
166:42 0 dot predicate so we can pass multiple
166:45 predicates to the route so we're going
166:48 to Define only one predicate this
166:50 predicate is going to be a predicate of
166:52 type path right I'm so I'm going to type
166:55 path equals slash API slash product
166:60 so whenever we receive a request with
167:02 path predicate AS Slash API product then
167:06 our API Gateway will route this
167:08 particular request to the product
167:10 service so this is what this actual
167:13 three lines means so in this way we can
167:16 also Define multiple routes for our we
167:18 can also Define the route for order
167:20 service so for that I'm just going to
167:22 copy this whole information one more
167:25 time I'm going to paste this and rename
167:27 the product service route to order
167:29 service route and this is a second route
167:32 we are defining inside our application
167:34 properties file so I'm going to change
167:37 the index one zero to one
167:41 and I'm going to change the
167:44 ID from product service to order service
167:47 also going to change the url from our
167:50 product service to order service and
167:52 lastly I'm also going to change the path
167:54 from slash API product to slash API
167:58 order so if you want to know more
167:60 information about the kind of predicates
168:02 we have you can refer to the
168:04 documentation section inside the spring
168:06 Cloud Gateway so if I open the route
168:09 predicate factories section you can see
168:11 different kinds of routes so so we can
168:14 define an after predicate a before
168:16 predicate
168:17 you can Define the predicate based on
168:20 the cookie if it contains some attribute
168:23 inside a cookie and we can also Define a
168:26 predicate based on the method so if you
168:29 want to just handle the post request for
168:32 a particular service you can also Define
168:33 the predicate as a method here and we
168:36 have the path predicate where we have
168:39 used So based on the URL path we are now
168:42 Define the we are defining the routing
168:44 scheme so we have different kinds of
168:46 routes so just make sure to check this
168:48 documentation section if you want to
168:50 have if you want to do something
168:52 differently so now what I'm going to do
168:54 is I'm going to open the API Gateway
168:56 application and let's try to run our API
168:59 Gateway application
169:00 all right so we are getting uh an error
169:03 whenever we are starting when we're
169:05 starting up the API Gateway application
169:06 it says web server fail to start put
169:09 8080 was already in use so we are
169:12 running the product service application
169:14 on port 8080 and the API Gateway
169:17 application also uses sport 8080 by
169:20 default
169:20 so what I'm going to do now is change
169:23 the port of the product service so for
169:26 that I'm going to open the
169:28 application.properties file and in here
169:30 I am going to type in server.put equals
169:33 0 so that it will take a random port
169:36 uh available to the machine so I am
169:40 going to restart the product service
169:42 rerun in debug mode
169:44 and also start the application API
169:48 Gateway application one more time in the
169:50 debug mode so now you can see that the
169:52 product service application is started
169:55 on Port 61795 and the API Gateway
169:59 application is able to start
170:00 successfully on the port 8080 and here
170:03 and now let's open our
170:06 Eureka server and check whether we are
170:08 able to see this API Gateway in the
170:12 dashboard or not open the Eureka server
170:14 localhost 80761 and just I'm going to
170:16 restart refresh the dashboard
170:19 and you can see the API Gateway is
170:22 registered in our Eureka server as a
170:24 client so that's perfect so what I'm
170:27 going to do now is make a request to the
170:29 product service through the API Gateway
170:31 so for that I am going to type in http
170:35 localhost
170:36 8080
170:38 slash kpi
170:39 slash product
170:42 and I'm going to make a get request so
170:44 I'm just going to click on send
170:46 and you can see that we are receiving
170:48 the response back from the product
170:50 service so it's not like your product
170:52 service is running on port 8080 it's
170:55 running on Port 61795 so let's test
170:58 whether the application the product
171:00 service is really running on the
171:03 61795 port or not so I'm just going to
171:05 copy this URL create a new tab and
171:09 change the port from 61795
171:13 from 8080 to 61795 and make a request
171:16 one more time and you can see that you
171:18 are still getting the response so that
171:20 means the request went through to the
171:23 API Gateway and reached the product
171:25 service so we can also verify this
171:28 inside the logs so I'm going to open the
171:30 logs and of the API Gateway application
171:34 and if you just scroll down you can see
171:37 some detailed information about the logs
171:39 so we
171:42 can see that the pattern API slash
171:45 product matches against value API
171:47 product that means which is defined for
171:49 the product service and you can see that
171:51 in the log says the route match is for
171:53 product service it's trying to make a
171:55 get call to this product service you can
171:58 see much more information about the logs
172:01 how it is matched how the routes are
172:04 matched and you can get a good
172:05 information and you can get good idea of
172:08 how the API Gateway is working behind
172:10 the scenes by enabling the logs right so
172:14 let's also test the whether the API
172:17 Gateway is working fine for the order
172:20 service or not so I'm going to open a
172:22 new tab change the post method to change
172:26 the HTTP method to post
172:28 and I'm going to type in HTTP localhost
172:31 8080 slash API slash order and I'm going
172:34 to just paste the request body which I
172:37 have used before so now I'm going to
172:39 click on send
172:41 and you can see that we have received we
172:44 received the response order placed
172:45 successfully so I am going to open the
172:48 API Gateway one more time and here you
172:52 can see that we see the logs that a
172:55 route is matched for the order service
172:56 and it's trying today and it's trying to
172:59 make a post call to the slash API slash
173:02 order endpoint
173:04 so in this way we can Define the routes
173:07 inside the API Gateway so in the next
173:09 section what we are going to do is we
173:11 are going to also access the Eureka
173:14 server so to access the Eureka server
173:17 right now we are going to the slab
173:19 localhost 8761 Port so what I'm going to
173:23 do now is also access try to access the
173:26 Eureka server through the API Gateway
173:28 right so for that what I can do is I can
173:31 open the application.properties file of
173:34 API Gateway
173:36 and I can Define another route
173:39 for the discovery server so I'm going to
173:41 copy this route information one more
173:43 time and change the comment as discovery
173:47 server route and the ID I'm going to
173:51 Define as discovery
173:53 server
173:54 and for the discovery server I am going
173:58 to define the URL as
174:01 localhost
174:04 8761 and the path is going to be not
174:08 slash API order but I am going to access
174:11 the Eureka server in a pot slash Eureka
174:16 slash web right as of now we are calling
174:20 8761 without any additional URL
174:23 parameters so what I'm going to do now
174:25 is I want to access Eureka server at
174:28 HTTP localhost 880 slash Eureka
174:33 slash web right whenever by I call this
174:36 particular URL I want my request to be
174:39 routed to the Utica server right so
174:42 that's why I added the predicate path
174:44 predicate AS Slash Eureka slash web I'm
174:47 going to also change the index to 2
174:50 instead of 1.
174:53 right so now I'm going to restart the
174:57 API Gateway application so I'm going to
174:60 now restart rerun API Gateway
175:02 application and so our API Gateway is
175:04 restarted successfully so now I'm going
175:07 to open the browser and now I'm going to
175:10 type in localhost 8080 slash Eureka
175:12 slash web
175:14 and I'm going to
175:16 if I press enter we are getting a 503
175:19 error this is because what our API
175:22 Gateway is trying to do is it's trying
175:24 to call localhost
175:26 8761 slash Eureka slash web but there is
175:30 no route in the Eureka server for the
175:33 path Eureka slash Eureka slash web right
175:35 so what the API Gateway is trying to do
175:37 is it's trying to pass in this URL
175:40 parameters also to the Eureka server
175:43 right as there is a no route for Eureka
175:46 server for this ulica slash web is we
175:48 are getting a 503 error right so
175:51 whatever API Gateway should ideally do
175:53 is whenever it receives a request
175:57 at localhost 8080 slash Eureka slash web
176:00 you should call
176:02 the Eureka server at localhost 8761
176:05 right you should make a call without the
176:07 URL parameters right so how can we do
176:10 that so previously I mentioned that we
176:13 can modify the requests in our API
176:16 Gateway with the use of filters right so
176:18 we can make use of filters to actually
176:21 remove the URL parameter and you can
176:23 change the path of the your path of a
176:27 request right so let's do that I'm going
176:30 to open the application.properties one
176:32 more time so what I'm going to do is I
176:34 am going to define a
176:36 filter for this particular route so for
176:39 that I am going to type in Spring Cloud
176:41 Gateway dot routes of two
176:45 dot filters so I'm going to define a
176:49 filter so this filter is going to be
176:52 called as set path
176:55 and the set path as the name suggests it
176:57 will try to change the path of the URL
177:00 request so path of the path of the URL
177:03 so so instead of Slash Eureka slash web
177:06 I'm going to define the URL path as just
177:09 slash right so whenever we make a call
177:12 to the API Gateway with Slash Eureka
177:15 slash web it will just
177:17 it will just forward this request to
177:19 http localhost 8761 without any URL
177:23 parameters so after we have defined this
177:26 filter I am going to again restart the
177:29 application
177:30 so after the application Gateway so
177:33 after the API Gateway is restarted I am
177:35 going to open
177:36 the browser one more time and now I'm
177:39 going to type in slash 8080 slash Eureka
177:42 slash web
177:44 okay so we are still getting the 503
177:46 error so let's open the logs and check
177:50 what may be the reason here so if you
177:54 see that we have
177:57 so if You observe the logs that the
177:60 discovery server route definition is
178:02 matched successfully but you can see
178:04 that inside the round robin load
178:06 balancer class it says that no servers
178:09 available for service localhost right
178:12 because as we only have one instance of
178:15 the discovery server it's not able to
178:17 actually load balance the request to the
178:20 Discovery server yes because as you have
178:22 defined the text LB we have wanted to
178:26 also load balance the request to the
178:28 Discovery server this is not working as
178:31 there is only one instance so here what
178:33 I can do is instead of putting lb I can
178:35 put it back as http
178:38 and let's try to check if it is working
178:40 fine or not so I'm just going to restart
178:42 the API Gateway One More Time rerun in
178:45 the debug mode
178:47 and let's now test whether this is
178:49 working fine or not so now if I open the
178:53 browser and go to localhost 8080 slash
178:55 Eureka slash web
178:58 you can see that we are able to access
179:01 the Eureka server through the API
179:03 Gateway but we are getting a plain HTML
179:07 instead of the CSS and the other static
179:10 resources because we defined the route
179:13 only for the Eureka server but not the
179:16 web static web resources right it's not
179:19 only should not it should not only load
179:21 the HTML but also some other static
179:23 resources like CSS and JavaScript files
179:26 so for that we have to Define another
179:27 route
179:29 for the Eureka server just for the
179:31 static resources right so what I'm going
179:33 to do is I am going to copy this route
179:37 one more time and I'm going to Define
179:39 this route as Discovery server
179:44 static resources route
179:47 right so I'm going to change the index
179:50 from 2 to 3
179:53 and change the ID as Discovery server
179:56 static
179:58 and here I don't need the filter anymore
180:01 but instead of the path slash Eureka
180:03 slash web what I can do is I can just
180:06 add
180:07 double Stars so whatever request which
180:11 is made from the path slash Eureka slash
180:14 star it will be routed to the Discovery
180:18 server so whatever request we make on
180:20 the path slash Eureka slash star it will
180:23 be routed to the Discovery server right
180:25 so I'm just going to again restart the
180:29 HIPAA Gateway application and once the
180:31 API Gateway is up and running I am going
180:34 to open the web browser one more time
180:36 and I'm going to refresh and now you can
180:39 see that the CSS is able to load
180:42 successfully and we are able to access
180:45 also the discovery server through the
180:47 API Gateway that's it from this tutorial
180:49 guys and in the next part we are going
180:52 to secure our microservices by using
180:55 keyclock so we are going to implement
180:58 the security through the API Gateway so
181:00 I will see you in the next video Until
181:01 Then happy coding the keys
181:03 so now let's talk about securing our
181:06 micro Services architecture we have an
181:08 API Gateway in place and we also have
181:10 our services the product service order
181:12 service and the inventory services so
181:14 now what we want to do is we want to
181:15 secure this Services as of now you can
181:19 access this any of the services any of
181:21 the API endpoints without any
181:23 authentication we would like to
181:25 introduce this authentication kind some
181:26 kind of authentication mechanism to be
181:29 able to access the service endpoints
181:31 right so for that we are going to make
181:33 use of an authorization server called
181:35 ASCII clock so using key clock we can
181:38 Outsource our authentication and
181:40 authorization related
181:41 configurations to geek log and we don't
181:44 need to implement the authentication
181:46 mechanism by hand in our microservices
181:49 so before going ahead and learning more
181:52 about how to configure keyclock in our
181:54 project I would like to recommend you to
181:56 watch the Spring Security auth to key
181:59 cloak video before going ahead because
182:02 I've explained all the watch two related
182:04 Concepts in that video and I'm not going
182:07 to repeat those Concepts one more time
182:09 so now let's go ahead and install key
182:11 clock in our machine so for that I open
182:13 the keyclock.org website and in here I'm
182:17 going to click on the section guides and
182:20 I'm going to in the getting started
182:22 section I'm going to click on Docker so
182:25 again to mainly use a Docker image to
182:27 run keyclock I'm not going to install
182:29 this manually so I'm just going to copy
182:32 the docker run command here and I'm
182:34 going to open my IDE and open the
182:37 terminal section you can also open you
182:40 can also run it on your terminal I'm
182:42 going to copy this command Docker run
182:44 command where we are running the key
182:47 clock image on port 8080 and we are
182:51 exposing the port 8080 from the
182:53 container and we are providing the
182:56 environment variables key clock admin
182:57 user as admin admit password as admin
183:00 followed by the image name so this is
183:03 the kawaii.io keyclock 18.0 this is the
183:06 image name and we are running on a Dev
183:09 profile so that's why we have the start
183:10 Dev
183:11 command here before running this command
183:14 I am going to make a small change here
183:16 key clock is by default running on port
183:18 8080 inside the container you want to
183:20 access this key clock not on port 8080
183:23 on our host machine but on a different
183:25 port because on port 8080 we have our
183:28 API Gateway running right so for this
183:30 reason I am going to change the port
183:33 8080 to 8181 so that we can access the
183:37 geek look on Port 8181 and finally I'm
183:41 going to press enter what this will do
183:44 is it will first check whether your
183:46 whether the key cloak image is present
183:49 in your Docker registry or not if not it
183:51 will first try to download the docker
183:54 image and after that it will start the
183:56 container so once the key clock server
183:58 is up and running I am going to open the
184:01 browser and go to the URL http localhost
184:05 8181
184:08 8181 so this will open key clock in our
184:11 browser so what I'm going to do is I'm
184:13 going to click on the administration
184:15 console link this will take us to a
184:17 login page here I am going to type in
184:19 the username and password we have
184:21 provided while running the container so
184:23 that would be admin and admin
184:26 so here we are taken to the home page of
184:28 key clock and in Click look we have some
184:30 concept called as realm where you can
184:33 group all your clients your oauth
184:35 clients into a single logical entity
184:38 called as a realm so by default Key Club
184:41 provides us a master realm and we can
184:44 also create our own realm so what I'm
184:45 going to do is I'm going to click on add
184:47 RAM and I'm going to provide the realm
184:50 name as
184:51 spring boot micro services
184:55 Rel
184:56 and I'm going to click on OK
184:59 and in this Ram we can create our
185:01 clients our oauth to clients so for that
185:03 I am going to click on the clients
185:05 section here and I'm going to click on
185:08 the create button
185:09 and in here I am going to provide client
185:12 ID as spring Cloud client and I'm going
185:16 to click on Save
185:17 this will take us to a different another
185:20 screen where we have many fields and in
185:23 here we can configure what kind of
185:24 client we can have so by default we can
185:27 have different kinds of client we can
185:29 have a
185:30 client up we can have a public client we
185:33 can have a confidential client I can we
185:34 can have a bearer only client so for
185:37 this project we are going to select the
185:39 confidential client because as we are
185:41 not going to make use of any public many
185:44 websites to make the to make the
185:47 authentication we are going to call we
185:49 are going to call the services through
185:51 Postman client so for that it's enough
185:53 to select the access type as
185:55 confidential again if you're feeling
185:56 lost have a look at my Spring Security
185:58 or through key clock video I have
186:00 explained all these details what is a
186:02 what is an access type what is a client
186:04 what is a resource all the concepts I
186:07 have explained in that video in detail I
186:09 am going to so I have selected the
186:11 access type as confidential and here we
186:13 have one option called standard flow
186:16 enable I am going to disable this option
186:17 I'm also going to disable the DirectX
186:20 granted enabled option and I'm going to
186:22 select the service accounts enabled
186:24 option so what this will do is it will
186:26 enable the client credentials Grant and
186:29 after enabling this I am just going
186:30 click on save this will create a client
186:34 secret we can see the client Secret by
186:37 going to the credentials Tab and I can
186:39 just copy this client secret and I can
186:41 come back to this client secret whenever
186:44 I want to authenticate using this client
186:45 secret so after configuring key cloak
186:48 what we have to do is we have to provide
186:50 the URL of keyclock in our API Gateway
186:53 so for that I'm going to go to the realm
186:56 settings and click on open ID
186:58 configuration and in here I am going to
187:01 copy the issuer ID the object of type
187:05 issuer so this would be HTTP localhost
187:08 8181 slash Realms slash spring boot
187:12 microservice right I'm going to copy
187:14 this uh this URL and I have to provide
187:18 this URL inside our API Gateway because
187:20 we are going to configure our API
187:22 Gateway to talk to the key closer right
187:24 as we have discussed before so the next
187:26 step is to configure our API Gateway as
187:29 a spring over to Resource server and to
187:32 also enable the spring security so that
187:34 we can configure the API Gateway with
187:37 geek look details so for that I am going
187:39 to open the pom.xml of my API Gateway
187:42 project and just below the dependency of
187:46 spring Cloud starter networks Eureka
187:48 client I am going to add another
187:49 dependency with group ID as org
187:52 springframework.boot
187:55 and the artifact ID
187:57 is going to be spring boot
187:60 starter oauth 2 resource server
188:04 and I'm going to remove the version DAC
188:06 because I want the same version as the
188:09 parent I am going to copy this
188:10 dependency one more time
188:12 and change the artifact ID from Spring
188:15 boot starter over to Resource server
188:18 to Spring boot starter security and to
188:22 enable IntelliJ to download the maven
188:24 dependencies I'm going to click on this
188:25 Maven icon
188:27 and once the dependencies are downloaded
188:31 you have your all set to go ahead and
188:34 configure the issuer URI in your API
188:38 Gateway application so for that I'm
188:40 going to open source main resources
188:42 application.properties file and below
188:44 the
188:46 routes configuration I'm going to type
188:48 in Spring dot security code to
188:52 The Source server dot JWT Dot issueruri
188:56 right and this issue or URI we have
188:59 copied previously in the from the
189:02 discovery document so from this URL this
189:05 spring boot microservices well known
189:08 open ID configuration from there I'm
189:10 just going to copy this issue or URI and
189:13 I'm going to paste it in here just a
189:16 small overview of how spring how this
189:19 will work spring boot at the time of
189:20 starting up the application will read
189:22 this the discovery document we'll read
189:25 this discovery document and from this
189:27 discovery document it will fetch all the
189:29 endpoints to authorize
189:31 to to all the endpoints to do the
189:34 authorization so we have this
189:35 authorization endpoint as spring boot
189:38 micro Service as well and protocol
189:40 openid connect auth and we have a token
189:42 ID token endpoint where springboot can
189:45 make a call to this particular endpoint
189:47 and verify whether a token is valid or
189:50 not so instead of configuring all these
189:52 values in our application.properties
189:54 what we can do is we can just configure
189:55 the issuer URI and spring boot will do
189:58 the work behind behind the scenes for us
189:60 by reading the open ID configuration and
190:02 will receive all the information it
190:04 needs so the next thing I'm going to do
190:07 is to create a security configuration
190:09 class inside our API Gateway so I'm
190:11 going to click on new class
190:14 and instead of creating a package I will
190:17 type in the package name as
190:20 config.securityconfig
190:23 this is one of the shortcuts which was
190:26 suggested to me by one of the
190:27 subscribers so thanks for the hint
190:30 always nice to learn new shortcuts so
190:32 once we have created the security config
190:34 class I am going to add the
190:37 configuration annotation and we want to
190:39 enable the web security for the web flux
190:42 project right so for that I am going to
190:44 type in
190:45 enable web flux
190:49 security so we have added enable web
190:51 flux security this is because we are if
190:54 you remember the API Gateway spring
190:56 Cloud Gateway project is based on Spring
190:59 web flux project but not spring webmdc
191:02 so that's why we need the enable with
191:04 flux security annotation here so the
191:06 next thing I'm going to do is to create
191:08 a bin inside the security configuration
191:11 class called as Spring Security filter
191:13 chain so for that I am going to create a
191:15 bean and this beam is going to return of
191:19 is going to be of type Spring Security
191:20 with Filter chain and I'm going to type
191:23 in Spring Security
191:25 filter chain
191:30 and this method is going to take a
191:32 method parameter of type server HTTP
191:36 security and inside this method now we
191:38 have to configure the webflux security
191:41 details so for that I am going to first
191:44 what I'm going to do is I'm going to
191:46 first disable csrf because as we are
191:49 only communicating through the rest API
191:53 through the postman client
191:54 so I am going to disable the csrf here
191:57 and I'm going to add some exchange
191:59 information here by typing authorize
192:02 exchange the second option where I can
192:05 provide the exchange information so I am
192:08 going to type in exchange and create a
192:10 Lambda and in here I'm going to provide
192:13 some more details of how web webflux
192:17 should handle the security so first of
192:19 all I'm going to add some path matches
192:22 here so pass methods is going to be for
192:26 the
192:27 but Eureka slash star so we if you
192:30 remember in the last tutorial we have
192:32 created for the the route configuration
192:34 for the Eureka discoveries static
192:37 resources so whenever we are accessing
192:40 the static resources we want this calls
192:42 to be not authenticated right so
192:45 whenever we're retrieving the CSS and
192:46 JavaScript files we don't want to send
192:48 some kind of token to with it right so
192:51 for that we need to exclude this call
192:53 from our security configuration so
192:55 that's why we are adding the path
192:57 matches to this this is URL pattern and
192:60 we are going to permit all the calls to
193:02 this URL pattern by typing DOT permit
193:05 all and after that any exchange for any
193:08 exchange for any access we want all the
193:11 calls to be authenticated right for only
193:14 the Eureka Statics resources we want to
193:16 permit all the calls but any other calls
193:19 we want to make sure that those are
193:21 authenticated right and after adding
193:23 this information we are also going to
193:25 enable the resource server capabilities
193:27 in our API Gateway so I am going to type
193:30 dot oauth 2 resource server and this
193:34 resource server capabilities we also
193:35 need to enable the Json web token
193:38 capabilities so for that I am going to
193:39 type in server HTTP security
193:42 dot who wants to Resource server spec
193:45 and in here I am going to access the
193:48 method JWT from this class so for that I
193:51 am going to make use of java 8 method
193:54 reference so I'm going to type two
193:56 columns and then select the method JWT
193:59 in here and after that I am going to
194:03 type in server httpsecurity dot build so
194:07 that this will create a object of type
194:09 security web filter chain because this
194:12 is what we want to we are expecting out
194:14 of this method and finally I'm also
194:16 going to add the return statement to
194:18 this to this method so now we have
194:20 enabled security configuration in our
194:23 API Gateway and we have also configured
194:25 our API Gateway to talk to the to the
194:27 geek look to authorization server so as
194:31 you may observe this is very simple to
194:33 configure the gateway to talk to the key
194:35 clock just need to add the dependency
194:37 for the resource server and you need to
194:39 add a property file for the issuer URI
194:43 and everything will happen in the
194:45 background all our routes will be
194:47 protected by Spring Security along with
194:50 key clock so let's test whether our all
194:52 the endpoints are really secured or not
194:54 so for that I just restarted the API
194:57 Gateway application you also make sure
194:58 to restart the application before the
195:00 test so I am going to open the Postman
195:04 client I am going to create a new tab
195:06 and I'm going to test whether the
195:08 product service is still accessible or
195:11 not
195:12 so I'm going to type in localhost 8080
195:15 slash API slash order
195:19 so I'm going to select the HTTP method
195:22 as get and I'm going to click on the
195:23 send button and now you can see that we
195:25 are getting a 401 unauthorized error
195:28 right because we didn't provide any any
195:31 authentication details so in this case
195:32 we have to provide a bearer token a JWT
195:35 token so for that what we have to do is
195:38 we have to create the token we have to
195:39 request the token first of all from Key
195:42 clock by providing our credentials and
195:45 then we have to provide those
195:46 credentials to the API Gateway through
195:49 the authorization header in the the type
195:52 of Bearer schemes right so for that I
195:55 can click on the authorization Tab and
195:58 in Postman we have a nice functionality
195:59 to directly contact keyclock and get the
196:02 token so for that in the authorization
196:04 type I am going to click on the drop
196:06 down menu and I'm going to select the
196:09 type as watch 2.0 and in here we can
196:12 provide the information about our
196:13 keyclock client we have configured a
196:15 while ago so in the configurations
196:17 options we can provide a name uh any
196:20 renamed for the token you are going to
196:22 create the grant type I'm going to
196:24 select as client credentials because the
196:27 client's credentials is the correct
196:29 Grant type whenever we want to have a
196:31 machine to machine communication and in
196:33 the access token URL we have to provide
196:35 the endpoint of the token endpoint which
196:38 we have seen in the in the discovery
196:40 document so I'm going to open this one
196:42 more time and yeah so if you remember we
196:45 are open this open ID configuration URL
196:48 and in here I'm just going to copy the
196:50 value for the token endpoint which is
196:53 HTTP localhost 8181 slash Realms
196:57 springboot microservice as well slash
196:60 protocol slash open ID connect slash
197:03 token right so I'm going to just copy
197:06 this and paste it in the
197:08 section of access token URL so I'm going
197:11 to just paste it again just and the
197:13 client ID I am going to provide it as
197:15 the client ID we have created and the
197:18 client secret I'm also going to provide
197:20 the client secret which I will get from
197:22 my key clock realm so let me open the
197:24 quick look quarterly one more time so I
197:27 have to login to login to my eclog
197:29 account one more time so let me log in
197:31 with the credentials admin and now once
197:34 I'm logged into the key cloak make sure
197:36 that I am in the micro Services realm
197:38 spring boot micro Services Ram which
197:39 I've created before and I'm going to
197:42 click on clear clients and click on
197:44 Spring Cloud client and here I'm going
197:47 to open credentials and then you just
197:49 have to copy this this secret what you
197:53 have here right so I'm going to copy
197:55 this secret and I'm going to paste it
197:57 inside my client secret section the
197:60 scope uh you can just leave it as it is
198:02 the default as open ID and offline
198:04 access and the client authentication
198:06 also you can leave it as send as record
198:09 her and now after configuring all these
198:11 details you can click on get new access
198:13 token and you can see a message called
198:15 as authentication complete and once the
198:20 authentication is completed what Postman
198:23 is going to do is going to request quick
198:26 look for the access token and here you
198:28 can see this is the access token which
198:30 we received as a response from Key clock
198:32 right so I can now click on use token
198:35 and this token will be used now whenever
198:38 we are making a call to our micro
198:40 services so when I click on the send
198:42 button now you can see that we are
198:45 getting uh four zero five method not
198:49 allowed error yeah this is because I am
198:51 trying to access the order service but I
198:54 want to access the product service to
198:57 get the product information so let me
198:58 change the URL and click on the send
199:00 button one more time
199:02 and now you can see that we are getting
199:03 the response back from our product
199:05 service and we are able to access this
199:08 through our API Gateway right so in this
199:10 way we can secure our API Gateway so
199:13 let's also do the test for the placing
199:15 for placing an order so I'm going to
199:17 change the url as HTTP localhost 8080
199:20 API in order I am going to select the
199:23 HTTP method as post I am going to create
199:27 the same body as I've created before and
199:30 in the authorization tab I am going to
199:32 select the token which I've created in
199:35 this call right so for that I can just
199:38 select the type as OS 2.0 one more time
199:41 and I can select the token here in the
199:45 available tokens I can click on manage
199:48 tokens and I can select this token and
199:52 click on the use token button so that we
199:54 can use that particular token when
199:55 making this call so I'm going to click
199:57 on the send button now and now you can
200:01 see that we getting the order placed
200:03 successfully as a response
200:06 that's perfect so both our product
200:08 service and Order service are now
200:10 accessible through keyclock so the next
200:12 thing we are going to do is to try to
200:15 access
200:16 Eureka from the browser so and let's see
200:19 how is this going to behave right so I'm
200:21 going to create a new browser Tab and
200:24 I'm going to type in HTTP local
200:27 host 8080 slash Eureka slash web you can
200:32 see that we are getting a 401 error so
200:34 to fix this we have to enable some kind
200:36 of authentication also on our Discovery
200:39 server so we cannot use the
200:41 authentication mechanism with the token
200:43 because as you may have already observed
200:45 previously we are trying to contact our
200:47 product service and the order service
200:49 through the postman client now we have
200:52 to contact the Eureka Discovery server
200:55 through the browser right we cannot send
200:57 the token we cannot change the request
200:60 through the browser and send a token
201:02 manually right so for this we need to
201:04 enable basic auth in Discovery server so
201:07 that we can provide the username and
201:08 password while accessing this discovery
201:10 server URL so for that I am going to
201:12 open my entire my ID and I'm going to go
201:14 to the Discovery server palm.xml and I'm
201:18 going to add the dependency with group
201:20 ID org spring framework dot Boot and
201:23 here for the artifact ID I'm going to
201:26 type in Spring boot starter security
201:28 similar to the previous dependencies I'm
201:31 going to remove the version because I
201:32 want to download it from the parent
201:34 let's Force IntelliJ to download the
201:36 dependencies by clicking on the maven
201:38 icon and now finally I can create the
201:41 security config class inside our
201:43 Discovery server so for that I'm going
201:45 to open source main Java and I'm going
201:48 to create the security config class by
201:50 typing config
201:52 config now I'm going to add the
201:55 configuration and enable web security
201:57 annotation on top of the security config
201:60 class and this security config class is
202:03 going to extend the web security
202:04 configurer adapter class because now we
202:07 are using spring MVC so for spring MVC
202:11 security we can make use of web security
202:13 configure adapter class instead of the
202:16 classes from webflux so this class now
202:19 should override a method called as
202:22 configure so I'm going to type in
202:23 override
202:25 public void configure and this is going
202:29 to take a method parameter of type
202:32 Authentication
202:33 manager Builder
202:36 let's also create
202:38 the reference variable of this
202:41 authentication manager Builder and
202:43 inside this authentication manager ends
202:45 inside this method I'm going to type in
202:46 authentication manager Builder dot in
202:49 memory authentication with user and
202:54 with user as
202:57 Eureka and Dot
203:01 password as
203:04 password and I'm going to add a basic
203:07 authorities called as user so we are
203:10 just going to rely on the in-memory
203:12 authentication for the discovery server
203:14 so that's why I added the in memory
203:16 authentication schemes let me also we
203:20 have an error here because this method
203:22 is going to throw a exception so let's
203:25 add this exception to the method
203:26 signature so now we have a basic
203:29 auth created in our security config so
203:33 let's add some additional configuration
203:34 to our class so by overriding another
203:37 configure method so I'm going to type in
203:39 add hover ride public void configure and
203:43 this time I'm going to add the method
203:45 parameter as HTTP security and inside
203:49 this configured method I am going to
203:51 configure a basic security
203:53 a default security details like HTTP
203:56 security dot csrf dot disable
204:00 and authorize requests dot any request
204:05 and authenticated
204:09 and lastly I'm going to add the HTTP
204:11 basic authentication because enable the
204:14 HTTP basic auth and also the csrf method
204:17 is going to throw a exception let's add
204:20 this exception to the method signature
204:21 and now we have a basic security
204:23 configuration with basic auth enabled
204:26 and this basic watch we are going to
204:28 rely on the in-memory authentication
204:30 right and one thing I don't like here is
204:32 we are trying to hard code this username
204:35 and password inside our code so what we
204:39 can do is we can externalize this
204:41 properties to the properties file so
204:43 what I'm going to do is I'm going to
204:45 open the resources
204:46 application.properties and I'm going to
204:48 type create two properties called as
204:51 Eureka dot username it's going to be
204:55 Eureka
204:56 and Fabrica dot password is going to be
204:60 password right I can also try to
205:03 generalize this a bit more and I can try
205:07 to access this Eureka username and try
205:10 to send this you like a username and
205:11 password through the environment
205:13 variables so for that I can type dollar
205:16 Eureka underscore username
205:21 is going to be username
205:25 dollar
205:28 dollar Eureka underscore password so in
205:32 this way if we have configured the
205:34 Eureka username and password as
205:36 environment variables we can just pass
205:38 it while running the discovery server
205:40 and this will be picked up at the time
205:42 of startup and now inside the security
205:44 config class I'm just going to remove
205:46 this Eureka
205:48 and password the hard coded strings and
205:50 I'm going to make use of the values
205:53 which we have configured in the
205:54 properties file so for that I can use
205:57 the value annotation to read these
205:59 properties so
206:01 I am going to type in dollar Eureka dot
206:05 username and this is going to be stored
206:07 inside a variable called as username
206:10 and again I'm going to copy this tool
206:13 configuration to the next line and
206:15 Define the field for the password called
206:20 asurika.password and I'm going to rename
206:22 this variable as
206:24 password let's use this username and
206:26 password inside the inmovie
206:28 authentication configuration so for the
206:31 with user method I am going to provide
206:32 username and for password
206:35 method I'm going to provide the password
206:37 field as a method parameter so and
206:40 lastly we also have to define a password
206:42 encoder whenever we want to define a
206:45 username and password so for that after
206:47 the in-memory authentication method I
206:49 can type in with
206:50 DOT password encoder and for this case
206:54 for this use case I am going to go with
206:56 the no op password encoder
206:60 no of password encoder this is
207:02 deprecated but just to make the demo
207:04 purpose easier I am going to make use of
207:06 no password encoder but in the
207:08 production you have to not use this no
207:10 password encoder and as this is for demo
207:12 purposes I am using this but I strongly
207:14 suggest you to use a password encoder
207:17 like a Big Crypt password encoder or
207:19 some other strong password encoders in
207:21 your production code please don't use
207:23 this because I'm not using decrypt
207:25 password encoder because this needs some
207:28 more configuration in the project and
207:31 that feels like diverting the actual
207:34 goal of this project because I want to
207:36 show you the how show you how the
207:39 microservices project works together and
207:41 I don't want to go in more depth than
207:43 what I'm covering so that's why I'm not
207:46 covering how to configure decrypt
207:48 password encoder here but I will leave a
207:50 link down in the description if you are
207:52 interested how to configure decrypt
207:55 password encoder I will leave a Blog
207:57 link where I found so that you can have
207:59 a look at it and understand how to
208:01 configure password encoder so moving
208:03 ahead I am going to create the new
208:05 password encoder and I'm going to type
208:07 the method dot get instance so that we
208:09 will get a basic no off password encoder
208:12 so this is nothing but your password
208:15 will not be encoded it will just be
208:17 displayed in plain text right so that's
208:19 why this is not this is not recommended
208:23 but for demo purposes as I said before
208:25 I'm going to use this so let's move
208:28 forward what I'm going to do now is
208:30 restart the discovery server application
208:33 so now let's open the
208:37 Discovery server application so I'm
208:40 going to rerun my application in debug
208:43 mode and now I'm also going to so once
208:46 this is up and running so without any
208:47 errors yes it's able to start without
208:49 any errors I'm going to start all
208:51 restart all my other services this all
208:54 applications and the API Gateway
208:55 application
208:56 in debug mode one more time audio
208:58 service for example is not able to reach
209:00 the discovery server because we have
209:02 enabled authentication so what we have
209:04 to do is we have to provide the username
209:06 and password inside the discovery server
209:10 URL right so for example if I open the
209:12 application.properties
209:14 application.properties of API Gateway we
209:16 have our Eureka client service URL
209:18 default Zone and this is pointing to the
209:21 URL of Eureka dashboard so what I'm
209:24 going to do here is I'm going to add the
209:26 username and password information as
209:28 part of the URL so I'm going to type in
209:30 Eureka
209:31 password at localhost 8761 Eureka right
209:35 so this is similar to the jdbc URL
209:37 scheme providing the username and
209:39 password as part of the URL
209:41 so I'm going to add the same
209:43 configuration also to our other services
209:45 so let's copy the whole property key
209:48 value pair and replace it in all our
209:50 services so I'm going to open our
209:52 inventory service
209:54 application.properties replace this
209:57 property
209:58 and similarly I'm going to do this for
209:60 order service and as well as the product
210:02 service
210:06 yeah so now I have configured the URL in
210:09 all our services so let's try to restart
210:12 our all our services and see whether we
210:15 are able to start these services without
210:17 any errors all right so now you can see
210:19 the order service is started without any
210:21 errors that means our order service is
210:23 able to access the discovery server
210:25 through the basic auth scheme that's
210:28 very good so now let's try to access
210:30 HTTP localhost
210:33 8761 this one and you can see that it is
210:35 prompting us for the username and
210:37 password so let's try to type Eureka and
210:40 password as our credentials
210:44 and sign in and now you can see that we
210:46 are able to access our Discovery server
210:49 through the basic auth and here you can
210:51 also see that our dashboard is showing
210:54 us all our services as we are expecting
210:56 so that's it for this part guys so in
210:58 the next part I'm going to discuss about
211:01 the next component in our micro Services
211:04 called as circuit breaker and we are
211:06 also going to integrate the open find
211:08 client we are going to replace the web
211:10 client with open find client so I will
211:13 see you in the next part and until then
211:14 Happy coding techies so in the previous
211:16 video we saw how to secure our
211:18 microservices using the key cloak
211:20 authorization server so now let's
211:22 continue our project by introducing
211:25 another component called as circuit
211:27 breaker so a circuit breaker is mainly
211:30 used when we want to have a resilient
211:32 communication between our services so if
211:35 you remember we have our order service
211:37 and the inventory service and we are
211:39 calling the orders inventory service
211:40 from our order service using the web
211:42 client which is like a synchronous
211:44 communication so the problem with the
211:46 synchronous communication is like we can
211:48 have many problems while calling the
211:50 inventory service from the order service
211:51 so one of the problems is like the
211:54 inventory service may be not available
211:56 it may be down that is one problem and
211:58 the other problem is like this calls we
212:00 are making from a order service like for
212:03 some reason the inventory service May
212:05 respond slowly to the to the order
212:07 service right so by Design the remote
212:10 service calls can be slow so if
212:13 something goes wrong in the inventory
212:14 service like if due to your performance
212:16 issue a database issue we can expect
212:18 slow uh API calls while calling the
212:21 inventory service right so when we Face
212:24 these kinds of problems in a micro
212:26 service environment we have to make sure
212:28 that our system is resilient we don't
212:30 want the request to terminate abruptly
212:33 right we have to fail fast and we have
212:36 to fail when we have to provide some
212:38 kind of resiliency to these kinds of
212:39 issues so this circuit breaker pattern
212:41 is nothing but a set of States we may
212:44 maintain in our in our application so
212:47 let me scroll down and open this flow
212:50 diagram right so by default if the
212:53 communication between the services is
212:55 working fine that means if you don't
212:56 find any issues the state of the circuit
212:58 breaker will be in will be in a closed
213:01 state right so if a circuit breaker is
213:03 in closed States communication will be
213:04 allowed as it is and imagine that if our
213:07 inventory service is not working as
213:10 expected due to maybe temporary network
213:11 issues or something what will happen is
213:13 the circuit breaker will go to a state
213:15 of open so in the open State what will
213:17 happen is as we already know that the
213:20 calls to inventory service is not yet is
213:22 not working it doesn't make any sense to
213:24 keep on calling this inventory service
213:26 right so until and unless we the issue
213:28 is solved it doesn't make any sense to
213:30 keep calling the inventory service like
213:31 in the example for this reason the
213:33 circuit breaker will change to a state
213:35 of open that means it won't allow the
213:37 calls from our order service to the
213:39 inventory service anymore right so it
213:42 will not allow it for the certain number
213:43 of time for the certain number of
213:45 duration we can configure it ourselves
213:47 so during this period either it can just
213:50 throw the error message or it can also
213:52 execute some fallback logic right so for
213:55 example if the inventory service is not
213:58 available what we can do is we can cache
214:00 the response from the inventory service
214:02 and we can just use the cache right
214:04 maybe this is not a good idea this is
214:05 not a good example but we can maybe try
214:08 to execute some fallback logic right if
214:11 the inventory is not available maybe I
214:12 don't know like we can try to execute
214:14 some fallback logic and we can execute
214:16 this with the help of circuit breaker
214:18 when the state is in open when the state
214:21 is in open after a certain number of
214:23 after a certain duration the circuit
214:25 breaker will change the status into half
214:28 open that means it will slowly start
214:30 taking the request from the to the
214:32 inventory service and it will check
214:34 whether the requests are going through
214:36 or not right if the requests are still
214:38 not going through it will change the
214:40 status to open that means it will not
214:42 allow the calls and it will just execute
214:44 the fallback logic right and if the
214:47 requests are executed successfully then
214:49 it will change the status of the circuit
214:51 breaker back to closed so this Loop will
214:54 be followed like whenever we are making
214:56 a calls to the inventory service from
214:59 our order service so as you uh saw this
215:02 so as you saw this Theory now let's go
215:04 ahead and implement this circuit breaker
215:06 logic in our project so to implement
215:08 this logic we don't need to write the
215:11 logic ourselves we have a library called
215:13 as resilience 4J which will help us with
215:15 the circuit breaker logic so I can open
215:18 the website of the resilience 4J so this
215:20 is a lightweight easy to use fault
215:23 tolerance Library which is inspired
215:24 mainly by Netflix histrix if you are
215:27 aware of the micro Services landscape we
215:29 used to mainly use his tricks as the
215:32 fault tolerance Library so a resilience
215:33 4J is mainly like a newer alternative to
215:37 Netflix right so we can make use of this
215:41 library in our project by using a spring
215:44 Cloud project called as spring Cloud
215:46 circuit breaker will be mainly using
215:47 this project to implement the
215:50 fault tolerancy using the resilience 4G
215:53 so if you see spring Cloud circuit
215:55 breaker can have multiple
215:56 implementations one is with the Netflix
215:58 histricts resilience for JS Sentinel and
216:01 spring retry will be mainly using
216:03 resilience 4J not project so I am inside
216:05 the IDE and I'm going to open the order
216:08 Service as we are going to mainly
216:09 implement the circuit breaker pattern
216:12 inside the order service so for this I
216:14 am going to first add a dependency
216:16 called as spring Cloud circuit breaker
216:18 so for that I'm just going to add
216:20 another dependency just below the spring
216:23 Cloud starter Netflix Eureka client
216:25 dependency I'm just going to copy this
216:27 redundancy and change the artifact ID to
216:30 Spring Cloud starter circuit breaker
216:34 resilience 4J as we'll email using
216:36 resilience 4G here and after that we
216:39 also need another dependency called as
216:41 spring boot
216:43 starter actuator so for that I'm just
216:45 going to
216:46 change the artifact ID to Spring boot
216:49 starter actuator and also the group ID
216:53 to aux spring framework Cloud to org
216:56 spring framework Boot and
216:58 yeah so once I had added these artifact
217:01 ID in the group ID the maven
217:03 dependencies will be downloaded by
217:04 clicking on the maven icon if you are
217:07 using IntelliJ so once we have added
217:09 these two dependencies we can go ahead
217:12 and configure resilience 4J in our
217:15 project so I'm going to open source main
217:17 resources
217:19 application.properties and the first
217:22 thing I'm going to do here is first to
217:24 enable actuator right so using actuator
217:26 we can have a look at different kinds of
217:29 Health metrics for our application so
217:31 for that I am going to type in
217:32 management DOT health dot
217:35 circuitbreakerenabled equals true so
217:37 first I am going to enable the circuit
217:39 breakers endpoints in the actuator and
217:42 after that I am also going to
217:44 type in management
217:46 Dot endpoints.web
217:50 dot exposure dot include right so for
217:54 this I'm going to type start so that our
217:57 actuator will expose all kinds of
217:59 endpoints by default it will only expose
218:01 the health metrics endpoints so we want
218:04 all the endpoints to be enabled to us
218:06 right so for that I've just provided the
218:08 the value star here and I'm also going
218:10 to type in management dot endpoint DOT
218:14 health
218:15 dot show details so the health show
218:18 details property is by default set to
218:21 never so we're going to always show the
218:25 complete Health details of our
218:26 application so that's why we're going to
218:27 use this configuration and now we can go
218:30 ahead and Define the resilience 4J
218:33 properties so for that I am going to
218:35 type resilience
218:37 4G
218:39 properties and for these properties I'm
218:42 just going to copy and paste some
218:44 properties so that you don't watch me
218:47 type all these properties I will just
218:48 explain what are these properties and
218:50 why we have to use these properties
218:52 right so the first property is called as
218:54 resilience 4J circuit breaker instances
218:57 inventory and register Health indicator
219:00 right we want to register a health
219:03 indicator so that we want to see the
219:05 different states of circuit breaker we
219:07 have mentioned before right the circuit
219:08 breaker maintains different kinds of
219:10 States closed open and half open so to
219:13 be able to see those States we have to
219:15 first register the health indicator for
219:18 our resilience 4G Library so that's why
219:20 we provided this property as true then
219:22 after that we also have to provide an
219:24 event consumer buffer size as 10 so that
219:27 means how much buffer size it is going
219:29 to take for the events we have to
219:31 provide it as 10 and after that what the
219:34 circuit breaker will do is it will make
219:37 sure that it won't open the circuit
219:40 breaker right away right so if it if the
219:43 circuit if resilience 4J understands
219:45 that a call to the inventory service is
219:47 failing it won't change the state from
219:49 close to open right away right it will
219:52 wait for some number of uh requests to
219:54 fail like for example if only five
219:57 requests are failing back if if only
219:59 five back-to-back requests are failing
220:01 then only we will change the state from
220:03 close to open right so to be able to
220:06 count these number of requests we have
220:08 to provide the sliding window type as
220:11 count based right so resilience 4J will
220:14 use a sliding window algorithm to be
220:16 able to determine when to close and open
220:18 the uh the state of the circuit breaker
220:20 so for this we have to provide the
220:21 sliding window type as count paste right
220:24 and the sliding window size S5 that
220:26 means after five failed requests the
220:29 circuit breaker will try to change from
220:31 the close to the open State and the
220:33 failure rate threshold will be 50 that
220:35 means when the 50 of the calls are like
220:37 failed then the rate threshold we are
220:39 setting it as 50 and after that we have
220:42 we will also configure how much time we
220:45 can wait in the open state right uh
220:47 before going to the half open State we
220:49 don't want to allow all the calls we
220:52 have to execute the fallback logic so
220:54 for this we can configure the
220:57 resilience 4G to wait for five seconds
220:59 in the open State before it changing to
221:02 the half open state right so the next
221:04 property is permitted number of calls in
221:06 half open state right how many calls we
221:08 have to allow in the half open state so
221:11 that resilience 4J we can check whether
221:13 the communication between our order
221:15 service and inventory service is working
221:17 fine or not so this is going to be three
221:19 calls we will allow only three calls in
221:21 the half open state if the communication
221:25 is still failing then resilience 4J will
221:28 change the status to the open right like
221:30 how we see here when the circuit breaker
221:33 is understands that three calls are
221:35 failing then it will change the status
221:36 too so and lastly we are also going to
221:39 configure the property Automatic
221:41 Transition from open to half open
221:43 enabled it's true so we want to
221:45 automatically uh transition like enable
221:48 the transition between open to half open
221:50 after adding all these properties this
221:53 is uh this this is this is the only
221:55 thing we have to do after configuring
221:57 this properties next what we can do is
221:59 we can configure our code to make use of
222:02 this circuit breaker so we'll have we'll
222:04 see how to do that next all right so to
222:06 add the circuit breaker logic to our
222:08 code I'm going to open the order
222:11 controller this is the place where it
222:14 everything starts right whenever we are
222:16 trying to place the order the request
222:18 goes to the place order method inside
222:21 the order service and inside the order
222:22 service we are actually making the call
222:25 to the inventory service here right we
222:27 can also add the circuit breaker logic
222:29 inside the order service but just for my
222:31 convenience I'm adding it to the order
222:33 controller right so on above the place
222:37 order method I am going to type
222:39 annotation called as circuit breaker and
222:42 to this circuit breaker annotation I am
222:44 going to provide an attribute called as
222:46 name so this name I'm going to provide
222:49 it as inventory right so what this dual
222:53 do is resilience 4J will apply the
222:56 circuit breaker logic to this particular
222:58 place order method like whatever calls
223:00 whatever API calls which are being made
223:03 inside this place order method call it
223:06 will apply the circuit breaker pattern
223:07 here right so so this is all you need to
223:10 do to implement the circuit breaker
223:12 pattern in your project so one thing we
223:14 also understood is we want to also
223:16 Implement a fallback logic whenever the
223:19 calls are failing right whenever the
223:21 circuit breaker in the state is in the
223:23 state of open so for this reason as a
223:25 fallback logic what we can do is we can
223:27 provide a fallback method to the circuit
223:30 breaker annotation so that whenever we
223:33 the circuit breaker understands that
223:34 there are failed calls it will just
223:36 execute the fallback method right so for
223:39 this I'm just going to add another
223:41 attribute inside The annotation called
223:43 as fallback method and here we have to
223:46 provide the name of the fallback method
223:48 so for this I am going to provide the
223:50 name as fallback method just and what
223:53 I'm going to do is I'm going to create
223:55 another method with written type as
223:58 string because we need to have the same
223:60 written type as the original method and
224:02 I'm going to provide the name as
224:03 fallback method
224:05 and I'm also going to make use of the
224:08 same method signature we'll take an
224:10 order request
224:12 and additional to that we also have to
224:14 add another method method parameter
224:17 called as runtime exception because
224:19 whatever exception is raised from this
224:22 particular method from the place ordered
224:25 method we will also consume this
224:26 exception into the fallback method so in
224:29 here I am going to return a status
224:32 message something like
224:34 whoops something went wrong
224:38 please order
224:40 after some time
224:45 right we are going to provide this
224:47 particular error message so instead of
224:50 showing something called as a runtime
224:51 exception or maybe timeout or HTTP
224:54 status fail something some exception we
224:56 can just provide a good error message to
224:59 the user so this fallback method will be
225:02 executed by the circuit breaker so now
225:03 it's time to test our implementation
225:05 whether the circuit breaker is working
225:07 fine or not so for that I'm just going
225:09 to start all my services also make sure
225:12 you start all services and let's start
225:14 testing in The Next Step so all my
225:16 services are up and running I am going
225:18 to I'm just I'm just verifying
225:20 everything is fine or not in the Eureka
225:22 portal so everything is working fine so
225:25 now I'm going to open Postman make a
225:27 call to the order service and let's see
225:30 what will be the response I'm just going
225:32 to first make sure that I have a valid
225:35 access token so I'm just going to get
225:37 click on get new access token and click
225:39 on proceed click on use token and I'm
225:42 going to send this request
225:44 and you can see that we have received
225:46 the status as order placed successfully
225:48 right this is perfect so now what I'm
225:51 going to do is I'm going to stop the
225:54 inventory service right
225:56 um just before stopping the inventory
225:57 service I'm just also going to show you
225:60 the status of actuator by typing
226:02 localhost 8081
226:04 slash actuator
226:06 slash
226:08 health
226:11 and here you can see that we have the
226:14 information about the circuit breakers
226:15 and the circuit breaker state is closed
226:18 so I am going to go back to my IDE and
226:21 I'm going to stop the inventory service
226:23 so the inventory service is now stopped
226:27 and let's try to make a call to the
226:30 order API one more time so I'm going to
226:32 send the request to the order API one
226:34 more time and you can see that we have
226:36 received the fallback error message and
226:39 I'm going to first check the actuator
226:43 health so actuator endpoint I'm going to
226:45 refresh and you can see that the state
226:47 is still closed so I am going to execute
226:51 this command one more time place this
226:53 execute this API call four more times
226:56 one
227:02 three four five right uh we have
227:06 executed five times and quickly I'm just
227:08 going to uh refresh and you can see that
227:11 our
227:12 circuit breaker is already changed to
227:14 the state of half open right so the
227:17 circuit breaker will stay in open state
227:19 for only for some time like we have
227:21 configured to stay in open state for how
227:24 much time so it's
227:26 for five seconds right I think we are
227:28 too late after five seconds it's changed
227:31 the state of the circuit breaker from
227:33 open to half open right and now we can
227:37 see that now let's what you can do is
227:40 now let's start the inventory service
227:42 and after starting the inventory service
227:44 once it is up and running just make sure
227:47 to leave that inventory service for 30
227:49 seconds because it will take 30 seconds
227:51 to register the inventory service back
227:53 with the Eureka server so I'm just going
227:55 to leave this inventory service for 30
227:57 seconds so that it will warm up and just
227:59 send the request to the Eureka server
228:01 and after 30 seconds let's try to
228:03 request the order endpoint one more time
228:06 okay after 30 seconds I started placing
228:09 the calls again and now you can see that
228:10 we have received the response order
228:12 placed successfully and if I open the
228:14 health endpoint and refresh you can see
228:16 that the status is still in half open
228:18 now let's try to make some more calls
228:21 to place the order
228:23 and you can see that we have made more
228:25 than three calls here so now let's see
228:28 whether the status is changed to closed
228:30 or not and you can see that the status
228:32 is changed from half open to closed
228:34 right so in this way we can implement
228:37 the resilience 4J um in our library and
228:41 the circuit breaker design pattern in
228:43 our project all right so now let's go
228:45 ahead and Implement some Advanced use
228:47 cases now we saw how our resilience 4G
228:50 Library will handle requests when the
228:53 inventory service is now is not up and
228:55 running right if when the inventory
228:57 service is down so now let's also try to
228:59 simulate the slow network connection
229:01 case right so imagine that our order
229:04 service is calling the inventory service
229:05 and for some reason the inventory
229:08 service is not communicating like not
229:10 responding in time right maybe it's slow
229:14 due to a database performance issue so
229:16 how will resilience 4J can handle this
229:19 situation we can we can introduce a
229:22 timeout right that means order service
229:24 will wait for the inventory service
229:26 response for certain duration of time
229:28 maybe five seconds or 6 seconds and if
229:31 the inventory service is not responding
229:34 even after five seconds it will
229:36 terminate the call it will not wait
229:38 anymore and it will just throw a timeout
229:40 exception so to implement the timeout we
229:44 can open our ID and we can just have we
229:48 just have to add some properties in our
229:51 application.properties file so this is
229:54 going to be resilience
229:57 4J timeout properties I'm just going to
230:01 type in resilience4j dot time limiter
230:04 dot instances dot inventory right and
230:08 I'm going to provide the timeout
230:10 duration as 3 seconds
230:13 right so we want to wait a maximum of
230:15 three seconds before terminating that
230:18 call because a maximum of three seconds
230:21 before terminating that call and
230:23 throwing the timeout exception right so
230:26 to implement the time mode I can open
230:28 the order controller class one more time
230:31 and all I have to do is I am going to
230:34 add an annotation
230:36 called as time limiter
230:39 so this is again coming from the
230:41 resilience 4G library and the name I'm
230:43 going to provide the as inventory right
230:47 so one important thing we have to
230:49 remember is like you have to provide the
230:52 same name inventory as you have provided
230:54 inside the properties right so after
230:56 instances we can provide some certain
230:58 key right you don't need not provide
231:01 only the name as inventory you can
231:03 provide any name you like but just make
231:06 sure that you use the same key inside
231:09 The annotation also in the name
231:11 attribute so whatever attribute you have
231:14 added here you should also add the same
231:16 attribute inside the name property for
231:19 time limiter and as well as the circuit
231:21 breaker annotations so after adding the
231:23 time limited annotation we also have to
231:25 change the method signature of the
231:28 method so instead of returning a string
231:31 we have to return a completable future
231:35 of string because this will make an
231:37 asynchronous call you have to return a
231:39 completable future
231:40 so I am also going to change the written
231:44 type of the fallback method to return a
231:46 completable future instead of supplying
231:48 instead of returning just as normal
231:50 string I will be providing completable
231:52 future
231:56 dot Supply async
231:60 and then I will add this string here so
232:04 that it will return the string it will
232:07 execute this logic inside a new thread
232:09 and it will return a completable feature
232:11 we also want to do this same thing
232:13 inside for the place order method so I'm
232:16 just going to
232:17 copy this message order placed
232:20 successfully so we can change the
232:22 written type of the place order method
232:24 to return a string instead of white so
232:26 I'm just going to add change the return
232:28 type as string and I'm just going to
232:32 scroll down and just after the save
232:33 method I'm just going to add the return
232:35 statement as order placed successfully
232:41 right so this place order method is now
232:44 returning the response string as we
232:47 expect and let's go back to the order
232:50 controller method and now I'm going to
232:52 execute this place order method in a
232:54 separate thread so for that I am going
232:56 to type again completable future dot
232:59 Supply async
233:01 and I'm going to call the place order
233:04 method from inside this Supply async
233:06 method call right I'm just going to
233:09 return this particular completable
233:12 future back
233:14 to the method
233:15 and in this way we can implement the
233:18 timeout so this place order method will
233:21 be executed in a different thread and
233:23 whenever the timeout is increased like
233:26 whenever the time limit has increased it
233:28 will just throw a timeout exception
233:30 right so to be able to reproduce this
233:35 slow Behavior we also have to change the
233:37 logic inside the inventory service so
233:39 for that I'm just going to open the
233:40 inventory service
233:42 go to the inventory service class and
233:46 inside the is in stock method I'm just
233:49 going to add a log statement so for that
233:53 I'm just going to add the slf4j
233:55 annotation
233:57 I'm going to type
234:00 log dot info
234:03 um
234:04 weight started something like this
234:08 and another call as weight ended all
234:12 right so now I'm going to simulate a
234:15 slow Behavior here so for that I'm just
234:17 going to add thread dot sleep and I'm
234:21 going to call the thread.sleep method
234:23 here with a timeout of 10 seconds so
234:26 here the execution will stop for 10
234:28 seconds and it will then start executing
234:31 the remaining logic so in this way we
234:34 can actually simulate a slow Behavior
234:36 inside the inventory service and the
234:39 Sleep Method is going to throw an
234:41 exception so for that we can make use of
234:44 sneaky throws exception just so that
234:47 this except section will be consumed
234:48 don't use this snakey throws exception
234:50 in your production code I'm just adding
234:52 it for this demo purposes
234:55 usually in the production code you
234:57 should be able to try catch this
234:59 particular exception and read through
235:01 this exception okay
235:03 so now as we have simulated this
235:05 Behavior what I'm going to do now is I'm
235:08 going to restart my order service and
235:10 the inventory service
235:12 right so go ahead and
235:14 rerun in debug mode
235:16 and also the inventory service I'm going
235:18 to rerun in the debug mode so once these
235:21 two services are started again just make
235:24 sure that you provide 30 seconds you
235:26 wait for 30 seconds until the service is
235:29 visible inside your Eureka server and
235:31 then let's try to place an order and see
235:34 how this is going to behave so now I've
235:36 opened the postman client and just make
235:39 sure to access a new token when you are
235:42 making this new call so I'm going to
235:44 click on the send button for the place
235:48 order API post call method so let's
235:51 click on the send button
235:54 and you can see that we are getting a
235:57 500 error so let's see what is this
235:59 error
236:00 and yeah you can see that
236:05 we are receiving a timeout exception
236:07 right so that means the order service is
236:12 waiting for exact approximately three
236:14 seconds before throwing a timeout
236:16 exception but here you can observe that
236:18 the in the inventory service you can see
236:22 the log weight started and after
236:24 throwing the timeout exception this log
236:26 of weight ended is printed right so
236:29 let's try this let's see this one more
236:31 time I'm just going to clear the console
236:33 also inside the
236:35 order service application and I'm going
236:37 to
236:38 send this request one more time
236:41 and let's wait until we get the
236:42 exception so okay I'm going to open this
236:47 logs and you can see that we are seeing
236:48 the weight started log and after some
236:52 more seconds we saw the weight ended
236:54 lock right and meanwhile as the response
236:57 is taking more than three seconds like
236:60 the more than the timeout duration be
237:03 more than the timeout duration our order
237:05 service has terminated the call and it
237:08 through the timeout exception it's not
237:09 able to and it's not waiting for the
237:11 response from the inventory service
237:13 anymore right so in this way we can
237:16 implement the timeout using resilience
237:18 4G Library so now we are going to make
237:21 Implement another use case that is retry
237:25 right so whenever there is a slow
237:28 response or like whenever there is no
237:30 response coming in from the target
237:32 service we would ideally like to retry
237:35 the calls right like in this case we
237:37 would like to retry the call to the
237:39 inventory service is in stock
237:42 is in stock endpoint right so now we are
237:46 going to go ahead and implement the
237:48 retry mechanism how to do it in the
237:50 resilience 4G Library alright so to
237:52 implement the Retro mechanism I'm going
237:54 to open the application.properties file
237:56 of the order service and just below the
238:00 properties for the timeout I am going to
238:03 add a new comment
238:05 resilience
238:09 4J
238:10 retry properties
238:13 and I'm going to type in resilience4j
238:16 dot retry dot instances and I'm here I'm
238:19 going to call the inventory property and
238:24 I'm going to define the max atoms as 3
238:26 that means we have to attempt the retry
238:30 for three times and after that I'm going
238:32 to again call the same property result
238:35 dot inventory
238:37 dot weight duration
238:39 would be 5 Seconds
238:42 right so we have to wait for five
238:45 seconds before retrying this particular
238:47 call right so if we don't receive the
238:50 response in five seconds we will just do
238:52 a retry for three times and if this
238:55 retry is still not working then we will
238:57 just uh let the circuit breaker so
239:00 handle the actual request flow right and
239:03 after adding these properties we need to
239:05 add another annotation on top of our
239:07 controller class so I'm going to type at
239:11 retry and here I'm going to provide the
239:13 name attribute as inventory right
239:16 similar to the circuit breaker and the
239:18 time limiter annotations I am going to
239:20 also provide the name as inventory and
239:22 the retry annotation is going to pick
239:25 the properties which is defined with
239:27 this key and it's going to apply this
239:30 configuration whenever it's free trying
239:31 right so I'm going to restart the order
239:35 service one more time
239:37 so I'm going to rerun in the debug mode
239:40 and after the our service is restarted
239:43 again let's make sure to wait for 30
239:45 seconds so the order service application
239:48 is now up and running so what we can do
239:50 now is I can go to the postman client
239:54 one more time and let's try to access
239:56 let's try to request another new access
239:59 token I'm going to click on get new
240:01 access token method I'm going to click
240:03 on the proceed button and click on use
240:05 token
240:06 and let's try our luck whether we are
240:09 able to
240:11 um trigger the retry mechanism or not
240:13 I'm just going to click on the send
240:14 button and we can see that it's taking
240:16 some more time to actually get the
240:19 response so let's have a look at logs
240:22 what's happening here so I'm going to
240:24 open the inventory service log
240:26 and here you can see that already we are
240:30 getting the weight started
240:32 logs more than one time right and again
240:35 it started printing the weight started
240:38 log one more time and uh finally it's
240:41 printed the weight and it log three
240:42 times right so that means the order
240:45 service has tried to has tried to call
240:49 the inventory service exactly three
240:52 times with a weight duration of five
240:54 seconds and that's the reason why you
240:56 see the timeout exception now you can
240:59 also have a look at how this Statistics
241:02 how many times the circuit breaker has
241:04 restarted and how many times the circuit
241:06 breaker as has you know timed out you
241:08 can check this everything inside the
241:10 actuator Matrix so I'm going to open the
241:14 web browser
241:16 and I'm just going to refresh here the
241:19 actuator Health Matrix and if I just
241:21 scroll down you should be able to see
241:23 this but not here under the health
241:26 Matrix but let's go to the actuator
241:28 endpoint so just list localhost 8081
241:32 slash actuator and if you can just
241:33 scroll down you can see the metrics
241:35 about circuit breakers if you want to
241:37 see the metric support circuit breakers
241:39 we are actually interested in the
241:42 events about the retry events right so
241:45 if you just scroll down you can see one
241:47 object of cordless retry events if I
241:49 click on this particular object so you
241:51 can see all the retry events which are
241:53 triggered by resilience 4J right so it's
241:55 actually trigger the retry exactly three
241:58 times you can also check how many times
241:60 the time limit uh the time limiter is
242:03 triggered by the resilience 4J so this
242:05 is going to be time limiter events yeah
242:07 so this is the events which you have
242:09 created exactly three times there was a
242:12 timeout and all these events are
242:14 registered here through the actuator
242:16 right so so this is it for this video
242:19 guys in the next video we're going to
242:21 continue and Implement a much more
242:24 interesting patterns in our
242:26 microservices project so until then
242:28 Happy coding techies
242:34 all right so in the last video you saw
242:37 that we have configured circuit breaker
242:39 in our application architecture so we
242:41 have made the internal communication
242:43 between our services resilient so
242:45 whenever our service is down or whenever
242:48 we are facing a performance issue we can
242:50 be sure that we will have some kind of
242:53 fallback mechanism or even in worst
242:55 cases we will do a retry and maybe if it
242:58 takes long time then we'll do a timeout
243:00 of the request right so we'll have some
243:02 kind of resiliency in our system but how
243:05 exactly can we track down these issues
243:08 right so for sure we can have a look at
243:10 the logs but actually in a production
243:13 grid application where our where our
243:15 services can receive thousands of it's
243:18 not really possible to understand and
243:21 bug this point of performance problems
243:23 through logs right because you can have
243:26 thousands of log entries in our
243:28 thousands millions of log entries in our
243:30 system so for this reason there is a
243:32 design pattern called as distribute
243:34 tracing which is nothing but it helps us
243:38 to track the requests from the start to
243:40 the finish right as the name suggests it
243:43 helps us to trace the request from the
243:45 start to the finish so that if a request
243:47 is failed at any point of time we can
243:50 understand why white failed and may be
243:52 also where it failure so now let's go
243:54 ahead and understand how we can
243:56 Implement distributed tracing in our
243:58 project before going ahead and
243:59 implementing distribute tracing first
244:02 let's understand what really is
244:03 distributed tracing right so imagine our
244:06 example the user will place an order
244:10 place an order in our system so this
244:12 request will first reach the API Gateway
244:15 right and after that API Gateway will
244:18 proxy this request send this request to
244:20 the order service and the order service
244:22 will make a call to the inventory
244:24 service so this is our request flow so
244:26 to be able to track the request all the
244:29 way from API Gateway to inventory
244:31 service we need some kind of mechanism
244:33 to trace this request right so for this
244:36 reason we have something called as a
244:38 trace ID in distributed tracing right so
244:41 the phrase ID is nothing but a unique
244:43 identifier which will identify the
244:45 request which comes into a system along
244:47 with this Trace ID we also have
244:49 something called as a span ID so this
244:51 span ID is nothing but the number of
244:53 flips the request is going to take
244:55 inside our systems right we have one
244:58 trip to a packageway and other trip to
245:01 other service and lastly we have another
245:03 trip to inventory service right so all
245:05 these trips we have an unique identifier
245:08 for each destination right for the API
245:10 Gateway you will have a unique
245:12 identifier called as span one right we
245:14 call it as a span ID and for order
245:16 service we also have another span ID
245:18 called span 2 for example and for
245:21 inventory service we all have we also
245:22 have another span ID right so a span ID
245:25 indicates is like a unique identifier
245:28 for each request inside our individual
245:31 systems but the trace ID is a unique
245:33 identifier for the whole
245:35 by using these kinds of mechanism traces
245:38 and spans we can trace the whole request
245:41 life cycle in our services and we can
245:43 also understand if at all a service is
245:46 responding uh slowly or like if it is
245:48 having some performance issues we can
245:50 easily identify using the trace ID and
245:52 the span ID so now let's go ahead and
245:54 implement this mechanism in our
245:56 microservice project right so for this
245:59 we are going to make use of a project
246:01 inside a spring Cloud ecosystem called
246:04 as spring Cloud sleuth so the spring
246:07 Cloud sleuth actually it's like a
246:09 distributed tracing framework which
246:11 helps us to generate the trace ID and
246:14 the span ID whenever we receive a
246:15 request for microservices also need some
246:18 kind of UI to visualize this information
246:20 for this for this reason you have a tool
246:23 called as Zipkin which will help us to
246:25 visualize this information to get
246:27 started I'm going to add the spring
246:30 Cloud starter sleuth dependencies to our
246:33 project so first of all I'm going to
246:34 start with the API Gateway project and
246:37 in here I am going to add the dependency
246:40 with group ID
246:42 as org spring framework dot cloud
246:48 and for the artifact ID I'm going to
246:52 type in Spring cloud
246:55 starter
246:56 smooth right so this is going to
246:59 download the spring Cloud starters with
247:02 dependency and after that I'm just going
247:04 to copy this dependency one more time
247:06 and I'm going to type in
247:09 the artifact ID as spring cloud
247:12 slow it zip
247:15 right so the first dependency will
247:17 download the spring Cloud sleuth
247:19 dependency and the dependent and second
247:21 dependency is going to download the
247:23 Zipkin Library so I am going to copy
247:25 these two dependency information and I'm
247:27 going to do paste it inside the rest of
247:30 the projects right so I'm going to start
247:32 with the discovery server
247:43 all right so so I've added this
247:45 dependencies inside all the services so
247:47 the next step is to configure the
247:50 properties configured the spring Cloud
247:53 sleuth properties in our application in
247:56 in our services so for that I'm again
247:57 I'm going to start with the API Gateway
247:60 so inside the spring main resources
248:03 inside source main resources folder and
248:05 the application.properties file first of
248:07 all we need to configure the Zipkin
248:11 properties inside our inside our API
248:13 Gateway project but before that we have
248:15 to download the Zipkin to our machine
248:18 right so so if you go to the zipkin.io
248:21 website under the quick start section I
248:24 am going to scroll down to the docker
248:26 section and I'm just going to copy the
248:28 docker run command here
248:30 and I'm going to
248:33 open the terminal
248:34 and paste in and run this command as you
248:38 can see I already have the Zipkin
248:40 container up and running so you can just
248:43 run this command and it will start the
248:46 docker will start the Zipkin container
248:49 on your machine right and make sure that
248:51 you are exposing this Zipkin
248:54 um Zipkin container on the port 9411 go
248:57 back to the application.properties
248:59 inside our API Gateway project and in
249:02 here I am going to type in Spring dot
249:05 zipkin.base URL I'm going to type in
249:07 http
249:09 localhost
249:11 9411 right and after that I'm also going
249:15 to add another property spring dot
249:17 sleuth
249:19 .sampler dot probability has
249:22 1.0 so this sampler probability we have
249:25 set the value as 1.0 that that means we
249:28 want to send 100 of the request which we
249:32 are receiving to our system to Zipkin
249:34 right we want to start tracing hundred
249:37 percent of the requests which we are
249:38 receiving to our system all right so now
249:40 I'm going to copy this property and
249:42 paste this exactly into all the services
249:45 so I'm going to open the properties file
249:47 of our Discovery server I am going to
249:50 add this also let's add this inside our
249:53 inventory service our order service and
249:57 lastly the product service right so
250:01 that's all we need to do to enable
250:03 spring uh spring Cloud sleuth to enable
250:06 distributed tracing in our micro
250:09 Services project so let's go ahead and
250:11 test whether this discrete addressing is
250:13 really working or not so for this I'm
250:15 going to first start off with by
250:18 starting the discovery foreign
250:20 let's see if it is able to start up or
250:23 not without any errors all right so you
250:25 can see that the discovery server is
250:27 started without any problems and I think
250:31 it's able to also connect to Zipkin and
250:35 now I'm going to start off all the rest
250:37 of the services all right so if you see
250:39 the logs so if You observe the logs you
250:41 can already see that we have some
250:43 additional information in our logs right
250:46 for each log entry we have adding some
250:49 kind of additional information right so
250:51 first of all we are showing the service
250:53 name what service we have and we have to
250:56 blank spots here right so here this is
250:60 the place where we see the span ID as
251:02 well as the trace ID of each request so
251:05 I'm going to open the cement line and
251:07 I'm going to first of all make a request
251:09 to the product service first of all for
251:13 that I'm just acquire a new access token
251:15 for that I'm just going to click on get
251:17 new access token and click on proceed
251:20 and then click on use token and then I'm
251:24 going to send this request
251:26 to the product service and here you can
251:29 see the unique identifiers here right we
251:33 received this information
251:35 um to the product service this request
251:38 contains a span ID as as well as the
251:41 trace ID now let's go ahead and also
251:44 place an order in our system so for that
251:47 I'm going to go to the
251:50 Postman client one more time I'm going
251:52 to acquire a new access token one more
251:55 time
251:56 and then click on so as you remember
251:59 that we have added a slight delay in our
252:03 inventory service so for this reason we
252:05 are not getting the response right away
252:07 for sure we'll getting we will get a
252:09 timeout exception inside our order
252:11 service so because that's good we can
252:14 actually have a look at how our this
252:17 timeout exception is going to be handled
252:19 through the distributed tracing let's
252:21 open the Zipkin UI and visualize this
252:23 information so I'm going to open my
252:25 browser and go to localhost 9400 Zipkin
252:29 and here you can see the the home page
252:31 of the Zipkin application and here I'm
252:34 just going to click on run query and you
252:36 can see that it is displaying us the
252:39 information about the request we have
252:42 made you can see that the request has
252:44 first reached the API Gateway and it
252:46 actually took 19 seconds for this
252:49 request to be completed and it actually
252:51 made three stops that's why it has three
252:53 spans here so if I click on the show
252:56 information in here you can see that
252:58 first it made a request to API Gateway
253:00 then it made a request to order service
253:02 right if I click on show all annotations
253:05 and see some information about this
253:07 request you can also see what is the
253:10 error error status of this particular
253:13 call what is the HTTP method the path of
253:16 this particular call and we can also see
253:18 the error message of this particular
253:20 call which say the order service so if
253:23 you want to see more than 10 weeks so by
253:25 default it's showing 10 results right so
253:28 if you want to see more than 10 results
253:30 I can click on the settings button and
253:32 the limit I can set as maybe 1000 right
253:36 and I can click on the Run query and now
253:39 you can see the all the requests in the
253:42 last 15 minutes all right so now you you
253:44 may get a doubt that are we not able to
253:46 see the inventory service in this whole
253:48 call stack right you can only see the
253:51 API Gateway and as well as the order
253:53 service in this particular Trace
253:55 information and if you check the order
253:57 service and if you click on the show
253:59 button for the order service we are able
254:00 to call this see a call to the inventory
254:02 service here this is because we are
254:05 making a call to the inventory service
254:07 in a different thread right we are using
254:09 we are making use of a circuit breaker
254:11 here right so what this circuit breaker
254:14 will essentially do is it will create a
254:16 new thread from our order service and it
254:19 will make a call to the inventory
254:20 service in that new thread right so if
254:23 you take this into consideration this is
254:25 not a same request right this is a
254:27 different request so from our API
254:30 Gateway to order service we are making a
254:33 one request and from order service to
254:35 inventory service this is completely a
254:37 different request as this is running on
254:39 a different thread right so that's why
254:42 we are not able to see this inventory
254:44 service information inside the inside
254:47 the previous Trace request right if you
254:50 want you can try to disable the circuit
254:53 breaker in our order service so if I
254:56 open the
254:57 order service class you can try to
255:01 disable the circuit breaker but let's
255:03 try to disable the circuit breaker
255:04 annotation and let's see what will
255:06 happen
255:24 and now if you see the API Gateway line
255:29 and click on the show button you can see
255:31 all the information here right you can
255:33 see that the API Gateway is making call
255:35 to order service and or Services we can
255:38 call to inventory service right so you
255:41 can see that in the whole call to the
255:43 order service the inventory service is
255:46 taking 292 milliseconds and the order
255:49 service is taking 625 milliseconds and
255:51 the whole request is taking
255:53 637 milliseconds to complete right and
255:57 you can also have a look at the span IDs
255:60 for each of the services so for the
256:02 inventory service the span ID is 755
256:05 starting with 755 let's see if we are
256:08 able to find the span ID inside the logs
256:10 or not so I'm going to to open the logs
256:13 of the inventory service
256:15 I am going to search for 755i and you
256:18 can see that the span ID visible is
256:21 visible here right and here we have the
256:25 trace ID which starts with 2 a c e let's
256:28 see whether this is the same Trace idbc
256:31 in the Zipkin UI or not
256:33 so and you can see that the trace ID
256:35 here is starting with 2ace and similarly
256:39 for the order service the span ID starts
256:42 with af82f and let's go to the order
256:46 service
256:47 and you can see that the span ID is also
256:49 the same here right so when we are
256:52 making a call in a single thread without
256:55 any creating additional threads we can
256:57 trace the request from start to the end
256:59 but if you are trying to create an
257:02 additional thread on top of the an
257:05 additional thread then this is not
257:06 really useful right you need to create
257:08 your own spans for that right so let's
257:11 see how to create our own span to be
257:14 able to understand this information
257:16 right so let's try to enable this
257:19 circuit breaker
257:21 one more time so I'm going to go inside
257:24 the place order method of the order
257:26 service and spring Cloud sleuth provide
257:29 us provides us a mechanism to actually
257:32 create our own span IDs here right so we
257:36 can do that by using the Tracer class
257:38 from Spring Cloud sleuth so I'm going to
257:40 type in private final Tracer which is
257:44 actually coming from the spring Cloud
257:46 smooth so
257:50 so I am going to make use of this Tracer
257:54 reference variable and just before
257:56 making a call to the inventory service I
257:59 am going to type tracer
258:02 dot next span dot name we are going to
258:06 provide a unique name to this particular
258:08 span we are going to create right so I'm
258:10 going to call this as inventory service
258:13 lookup
258:15 and let's store this written variable
258:18 inside a local variable called as
258:21 inventory service lookup so just before
258:23 the call to the inventory service I am
258:26 going to type tracer
258:28 dot with
258:30 span and here I am going to provide the
258:34 span as inventory lookup service lookup
258:37 dot start right
258:39 and what this will do is it will execute
258:42 the whole code first I have to
258:47 assign this inside a try finally block
258:54 and I'm going to inside the finally
258:57 block I'm going to type inventory
258:59 service lookup dot end and I'm going to
259:01 move all the code below this
259:05 into this try with resources block let's
259:08 format this so that it looks nice so
259:11 let's assign this particular with span
259:13 call to another reference variable
259:16 called as Tracer dot span
259:19 Span in scope and let let's call this
259:23 variable as
259:26 Span in scope and that's it so we have
259:30 to execute the whole block of code
259:31 inside this try with resources block so
259:35 what will happen is spring Cloud sleuth
259:37 will assign this particular span ID
259:40 which we have created in the previous
259:41 step to this particular piece of code
259:43 which is executed inside this block okay
259:46 so let's restart our order service so
259:50 after you have restarted the order
259:51 service let's go back to the postman
259:53 client and make sure to acquire a new
259:57 access token before you place the order
259:59 so after getting a new access token
260:01 click on the send button and you can see
260:03 that your the order is placed
260:05 successfully let's go back to Zipkin and
260:08 click on run query and under the API
260:11 Gateway row I am going to click on the
260:14 show button and you can see that we have
260:16 as we have created the as you have added
260:18 back the logic to full circuit breaker
260:20 we cannot see the inventory service here
260:22 anymore so let's go back to find trace
260:24 and run query one more time and I am
260:26 going to select the order service and
260:28 click on the show button now you can see
260:29 that we have already we can already see
260:32 the span ID we have created called as
260:35 inventory service lookup so in this way
260:37 we can create our own span ideas if you
260:39 want to trace this uh stress any
260:42 particular piece of code you can create
260:43 your own spans manually and and assign
260:46 that particular span ID to whatever
260:48 logic you want to execute so that's it
260:51 for this video in the next video we are
260:52 going to have a look at how to implement
260:55 event driven architecture in our micro
260:57 service system we are going to make use
260:59 of the Kafka message queue to implement
261:01 the email Microsoft Event driven
261:03 architecture so I will see you in the
261:04 next video and until then Happy coding
261:06 techies
261:11 welcome to part 8 of the springboard
261:14 micro Services Series so in the previous
261:16 video we saw how to implement
261:17 distributed tracing in our application
261:19 now let's go ahead and see how to
261:22 implement event driven architecture in
261:24 our microservices project right so
261:27 before we continue so let's understand
261:29 what exactly is event driven
261:31 architecture right so in our order
261:34 service we have we are communicating
261:36 with the inventory servers in a
261:38 synchronous manner right like we are
261:39 making a HTTP request through the order
261:41 service to place an order and the order
261:44 service is again calling the inventory
261:46 service using the HTTP request again
261:49 using the rest API using the rest call
261:51 right so this kind of communication as
261:53 we know is called as synchronous
261:54 communication the order service will
261:56 call the inventory service and it will
261:58 wait for the response from the inventory
261:60 service once it receives the response it
262:02 will send it back to the user right so
262:05 this is called as synchronous
262:06 communication we can we will have we can
262:08 have another kind of communication
262:10 called as asynchronous communication
262:12 where the order service will make a
262:14 request to the inventory service and it
262:17 will not wait for the response right so
262:19 it will just fire the request and it
262:21 will forget about the response so this
262:23 kind of communication is called as
262:24 asynchronous communication this kind of
262:26 separates communication can be enabled
262:29 by using event driven architecture which
262:31 is nothing but performing asynchronous
262:33 communication with the form of events
262:35 right so whenever we receive an order
262:38 through our order service and the order
262:40 is placed successfully our order service
262:43 will raise an event so in this scenario
262:46 it can be like an order placed event and
262:49 we can place this order placed event
262:50 object as a message inside the Kafka
262:54 broker and our notification service
262:56 which is the consumer your order service
262:58 will be a producer of the message and
263:00 the notification service will be a
263:02 consumer so it will consume this message
263:04 and will process this message
263:05 accordingly right so usually if you want
263:08 to implement the functionality it's like
263:10 sending out notifications either in the
263:12 form of email or SMS we are not going to
263:14 implement this functionality in the
263:17 notification service so what we are
263:18 going to do in this part is to set up
263:21 this event driven architecture right so
263:23 now let's go ahead and install Kafka in
263:25 our local machine so for the
263:27 installation we are going to use Docker
263:29 and Docker compose to create the docker
263:32 containers
263:33 so for this I am going to first open the
263:36 Apache Kafka quick start guide from
263:39 confluent developer website so here I'm
263:42 just going to select the option Docker
263:43 and the first thing we are going to do
263:45 is to copy the the docker compose file
263:48 because we already have the docker
263:50 compose file ready inside this conferent
263:52 developer website so what I'm going to
263:54 do is in the setup Kafka broker section
263:57 I'm just going to copy this whole file
264:01 and I'm going to open my IDE and in the
264:05 root folder I'm going to right click and
264:07 click on new file and I'm going to name
264:10 this file as
264:12 docker compost.yaml
264:15 let's add it to the git repository and
264:17 I'm just going to paste in
264:19 the whole code which I've copied from
264:21 the conference developer website
264:25 so in this rocker compose file we are
264:27 using a version 3 and you are mainly
264:29 downloading two Services the first one
264:33 is Zookeeper for zookeeper is used to
264:36 orchestrate the Kafka clusters so we
264:39 need a zookeeper instance to orchestrate
264:42 the Kafka cluster even if you are just
264:44 using a single Kafka cluster so we have
264:47 defined the docker image from confluent
264:49 Inc so we're using the version 7.0.1 and
264:53 the container name we are defining as
264:54 zookeeper and we are also providing a
264:57 couple of environment variables the
264:59 first one is Zookeeper client Port so we
265:02 are defining the port as 2181 and
265:05 zookeeper tick time has 2000
265:07 milliseconds
265:08 stick time is nothing but like
265:11 interval with which it will send the
265:13 Zookeeper will send heartbeat messages
265:15 right so this is something similar to
265:18 what Eureka is doing like we have this
265:19 heart rate messages for every 10 seconds
265:21 so similarly zookeeper we can also
265:24 configure this tick time zookeeper as
265:27 2000 milliseconds
265:29 next we have the Apache Kafka broker
265:32 itself so this uh Kafka is called as
265:35 broker is is defined by the time by the
265:39 tag called broker so we are using again
265:42 the docker image from confluent Inc and
265:45 with version s7.0.1 the container name
265:49 we are defining it as broker and we are
265:52 also defining the ports on which this
265:54 Kafka cluster will run so the single
265:57 load cluster will run so we are using
265:58 this 9092 port and we are also exposing
266:01 this port out of the docker container
266:03 we don't need this comment so I'm just
266:06 going to remove this comment and this um
266:08 Apache Kafka container also depends on
266:10 the Zookeeper container so that's why
266:12 after zookeeper container is up and
266:16 running so we will also start with the
266:18 our Kafka container and lastly we also
266:21 have the environment variables we have
266:23 the broker ID set as one we have the
266:26 environment variable for Kafka zookeeper
266:29 connect so here we are just referring to
266:31 the container of Zookeeper by
266:35 first referring the the container name
266:37 as zookeeper as if we defined here
266:39 followed by the the port so is 2181 as
266:43 we defined inside the Zookeeper client
266:46 Port enrollment variable and lastly we
266:48 also have this Kafka listener security
266:50 protocol map environment variable
266:53 which is defining the The Listener
266:56 security protocol we are using this
266:57 plain text and and plain text internal
267:00 and we also have the Kafka advertised
267:03 listeners so these are all the listeners
267:05 which are configured inside our Kafka
267:08 cluster and it's also sent us a metadata
267:10 to the clients defining it in the plain
267:13 text format we are defining it as
267:14 localhost 9092 and also we are defining
267:18 it using the hostname as plain text
267:21 internal
267:22 29092 as this is a single node Kafka
267:25 instance we are setting this offset
267:27 topic replication we will do as one and
267:29 also this transaction state
267:32 ISR S1 and also the transaction State
267:35 log replication Factor it has one
267:37 because we are setting it as one because
267:38 we are just using a single load here if
267:41 you are using multiple nodes in a
267:43 cluster you may want to also change
267:45 these values to the to suit the number
267:47 of instances you have so this is how
267:50 this so this is the configuration of
267:51 Docker compose so to download the docker
267:54 images and to run the docker containers
267:56 what we can do is I'm going to open the
267:58 terminal
267:59 and
268:01 make sure you are in the root folder
268:03 and you can type Docker space compose up
268:08 minus d right
268:11 so press enter so the first thing it
268:13 will do is it will first check whether
268:15 the image is existing in your machine or
268:18 not if not it will first try to download
268:20 and it will start the containers as I
268:23 already have the images on my machine
268:26 it's not downloading so when you're
268:28 running this command maybe it will
268:29 download this image and it will take
268:31 some more time
268:32 but you can see that these two
268:34 containers are started and you can also
268:37 verify that they are running fine or Not
268:39 by running the command Docker PS
268:43 and you can see that these two
268:45 containers are up and running
268:47 successfully you can also verify the
268:49 logs of our Kafka broker by typing
268:52 Docker blocks minus f
268:56 and I'm going to Define I'm going to add
268:59 the name of the Kafka broker
269:02 and you can see that the logs looks fine
269:06 there are no errors so that means our
269:08 Kafka broker is ramp and running
269:11 successfully
269:12 so now we can go ahead and configure
269:15 Kafka in our order service
269:18 all right so now let's discuss about the
269:21 spring framework which we are going to
269:23 use to integrate with Apache Kafka so
269:26 the library is called as spring for
269:28 Apache Kafka so this spring for Apache
269:31 Kafka project apply score spring
269:33 Concepts to the development of Kafka
269:35 based message Solutions it provides a
269:38 template as a high level abstraction for
269:40 sending messages it also provides
269:42 support for message driven projects with
269:44 calf collisioner annotations and a
269:46 listener container
269:48 so this is the description which we have
269:50 inside the spring project website so
269:52 what is going to what it says is like of
269:55 course we are going to use this library
269:57 to you know use it inside our
269:59 springboard projects and we are going to
270:02 and spring and this project is mainly
270:04 going to
270:05 um
270:06 provide the functionality using a class
270:09 called as a Kafka template class so
270:12 using this Kafka template class our
270:14 producers in our case this order service
270:16 can send the messages to our consumers
270:19 in a in this case it's the notification
270:21 service and the consumer side and from
270:25 the Communist consumer side we can
270:27 Define this consumer like this
270:28 notification Service as a listener by
270:31 adding this Kafka listener annotation so
270:34 we can use all these features inside our
270:38 project to integrate with Kafka from our
270:41 springboard projects so let's go ahead
270:44 and install this library in our project
270:46 so for that I'm just going to click on
270:49 the tab learn and I'm going to click on
270:51 the reference documentation section
270:54 and in here I'm just going to scroll
270:56 down until I find the dependency
270:58 information so under the quick door you
271:01 can just copy this dependency
271:02 information
271:04 under Maven and I'm going to open
271:08 the project in my ID and I'm going to
271:12 copy this dependency inside the palm.xml
271:16 of the order service class I'm going to
271:18 paste it in here auto service project
271:22 and and let's click on this load maybe
271:26 changes icon and the next thing I'm
271:28 going to do is to configure our order
271:31 service to configure our order service
271:35 to listen to the Kafka broker which is
271:37 running on our machine so for that I'm
271:39 going to open source main resources
271:43 application.properties I'm just going to
271:45 scroll down until the end of the
271:47 properties file and in here I'm going to
271:49 add the property
271:55 thank you
271:56 and the comment as Kafka properties and
271:59 I'm going to type in Spring
272:01 dot Kafka
272:03 bootstrap servers so these are the list
272:06 of servers where we can find Kafka
272:08 installation so as we are using a local
272:11 installation we can just provide the URL
272:14 as localhost
272:15 9092
272:17 right after adding this property Kafka
272:21 will be automatically configured in our
272:23 springboot project the next thing we can
272:26 do is I'm going to open the Java folder
272:29 and inside the order service
272:33 uh we are going to make a call to the
272:36 Kafka cluster whenever an order is
272:39 placed right so for that I'm just going
272:41 to inject the Kafka template class into
272:45 our order service so I'm going to type
272:47 in private
272:49 final
272:51 Kafka template
272:58 and just below the call to the order
273:00 repository dot save method I'm going to
273:03 type in Kafka template
273:05 Dot send and the send method contains
273:10 different text different arguments so
273:13 what we are going to do is we are going
273:15 to use this argument which contains the
273:17 topic as a which takes the first the
273:20 topic name as the first argument and the
273:22 actual later we want to send as the
273:24 second argument
273:26 so I'm going to select this method
273:28 and we are going to send the message to
273:32 a topic with name as
273:36 notification topic
273:39 and here
273:40 we want to send out the order number we
273:44 want to send out the order number as a
273:46 message to the queue so that in the
273:49 notification service we will understand
273:51 what is the order number which was
273:53 created right so for this reason instead
273:56 of sending out the order number like
273:59 this like order dot get number what we
274:01 can do is we can create a new class
274:03 called as an order replaced event and
274:06 send out this object as a Json message
274:10 right so for this art I'm going to do is
274:13 I am going to create a new package
274:17 called as event
274:20 and inside this package I am going to
274:22 create a new class called as order list
274:27 event
274:28 right and this is going to be just a
274:31 plain pojo class so let's go I'm just
274:34 going to add the data annotation and
274:37 hologues Constructor
274:39 no Ox Constructor and this is going to
274:42 be and this is going to contain just one
274:44 field and I'm going to type in private
274:47 string
274:49 order number
274:51 back to our class or the service class
274:54 I'm going to wrap this order number
274:57 with the New Order list
275:02 event object
275:05 so
275:07 let's pass this order number as a
275:10 Constructor argument
275:12 and now what Kafka template will do is
275:16 it will send this order placed event
275:17 object as a message to the notification
275:21 topic
275:22 so if You observe this method call Kafka
275:25 template.send that we are getting a
275:28 warning from IntelliJ that we can we are
275:31 making an unchecked call to this send
275:32 method right so if You observe the Kafka
275:35 template class I'm just going to
275:38 hold Ctrl and click on this Kafka
275:41 template class so that it will open the
275:44 sources and you can see that this Kafka
275:46 template class accepts two generic
275:49 arguments K and key and value and we did
275:52 not Define this key value pair when
275:55 actually defining the Kafka template
275:57 variable so for that we can Define the
275:59 key and value pair as string and the
276:02 order placed event why these two because
276:05 we are providing the notification topic
276:08 as a string and we are providing this
276:10 order placed event as a value so this
276:13 topic name is going to be string which
276:14 is a key and the order placed event is
276:16 going to the value so let's update this
276:19 Kafka template and declaration
276:22 with the key value pair so I'm just
276:23 going to type in string as the first key
276:26 argument and the order placed event as
276:29 the
276:30 second generic argument as a value right
276:33 and we can also Define the notification
276:37 topic as the default topic for our order
276:39 service right so if you want to send
276:41 multiple messages to Kafka so instead of
276:45 typing this notification topic for every
276:48 message we are going to send we can
276:50 Define this notification topic as a
276:52 default topic so that spring boot will
276:54 understand that we have to always send
276:56 the messages unless we explicitly
276:59 provide a different topic name so
277:02 springboard will understand that it
277:04 should always send the messages to this
277:06 notification topic so we can do that by
277:09 going to the application.properties file
277:11 and we have to add the property
277:14 spring dot Kafka dot template default
277:17 topic as the notification topic
277:20 and other than that we also have to
277:22 provide information to our springboot
277:25 project how to you know serialize these
277:28 these key and value pair when sending to
277:31 the
277:32 Kafka broker right so we have so we can
277:36 Define some serializers through spring
277:38 Kafka project so I'm going to add a
277:41 property called as spring dot Kafka dot
277:45 producer dot key serializer and we are
277:48 going to provide the serializer class as
277:50 our capacity Kafka common serialization
277:53 dot string serializer so using this
277:55 practical property springboot will
277:57 understand how we have to serialize this
277:59 particular key and send it to the Kafka
278:01 broker so likewise we also have to
278:03 define the serializer for the value that
278:06 is the audit list event so by default uh
278:09 it's okay that it's okay like if we send
278:12 this audit list event as a Json object
278:14 right so to be able to convert this Java
278:17 object into a Json we also have to
278:19 define the serializer so for that I am
278:21 going to add the property spring dot
278:24 Kafka dot producer dot value serializer
278:27 as Json serializer class which is coming
278:30 from orc spring framework Kafka support
278:33 dot serializer package
278:36 right so that's all we need to do to
278:38 configure our order service so now let's
278:41 conf so now let's go ahead and create
278:43 the consumer which is the notification
278:46 service and configure the that service
278:48 with Kafka alright so now let's go ahead
278:51 and create the notification service
278:54 project notification service module in
278:57 our project so for that we need the
279:00 starting code so so for that I'm going
279:03 to open the
279:05 star.spring.io website and I'm going to
279:08 generate one springboot project so I'm
279:11 going to use the group ID as com
279:16 programming.techy artifact ID as
279:18 notification
279:20 service and I'm going to use Java 17 and
279:24 packaging as jar and I'm going to add
279:27 some dependencies here the first one is
279:29 going to be lombok
279:31 and we are also going to use spring web
279:35 lastly we're also going to use spring
279:37 for Apache Kafka so I'm just going to
279:40 search for Kafka and add this dependency
279:43 now I'm going to copy the palm.xml
279:46 because that's what I need to create the
279:49 service so I'm just going to click on
279:51 explore
279:54 and just copy everything copy all the
279:58 information right from the properties
280:01 until the end of the spawn.xml right so
280:06 I'm going to copy this whole code and
280:08 I'm going to go back to my
280:10 IDE now I'm going to create a new module
280:16 so there's going to be a maven module so
280:19 I'm going to I've already shown you the
280:22 process of creating a module so I'm just
280:24 going to not explain this one more time
280:26 so I'm going to click on next I'm going
280:29 to call this as
280:31 a notification service and make sure you
280:35 have the parent as microservice new and
280:39 not any other project so or else if you
280:43 select for example order service it will
280:46 create this notification service inside
280:48 the order service so we don't want that
280:50 so that's why make sure you select the
280:52 module as micro Services New
280:55 and artifact and coordinates and the
280:57 version is fine so I'm just going to
280:60 click on finish
281:02 click on ADD
281:04 so this will create a new module inside
281:06 our microservices new project so let's
281:09 verify whether this module is added
281:12 correctly by opening the form.xml
281:15 and let me close this annoying pop-ups
281:17 and inside the modules stack you can see
281:21 that our notification service module is
281:23 added successfully so I'm going to go
281:25 back to the palm.xml of the notification
281:28 service and in here I'm just going to
281:30 paste the code which I copied from the
281:33 star.spring.io website right
281:36 so
281:38 I think we don't need the properties tag
281:41 I can just copy this java.version I can
281:43 I think I don't need the properties tag
281:45 as it's already there
281:47 inside the form.xml here we are
281:50 providing the java.version so let's
281:54 add this
281:55 property inside the
281:58 original tag and I'm going to remove the
281:60 properties tag
282:02 and click on this load maybe changes
282:05 icon
282:07 so now let's go ahead and also build
282:09 some Java classes inside our
282:11 notification service module so inside
282:15 the source main Java package directory
282:18 and I'm going so inside the source main
282:21 Java directory I am going to right click
282:23 and create a package called as
282:26 com programming dot techie and in here
282:30 we need to create the notification
282:33 service application class which is the
282:35 main spring boot application entry point
282:37 class so for that I'm just going to
282:39 create a class called
282:41 notification
282:43 service application class I'm going to
282:48 add it to the git repository and instead
282:50 of typing out the code I can just use
282:53 the star.spring.io website to copy the
282:57 initial version of this notification
282:59 service application class so I'm just
283:01 going to copy everything
283:03 inside the
283:06 class here go back to my IDE
283:09 and
283:11 paste this inside the class right so now
283:15 we have the spring boot application
283:17 class up and running now what I need to
283:20 do here is to I have to define a Kafka
283:23 listener inside this project so I'm
283:26 going to do this inside the notification
283:28 service application class itself
283:30 I'm going to write The annotation Kafka
283:35 listener and this scarf collisioner is
283:38 going to listen to the topic
283:41 called as notification topic because
283:45 this is the name of the topic which we
283:47 gave you to our order service class so
283:51 if I open order service and here you can
283:53 see this is the name of the topic we
283:54 have configured so I'm going to use the
283:57 same name of the topic and I'm going to
284:00 define a method called as public void
284:04 handle notification
284:10 and this method is going to take the
284:12 order placed event object as the
284:16 as the input argument right so for that
284:18 we have to
284:21 Define this order placed event but we
284:24 don't have this class inside our service
284:26 so what I'm going to do is to create
284:29 a class called as order
284:32 placed
284:34 event
284:35 and I'm going to Simply copy the
284:38 existing logic
284:40 of the or replaced event so I'm just
284:43 going to open the order service
284:46 and find out
284:48 where is the find out the order placed
284:50 event class I'm going to just copy
284:53 everything from here paste it inside
284:55 this order placed event from
284:56 notification service
284:59 right so you may have one doubt that why
285:02 I'm creating these two classes inside
285:04 the order service and also the
285:06 notification service but why can't we
285:08 share this class between these two
285:10 service in general it's recommended not
285:12 to share any classes between these two
285:15 Services because we want to keep this
285:17 Services independent and clean so it's
285:20 always recommended if especially if
285:23 you're talking if you are trying to
285:25 receive and send payloads to have your
285:28 own version of this classes inside your
285:30 own services and now let's go back to
285:32 this order
285:33 handle notification method inside the
285:36 notification service application class
285:38 and in here I'm just going to usually we
285:43 wanted to send out the
285:46 an email notification we want to do send
285:49 out an email notification and as we are
285:52 going to keep this project pretty simple
285:54 I'm not going to add any business logic
285:56 here you are free to do it on your own
285:59 like you can add the necessary
286:00 dependencies and you can try to you know
286:03 expand the functionality of this project
286:06 but I'm just going to going to focus on
286:08 the micro Services part so I'm not going
286:10 to add the logic here but I'm just going
286:12 to add a log message
286:15 to acknowledge that we indeed receive
286:17 this message inside the notification
286:19 service so I'm going to make use of The
286:23 slr4j annotation from Romberg which uses
286:27 the slr4j library to provide a logger
286:31 implementation so I'm going to add the
286:34 log messages as log dot
286:37 info
286:39 received notification
286:42 for order
286:44 and I'm going to
286:48 add a placeholder for this and Define
286:51 the placeholder value as order placed
286:54 event dot get order number
286:57 right so whenever a message is placed
287:01 and an order placed event is raised we
287:03 are going to place a message from our
287:05 order service into the Kafka topic and
287:09 the notification service is going to
287:11 listen to this Kafka topic and it's
287:13 going to just print out this message
287:15 receive notification for the order
287:17 number right so let's run this
287:20 notification service now and test
287:23 whether everything is working fine or
287:25 not
287:26 and one thing I forgot to configure here
287:28 is our notification service so we didn't
287:31 add any application.properties file here
287:33 so I'm going to do that by quickly
287:36 clicking on new file
287:39 application Dot
287:42 properties so I completely forgot to add
287:45 the Eureka client sleuth and Zipkin
287:48 support to our notification service so
287:50 for that I'm going to go back to the
287:53 start.spring.io website and in here I'm
287:56 going to add some more dependencies the
287:59 first one is going to be Eureka
288:01 Discovery client for spring Cloud
288:03 discovery
288:04 I am going to add the dependencies for
288:06 Zipkin
288:08 for spring sleuth Cloud sleuth so after
288:13 adding all these three dependencies I'm
288:15 going to click on explore one more time
288:18 and I'm going to copy the relevant
288:20 dependencies from the palm.xml starting
288:24 from this
288:26 spring Cloud sleuth Zipkin spring Cloud
288:29 starter networks Eureka client and
288:31 finally spring Cloud starter sleuth
288:34 so I'm going to copy these three
288:36 dependencies
288:37 paste them inside the palm.xml
288:41 let's add this
288:43 below the spring Kafka dependency
288:47 then click on this Maven icon to load
288:50 the maven changes
288:52 so my win will try to download these
288:54 dependencies in the background
288:56 and now now we are
288:59 ready to add the necessary configuration
289:01 inside the application dot properties
289:04 file so I'm just going to drop in some
289:06 configuration code here so the first one
289:09 is the familiar Eureka client service
289:12 URL and the default zone I'm going to
289:15 provide the same default Zone I have
289:16 provided for rest of the services I have
289:19 to I am going to provide the application
289:21 name as notification service and we are
289:24 also going to run this notification
289:26 service application on a random Port so
289:28 that's why I provided the the
289:30 server.port as 0 and we are also going
289:33 to enable the spring sleuth integration
289:36 so for this the spring sloth integration
289:38 enabled is going to be true
289:40 and we are also we also have to provide
289:43 the URL for Zipkin so that it will send
289:45 out the trace information to Zipkin so
289:48 we have provided the URL of Zipkin
289:50 installation and we also have provided
289:52 the sampler probability as one as we did
289:55 for the remaining Services right so the
289:59 one more property we have to add is to
290:01 use to configure Kafka in our
290:03 notification service I know notification
290:06 service project so for that I'm just
290:08 going to make use of the property which
290:11 I've defined inside the the order
290:13 service so I'm going to open the
290:15 application.properties file so I'm going
290:17 to copy all these four properties which
290:19 I have defined inside order service and
290:21 I'm going to paste them inside the
290:23 notification service
290:26 like this and we have to make some
290:29 modifications to these properties so we
290:32 have defined the serializer on the
290:35 producer side because as you want to
290:36 serialize this um no key and value pair
290:39 and send it send it to the Kafka broker
290:42 and on the consumer side we have to
290:45 deserialize this information right like
290:47 whatever information which is coming as
290:49 part of the as part of the Kafka broker
290:51 we have to deserialize that read this
290:54 information so for that we have to
290:56 define a d serializer for key and value
290:59 objects right so for that we can make
291:02 use of the property spring dot Kafka dot
291:05 consumer instead of producer
291:09 and here we have to provide the
291:14 key as dot key
291:16 deserializer right and we have to
291:19 provide the
291:22 name of the deserializer as
291:24 string deserializer so
291:28 I can select the string B serializer
291:31 Class here and likewise we also have to
291:34 select the Json deserializer
291:37 this one Json deserializer for
291:42 spring dot Kafka dot consumer
291:47 dot value desertizer
291:51 yeah so now we are so now our springboot
291:56 application will understand how to
291:58 deserialize the incoming key and value
291:60 pairs that's good and one thing we also
292:03 have to keep in mind here is inside our
292:06 order service we are so let me go into
292:11 the order service
292:13 and here we are trying to serialize this
292:16 order placed event with fully qualified
292:19 name as com programming techie order
292:22 service dot event dot order placed event
292:25 right but coming to our notification
292:27 service let me open this order placed
292:30 event again the fully qualified class
292:32 name is going to be com programming
292:35 techie dot order placed event right so
292:38 whenever we receive this information
292:39 from product service to the notification
292:42 service springboot will not understand
292:44 how to map these two different order
292:46 placed events which has two different
292:48 qualified names right so for that we
292:51 have to provide the mapping type on site
292:54 in the producer side and also the
292:56 consumer side so on the producer side we
292:59 will Define what exactly is the type
293:01 which we are sending for the order
293:02 placed event and in the consumer side
293:05 will also Define what is the exact type
293:08 we are expecting so for this reason I am
293:12 going to go back to the so for this I'm
293:15 going to go back to the order service
293:17 dot properties order service
293:19 application.properties file and I'm
293:22 going to add a property
293:24 called
293:26 spring.kafka.producer.properties and
293:28 here we are providing the Json type
293:30 mapping as
293:32 Json type mapping and as part of this
293:34 Json type mapping we are providing one
293:36 token value followed by the fully
293:38 qualified class name of the body we are
293:41 sending as part of the message right so
293:43 the token value I have provided as some
293:45 random value as event and we have
293:48 provided the fully qualified class name
293:49 of the order placed event class right so
293:53 in this way springboot will understand
293:54 what so what is the Json type mapping of
293:57 the or replaced event right so we also
293:60 have to Define this property inside the
294:02 notification service so I'm going to add
294:05 this property again here
294:08 and this time this is going to be
294:10 spring.kafka Dot consumer.properties
294:14 right and spring Json type mapping
294:17 should have the same token as we defined
294:20 inside the
294:22 no order service properties file so it's
294:24 going to should be the same token as
294:26 event and the fully qualified class name
294:30 can be the fully qualified class name of
294:33 the order placed event inside the
294:35 notification service so that's going to
294:37 be com dot programming dot techie
294:41 and
294:43 followed by the
294:45 class name right so it's com programming
294:47 techie.org placed event
294:49 so let's verify if it is the same or not
294:52 so it's common programming or replaced
294:54 event right so now spring boot will
294:56 understand how to map these two
294:58 different no classes which is coming
294:59 from two different which has two
295:01 different fully qualified class names
295:04 so now that's all we have to do on our
295:06 order service and notification service
295:08 so we can go ahead and start all these
295:11 Services now and test whether the
295:13 functionality is working fine or not so
295:15 let's go ahead and do that all right so
295:17 now you may have observed that we are
295:19 getting a error when starting the
295:21 notification service application class
295:23 so it says an illegal State exception no
295:26 group ID found in a consumer config
295:28 container Properties or Kafka listener
295:31 annotation a group ID is required right
295:34 so what this is saying is that we have
295:37 to define a group ID because we have to
295:39 define the consumer group and also
295:41 Define a group ID on the consumer side
295:43 inside the application.properties of the
295:46 notification service project so for that
295:49 I am just going to Define uh arbitrary
295:51 ID cordless notification ID and I'm
295:54 going to you know assign it to the key
295:56 as assign it to the property
295:58 spring.kafka dot consumer dot group ID
296:01 equals notification ID right after
296:04 defining this property let's no start
296:07 the notification service application one
296:09 more time and this time this
296:13 application this class should be
296:15 starting successfully without any errors
296:17 and let's see like yeah it looks like
296:21 our application is started successfully
296:23 and you can see that we have adding the
296:26 log message adding newly assigned
296:27 partitions notification topic right so
296:30 it's subscribe to this notification
296:32 topic
296:33 so that's good now let's try to place an
296:35 order so I'm going to open Postman and
296:39 first of all we need to get a new access
296:41 token from Key clock so I'm going to
296:44 click on this authorization tab
296:46 and I'm going to scroll down and click
296:48 on get new access token click on proceed
296:51 so now we have the latest token and
296:54 let's click on this use token button so
296:56 that it will be added to this you know
296:58 header section
296:59 and
297:02 looking at the body we have created
297:04 order line items with screw code AS
297:06 iPhone 13 and let's click on send and
297:10 you can see that we got the message
297:12 ordered placed successfully so now let's
297:14 check the order service if we see any
297:17 errors or not so we don't see any errors
297:20 at least in the order service
297:22 and in the notification service
297:25 we can see a log message received
297:29 notification for order and we are able
297:31 to see the order ID here we are able to
297:34 see the order number here right so this
297:37 is how we can develop the given driven
297:40 applications using microservices and
297:43 using the Kafka message broker so I hope
297:45 you learned something new and in the
297:47 next part we will go ahead and dockerize
297:51 all our services
297:52 using Docker compose and in the
297:55 subsequent part we will you know migrate
297:58 this Docker compost to kubernetes and
297:60 then also refactor the architecture to
298:02 suit to kubernetes right so I will see
298:05 you in the next video Until Then happy
298:07 coding techies welcome to Part 9 of the
298:09 springboard micro Services project
298:11 Series so up until the last part you saw
298:14 how to develop our microservices around
298:16 different components and now let's
298:19 concentrate how to run these Services
298:22 together so as part of the first step we
298:25 are going to dockerize all our services
298:27 and then run them together using a tool
298:31 called as Docker compose right so I'm
298:34 not going to go into the basics or
298:37 fundamentals of what is Docker and what
298:39 is stalker compose and how to work with
298:40 them I am assuming that you have some
298:43 basic understanding about what what both
298:45 Docker and Docker compost and if you
298:47 want me to make a video like a Docker
298:50 tutorial for Java developers so just
298:52 give me a comment down below I will try
298:55 to cover everything about Docker no how
298:57 to run your Docker images for especially
298:59 for spring boot applications and what is
299:02 the best practices of maintaining the
299:04 docker files and running the docker
299:06 files and what are the latest ways to
299:09 use the docker files and all so if you
299:12 are interested just drop a comment below
299:13 and I will try to make a video for that
299:15 so let's go ahead and start drizing our
299:18 project all right so the first thing you
299:19 need is you need an installation of
299:22 Docker desktop on your machine so if you
299:24 are using a Mac or Windows operating
299:26 system then you can just make use of
299:29 either Docker desktop for Mac or Docker
299:32 desktop for Windows installations you
299:35 can now find out all these here inside
299:37 the docker desktop website inside the
299:40 docker website so after installing
299:42 Docker desktop on your machine you also
299:45 need to register for Docker Hub this is
299:48 where you will push your Docker images
299:49 after you've built the docker images on
299:52 your local I would suggest you to know
299:54 register for an account and the docker
299:57 Hub and once you have installed Docker
300:00 desktop make sure you also sign in
300:03 inside the docker desktop you will see
300:05 an application like this and once you
300:07 are logged in you will be you can see
300:10 your account at the top here like that
300:12 this is a free account you don't need to
300:15 pay for this account or anything like
300:17 that all right so now I'm back inside my
300:19 IntelliJ IDE so let's start with dock
300:22 Rising one of a service in our project
300:24 so we will start with the API Gateway
300:26 project
300:27 so I am going to make use of the
300:30 existing code I'm just going to explain
300:32 what we how we can dockerize our project
300:34 I'm not going to type in every command
300:37 and along with you so whenever you are
300:39 stuck you can now refer to the GitHub
300:41 project and if you have any doubts you
300:44 can refer to the branch
300:46 Part 9 here so you will find the
300:50 complete source code of all the docker
300:51 compose files and or the docker files
300:53 and all the configuration inside this
300:55 particular Part 9 Branch right so let's
300:58 go back to our project and I'm going to
301:01 expand the API Gateway folder here and
301:05 let's start with building a Docker file
301:07 let's start with dockerizing this API
301:09 Gateway service so for that first of all
301:12 I created a file called as Docker file
301:15 inside the root folder of the API
301:17 Gateway project and if you will have a
301:19 look at the stalker file it is a very
301:21 plain simple Docker file so we are using
301:24 this open jdk 17 as a base image and
301:27 after that we are copying the jar file
301:29 which will be present usually inside the
301:32 target folder into the container and we
301:35 are calling this jar file as app.jar
301:37 file and after that we are providing the
301:39 entry point command as Java jar app.jar
301:43 right so whenever the container is
301:45 running up is starting up it will run
301:48 the command Java minus jar and it will
301:51 point to this app.jar file so let's try
301:53 to build this Docker file so for that I
301:55 am going to open the terminal and make
301:57 sure I am inside this API Gateway
302:01 folder and to build this Docker file I'm
302:04 going to type in docker build minus t
302:08 and minus t so that means we are
302:11 providing we are building this image and
302:13 we are providing a tag for this image
302:15 right so what is the tag we can give is
302:18 API
302:20 Gateway
302:22 Docker file and we have to provide the
302:24 location where the docker file exists so
302:27 we are going to provide the present
302:29 folder as a location by providing the
302:32 dot here so that means Docker will
302:35 understand that the docker file exists
302:37 in the present directory right so if I
302:40 press enter so first of all it will try
302:42 to download this open jdk 17 image from
302:45 Docker Hub so you can see that this
302:47 Docker build phase is completed
302:49 successfully so now let's see how this
302:52 image looks like I'm going to type in
302:54 Docker images and these are all the
302:56 images I have in my local so we are
302:59 mainly interested for the AP API Gateway
303:03 Docker file
303:04 image so you can see that this is built
303:08 for 57 seconds ago and it has a size of
303:12 471 MB as you can see that our Docker
303:16 image is
303:18 is of size 471 MB
303:21 so we are not actually following some
303:23 best practices here and our Docker image
303:25 size is also pretty big it says 471 MB
303:29 there is some improvements we can do so
303:31 let's see how to do that in the next
303:33 section so one of the drawbacks we can
303:35 think about this Docker file approach is
303:38 first of all we are making use of jdk
303:41 instead of JRE so that's one thing and
303:44 the other thing is no matter what small
303:46 change I do in my API Gateway project
303:48 that means if I try to update one no
303:51 from one dependency one version of this
303:53 dependency in the form.xml or change the
303:56 application.properties this Docker file
303:59 will build the whole image one more time
304:02 right it does not have any contextual
304:05 understanding of what change and what
304:08 should be built what's what's the only
304:10 part we should build so for this reason
304:12 to improve the process of building the
304:15 docker files Docker has introduced
304:17 something called as multi-stage builds
304:19 so let's have a look at how to improve
304:22 our Docker file using the multi-stage
304:24 build mechanism right so for that I
304:27 created another file called as Docker
304:30 file dot layered and this file contains
304:33 a bit more code than the docker file we
304:36 saw before so let's see what we are
304:38 doing here so you can see that first of
304:40 all we are making use of the JRE this
304:44 Eclipse tamurin 17 JRE instead of the
304:47 open jdk and we are providing an alias
304:50 for this base image called as Builder
304:54 right and after that we are we are
304:56 creating a work directory called as
304:58 extracted and after that we are similar
305:01 to the docker file we are creating we
305:03 are adding this no uh jar file inside
305:06 the target folder into the container and
305:08 we are calling it as app.jar file right
305:11 and next step is we are running a
305:14 command called as Java minus D jar mode
305:17 equals layer tools right so what exactly
305:21 this will do is as part of java 9 or
305:24 Java 10 I'm not sure Java supports no
305:27 extracting different layers from the jar
305:29 files so if you have a look at our
305:31 springboot application it is divided
305:33 into different layers right so if I open
305:36 the target folder and if you try to open
305:39 this jar file you will find there are
305:41 different layers to our Springwood jar
305:44 file right so those layers will be all
305:46 the dependencies will be stored in a
305:48 folder called as dependencies all the
305:50 spring boot loader information is stored
305:53 in the spring boot load project in the
305:55 loader folder right and all the snapshot
305:58 dependencies are stored in the snapshot
306:00 dependencies folder and all the source
306:03 code application properties everything
306:04 which is inside the source main Java
306:06 folder will be stored inside the
306:08 application folder so for our spring
306:11 boot applications these are the
306:12 different layers which our jar Will
306:15 Spring boot jar will provide right so by
306:17 running this command Java minus the jar
306:20 mode layer tools what it will do is it
306:21 will extract all the layers this four
306:24 layers from our spring boot jar and it
306:26 will store these four layers four
306:29 folders you can say inside the four
306:31 inside the folder called as extracted
306:33 right so for this reason we are
306:35 providing also the command called as
306:37 extract which will extract all the
306:39 layers from your jar file and store it
306:41 inside the extracted folder so this is
306:43 the first stage of the docker bill we
306:45 wanted to and we want to do a second
306:47 stage also right so we want to copy all
306:50 these layers from our extracted
306:52 directory and we want to no run and we
306:56 have to copy this inside the and inside
306:58 another folder right so for this reason
307:01 we are creating another stage so similar
307:04 to the first stage we are making use of
307:06 the eclipse timer in JRE and we are now
307:10 creating another work directory called
307:12 AS application and we are copying all
307:14 the required layers all the required
307:17 folders from the extracted folder into
307:20 our application folder so right so this
307:22 is the four steps we have we are copying
307:24 the dependencies folders playing
307:26 bootloader folder snapshot dependencies
307:28 folder and application folder so after
307:31 copying all these folders We Are
307:33 exposing the port 8080 and just running
307:37 this command Java jar launcher right
307:40 when we run this container it will
307:42 follow all these steps and this is a
307:45 better way of building the docker files
307:49 so now if all you have changed is just a
307:51 version inside the dependencies so
307:53 spring the docker will understand that
307:55 okay the only file which is changed is
307:58 inside the dependencies folder so let's
308:01 build only this particular dependencies
308:02 folder or just copy this dependency
308:04 folder into our application work
308:07 directory and best of all will be the
308:09 same right so in this way you can you
308:12 can save the build times you can save
308:14 some time when building the image all
308:17 right so now let's go ahead and try to
308:19 build this this layer Docker file so for
308:21 that I am going to open the terminal and
308:24 I'm going to type in the command Docker
308:26 build minus t and I am going to provide
308:29 the tag name as
308:31 API Gateway
308:33 layered and as we are using not the
308:37 default Docker file but we are using a
308:39 different Docker file called Docker file
308:41 Lord layered I'm going to provide the
308:44 name as minus f
308:47 Docker file dot layered because if you
308:50 don't provide this command it will
308:51 always try to build the docker file the
308:54 default Docker file right so
308:57 next thing I have to do is provide the
308:59 the present directory so for that I am
309:01 just going to provide it as Dot and now
309:03 you can see that Docker is trying to
309:05 build our image in multiple stages so
309:09 first of all it's trying to add the
309:11 app.jar file into the working directory
309:14 of extracted after that it's extracted
309:17 all the layers from the image and it's
309:20 trying to copy all the folders into the
309:23 application folder right so now let's
309:26 investigate how this image is looking
309:28 like so I'm going to provide the command
309:30 Docker images here you can see that this
309:34 API Gateway Docker file which we built
309:36 using the default Docker file is 471 MB
309:40 and this API Gateway layered image which
309:44 we built using this Docker file dot
309:46 layered file is just 316 MB this is
309:49 already good and we also have this
309:51 multi-stage builds where you can build
309:54 only the parts which I've changed in the
309:56 application
309:57 so now in the next section we will see
309:59 how to build the docker images from your
310:03 source code without using any Docker
310:05 files right how to build the docker
310:07 images automatically without using the
310:09 docker file so now let's go ahead and
310:11 check that so we can make use of a
310:13 library called as jib from Google which
310:17 is mainly used to build containers from
310:20 our Java applications without using a
310:22 Docker file or maybe you know without
310:25 installing the docker itself now you can
310:27 read more about jib in this getting
310:29 started page so it mainly provides some
310:32 plugins using Maven or Gradle and you
310:34 can see that it will take the source
310:35 code of the project and it will try to
310:38 use this local name and local Docker
310:41 installation and it will try to
310:42 understand what is the build context and
310:44 it will build the container image and it
310:47 will also push this image to a container
310:49 image registry so in our in our case it
310:52 is Docker Hub so it will do all this
310:54 still all these things as part of our
310:56 build through a miven plugin so let's
310:58 see how to install this moment plugin in
311:01 our project so I'm going to open the
311:03 form.xml the rootform.xml under the
311:06 microservices new project so I'm going
311:09 to open this phone.xml and I am going to
311:11 add this plugin section right so where
311:14 the group ID is com Google Cloud tools
311:17 and the artifact ID is jib Maven plugin
311:20 the present version we are going to use
311:22 is 3.2.1 which is the latest version and
311:26 we also have to provide some
311:27 configurations here so inside this
311:30 configurations tag we have to provide
311:32 what is the base image we want to use
311:34 right so inside this Docker file layered
311:37 we saw that we are using this Eclipse
311:39 tamurin 17 JRE as the base image so we
311:43 also have to provide the base image
311:44 using the from tag and I'm going to
311:47 provide this inside the image tag called
311:49 as Eclipse tamirin 17 JRE
311:52 right so when you provide this jib will
311:55 make use of this as the base image
311:58 and we also have to provide the two tags
311:60 so as I mentioned before jib will try to
312:03 take your project and will understand
312:04 the build context and it will build the
312:06 container image and also push it to the
312:09 container registry image registry right
312:11 so for that we also have to provide what
312:13 is the address of this container image
312:16 registry so as we are using Docker Hub I
312:18 have provided the name of Docker Hub
312:20 here it's like
312:22 registry.hub.docker.com followed by my
312:25 username of the account I created and
312:27 the next thing we are going to provide
312:29 is the name of the project it is trying
312:31 to build right so if you add this plugin
312:34 on the root folder in the rootform.xml
312:37 this plugin will be available for all
312:39 the projects right so if I'm trying to
312:42 build the API Gateway image so I have to
312:45 provide the image name as API Gateway
312:47 right so for this particular reason I am
312:50 going to use the maven property called
312:53 as artifact ID so if I open the API
312:55 Gateway form.xml the artifact LED is API
312:58 Gateway right so in this way we can make
313:01 use of the artifact ID to build the
313:03 docker images so after adding this
313:06 plugin you don't need to do anything the
313:09 docker Maven plugin will try to build
313:12 all the images so for that all you have
313:15 to do is to have to type one command so
313:18 you can see what is that command but
313:20 before that you can see the different
313:22 goals which are part of this plugin I am
313:24 going to click on this Maven icon and if
313:27 you open this micro Services new project
313:29 and go inside plugins you can see the
313:31 plugin called as zip and if I expand
313:34 this you can see the different goals
313:36 which are part of this plugin and the
313:39 goal which we are interested is jib
313:42 build right so this will build the
313:44 docker image and it will also push the
313:46 docker image to the docker Hub and if
313:49 you want to just build the docker image
313:50 without pushing it you can make use of
313:52 the docker build goal now let's try to
313:54 create Docker image using the zip build
313:57 command so for that I am just going to
313:59 open the so I'm just going to open the
314:02 terminal
314:03 I'm just going to clear this and I'm
314:05 going to go back to the previous folder
314:07 under the micro Services new folder and
314:10 I'm going to type in Maven clean compile
314:14 chip
314:15 build
314:17 right so if I try to run this command it
314:20 will try to build all our projects and
314:23 you can see that we are getting this 401
314:25 unauthorized exception right because you
314:28 want to push this Docker image to Docker
314:31 Hub but we didn't provide any
314:33 information about what is the username
314:34 and what is the password right so for
314:37 this reason we have to provide this
314:38 information under the settings.xml file
314:40 so let's open this so if I open my
314:43 settings.xml file you need to add this
314:46 particular tag under the servers tag so
314:48 I am going to provide so I have to add
314:50 this particular server tag provide
314:53 followed by the ID as
314:54 registry.hub.docker.com the username has
314:57 your username which you have provided in
314:59 the docker Hub followed by the password
315:01 right so once you have added the
315:04 necessary information here and you
315:06 should try one more time and I'm going
315:08 to run this clean compile zip build
315:10 command one more time all right so this
315:13 time you can see that the build is
315:15 successful it took 42 seconds to run
315:17 everything on my machine and you can see
315:19 that this images are created
315:20 successfully by typing the docker images
315:23 command and you can see all the images
315:25 which are built in my local and where
315:29 let's also verify whether these images
315:31 are pushed into Docker Hub or not so I'm
315:34 going to open Docker Hub and I'm going
315:36 to sign into my account okay so I have
315:38 logged into my account and you can see
315:40 that this all the services notification
315:42 service API Gateway so on they are
315:45 pushed a minute ago so they are pushed
315:47 they are pushed now into the docker Hub
315:49 so we can make use of these images and
315:52 we can try to run all these download all
315:56 these images and run those containers
315:57 using Docker compose so now let's go
315:59 ahead and see how to make use of Docker
316:01 compose to run all these containers
316:03 alright so now as we have all our Docker
316:06 images in the docker Hub the next step
316:09 is to create the docker compose file
316:11 right so for that we can refer to this
316:14 Docker compose dot EML file and the root
316:16 for folder of the microservices new
316:18 project so if I open this Docker compose
316:21 file so let me collapse all the sections
316:24 so that I can explain first so you can
316:27 see that we have different Services as
316:30 part of this Docker compost file the
316:32 first service we have is the group
316:34 called as MySQL Docker compost config so
316:37 if I open the
316:39 architecture diagram one more time you
316:41 can see that our order service and the
316:43 inventory service is making use of the
316:45 MySQL databases right and our product
316:47 service is making use of mongodb and the
316:50 odd server is actually key clock which
316:53 is actually we'll use another micro
316:55 MySQL DB instance right so if you want
316:58 to run the my key clock you also have to
316:60 provide the database instance so for
317:02 that I am also going to use MySQL right
317:04 so first of all we are going to set up
317:06 all these external services in our
317:08 Docker compose file so that's why we
317:11 have this two instances for mySQL
317:14 database so the first one we have is
317:16 called as postgres order so this is the
317:19 service which is present which is
317:21 dedicated only for our order service
317:23 right so we are providing the container
317:26 name as postgres order and the image as
317:29 postgres so it will download directly
317:31 from the postgres
317:33 docker image registry and we are also
317:35 providing some environment variables the
317:38 first one is foreign
317:40 is going to provide it as order service
317:43 the followed by the username and the
317:45 password
317:46 right and we also have to provide the
317:49 environment variable called PG data
317:51 where it will store all the database
317:53 related information right so for this we
317:56 are going to store it under the slash
317:58 data slash postgres folder next we also
318:00 have to provide the information about
318:02 volumes right so if you want to run a
318:05 database usually Docker containers
318:07 doesn't store any data inside them right
318:09 so whenever you want so usually the
318:11 docker containers are ephemeral in
318:13 nature so if you run a Docker container
318:15 and try to insert a table and try to
318:18 create a table and insert a record and
318:20 if you restart that Docker container the
318:22 data will be lost right it will not be
318:24 persisted so for this reason we want to
318:26 create something called as volumes where
318:29 you can map the data folder inside the
318:32 docker container into our folder in our
318:35 host right so I want to create a volume
318:38 so that all the data inside the slash
318:41 data slash postgres folder will be
318:43 stored inside my own machine in a folder
318:46 known as postgres order right so this
318:49 looks something like this right I've
318:51 already created and run this Locker
318:53 compost that's why I have this postgres
318:55 order folder ready so basically you will
318:58 see that all the data will be created
319:00 like this in your folder and we want to
319:03 expose the port 5431 by default it will
319:06 use five four three two five four three
319:08 two Port but we want to make we want the
319:12 postgres order service to run on 5431
319:16 inside the docker container and also I
319:19 want to expose my host my own machines
319:22 5431 port to access this postgres order
319:25 database right and as we are not using
319:28 the default Port I also have to provide
319:30 the command as minus P 5431 so that
319:34 Docker will understand that postgres
319:36 should run on Port 5431 right I also
319:40 have provided the restart
319:42 um setting as unless stopped right so
319:45 similarly we also have another instance
319:47 for the inventory service called as
319:50 postgres inventory so we have the
319:52 container name as postgres inventory the
319:54 image and all the environment variables
319:57 we discussed before and also the volume
319:59 information
320:00 and here as we are you and here I'm
320:03 going to use the default Port 5432 so
320:06 that's why I'm just provided this sports
320:08 information without the without adding
320:11 any command here right because as we I
320:14 want to use the default port for the
320:16 inventory service and next we have the
320:19 docker compose configuration for mongodb
320:21 so I am going to
320:23 call this container as so that's
320:27 why we have this container name as
320:29 and I'm going to make use of this
320:31 image 4.4.14 and here also I'm going to
320:35 provide the restart policy as unless
320:37 stopped and I'm going to expose the
320:39 ports to 7017 which is the default Port
320:43 of mongodb and similar to postgres DB I
320:46 am also going to you know create a
320:49 volume to map all the data which is
320:51 inside the slash data slash DB folder
320:54 inside the mongodb container into my uh
320:58 into the mongodata folder inside my own
321:01 machine right you can also see this here
321:04 to the left side this is the data this
321:07 is how it looks like this folder next I
321:09 am going to create some configuration
321:11 for key cloak so first of all let's
321:14 start with the key clock MySQL
321:17 image so this container name is going to
321:20 be key cloak MySQL and this image is
321:23 going to be from MySQL 5.7 and similarly
321:27 I'm also going to create a volume from
321:30 the method from the folder where live
321:32 MySQL to this MySQL keycloak data folder
321:36 on my local machine I'm also going to
321:38 provide some environment variables like
321:39 MySQL root password database user and
321:42 password
321:43 right so now let's go to the next
321:46 configuration for key clock itself so
321:48 the container name is going to be key
321:50 clock I'm going to make use of the image
321:53 key clock 18.0 so this is how so this is
321:57 the full image name I'm going to use and
321:60 the environment variables we have
322:01 provided some environment variables
322:03 about the database and also the keyclock
322:05 admin user and the admin password here
322:08 so we are going to make use of the ports
322:10 8080 here so that's why I provided 8080
322:13 as both ports inside the container and
322:16 also the outside the container and
322:18 similar to the databases MySQL postgres
322:21 and mongodb we also want to copy the
322:25 data from Realms so we also have to copy
322:29 some metadata about the Realms we
322:31 created from opt geek log data import to
322:34 the realms right and uh we are also
322:38 running some commands called as start
322:40 Dev and import real right so if you run
322:43 this import realm command what it will
322:45 do is it will download automatically
322:47 this realm information from our host
322:50 machine to the docker container so if I
322:53 open this Realms command you can see
322:54 that I have added this realm export.json
322:57 file which is actually the complete
322:59 metadata about key clock so if you
323:02 remember in the previous parts we have
323:04 actually created the realm the client
323:06 and also all the information inside
323:09 keyclock so I've exported all this
323:11 information inside into this Json and so
323:14 if you start this container key clock
323:16 container you don't need to create this
323:18 realm then the clients one more time
323:21 everything will be present at the time
323:23 of startup and we also are depending
323:25 upon key clock MySQL so if you want to
323:28 start key clock Docker compose will make
323:30 sure that it will start it only after
323:33 the key clock MySQL no contain is up and
323:37 running successfully right if for some
323:39 reason key clock MySQL container is not
323:41 running or it has some issues it will
323:44 stop the key glow container so now let's
323:46 move ahead now we came to the Zookeeper
323:50 and the Kafka broker configuration I am
323:53 going to skip this because I've already
323:55 explained this about this configuration
323:58 in the previous part about the inventory
324:01 and architecture so I'm just going to
324:02 skip this part so now let's go ahead to
324:05 the next configuration called as a
324:07 Zipkin so here for Zipkin I am going to
324:10 provide the image name as open Zipkin
324:12 Zipkin I'm going to provide the
324:15 container name as Zipkin and I'm going
324:17 to make use of the I'm going to export
324:19 the default ports for Zipkin 9411 from
324:23 our Docker container into our Docker
324:25 into our Docker host alright so now
324:27 let's go through the services which we
324:29 developed in our micro services our
324:31 project so this will be our Discovery
324:33 server our API Gateway and all our other
324:36 services so we'll first start with the
324:39 Eureka server so the image we are going
324:43 to use is the Discovery server image
324:46 which we built as part of no the maven
324:49 compile zip build command so I got this
324:52 image Name by logging into my Docker hub
324:56 portal so let me open my Docker Hub and
324:59 in here I'm logged in to my Docker Hub
325:01 account and if I click on Discovery
325:02 server
325:05 and click on text
325:08 you can see that I did not create any
325:11 tag but usually it's a better practice
325:13 to create a tag so as I did not create
325:16 any tag it's updating in the latest tag
325:19 so it's using a default latest track so
325:22 what I did was I'm just copying this
325:24 information here this name
325:27 and I just pasted it here inside the
325:29 image section
325:30 right so this is the image name we want
325:33 to use so what Docker compose will do is
325:36 it will download this particular image
325:38 during the at the time of startup and it
325:41 will try to build this Docker image
325:43 right so we have given the container
325:45 name as Discovery server and we have
325:47 also defined the pull policy as always
325:50 so whenever we try to start up Docker
325:52 compost it will
325:54 pull this image every time and it will
325:57 build this image right so the ports we
326:01 are using 8761 as you know 8761 is the
326:03 default Port of Eureka server
326:05 and we are adding one environment
326:08 variable called as spring profiles
326:10 active docker right so we are providing
326:14 this profile name as Docker and what we
326:17 are doing inside our project is let me
326:20 open the
326:21 a discoveries of a project
326:23 and in here
326:26 I created another properties file called
326:29 AS application Docker properties so this
326:31 will be active whenever the environment
326:33 variables like whenever the profile
326:35 which is activated will be Docker right
326:37 so if you look at this application
326:40 Docker dot properties so we have defined
326:42 the server.port As 8761 and we also are
326:46 defining the spring Zipkin base URL as
326:49 HTTP Zipkin colon 9411 right the
326:54 difference between application Docker
326:56 and applicational properties here is the
326:58 Zipkin based URL because when you are
327:01 running the discovery server on local we
327:04 have this Zipkin URL as local so that's
327:07 why we put it as localhost the hostname
327:09 but as we are now running our Zipkin in
327:12 a different container we want to know
327:16 point to that Zipkin container using the
327:19 container team right so that's why we
327:21 created the separate Docker profile and
327:23 we have defined the Zipkin URL
327:27 so in this way it will point to this
327:29 Zipkin Docker container so let's go back
327:32 to the docker compose CML file and this
327:35 particular Discovery server container
327:38 will depend on Zipkin because first we
327:40 have to make sure that Zipkin is up and
327:43 running and then only the discovery
327:45 server can start so now let's go ahead
327:47 to the API Gateway service so we have
327:50 similarly for the discovery server the
327:52 image name as API Gateway with the tag
327:55 as latest
327:56 the container name is API Gateway the
327:58 full policy is also I am setting it as
328:01 always and here the ports
328:05 I'm exposing the port 8181 because 8080
328:09 is the default port for API Gateway as
328:13 we are already using 8080 for key clock
328:17 as you can see here I want to expose the
328:20 API Gateway on Port 8181
328:23 right and we are also defining the
328:26 environment variables similar to the uh
328:30 Discovery server spring profiles active
328:32 as Docker and we are also adding another
328:34 property logging level for the Spring
328:36 Security as Trace right so if I open the
328:40 API Gateway and the
328:43 application.co.properties you can see
328:45 that
328:46 our server.road is 8080 the Eureka
328:50 client service URL for the default Zone
328:52 I have provided it as username as Eureka
328:54 password as password and instead of
328:57 localhost we are pointing to the
328:59 Discovery server running on the docker
329:01 container so that's why the host name is
329:03 the container name Discovery server
329:06 right similarly for key clock I have
329:09 changed the source server JWT issuer URL
329:12 has HTTP keyclock 8080 instead of no
329:15 localhost 8080 was defined inside the
329:18 application.properties file so if I
329:20 scroll down you can see that the JWT
329:23 issue URI we are pointing to the Local
329:25 Host 8181 which is the local Docker
329:29 installation right so for the docker
329:32 container I have provided the hostname
329:34 as key clock 8080 and similarly we also
329:37 have the Zipkin base URL as Zipkin 49411
329:41 so let's go back to our Docker compose
329:45 file and similarly we have the product
329:47 service so I'm not going to repeat this
329:50 information one more time so this
329:52 product service is mainly dependent on
329:54 the mongodata database the discovery
329:57 server and the API Gateway right so this
330:00 makes sense because our product service
330:02 will run only when all these three
330:05 services are up and running successfully
330:07 right if any of this service is not
330:08 working then also a product service will
330:10 not start up
330:12 right and coming to the order service
330:14 it's uh same again we are making use of
330:17 the order service image with the latest
330:19 track uh when one interesting thing here
330:21 is we are providing this spring data
330:24 source URL as the property and this URL
330:27 is jdbc postgres SQL order 5431 order
330:31 service right so I can also Define this
330:35 inside the application docker.properties
330:38 directly so let me open the order
330:40 service Source main resources
330:44 I've also defined the spring data source
330:47 URL here but if you want to override
330:50 this information through the docker
330:52 compose you can do it by using this
330:56 data source URL variable right and this
330:60 is auto service is dependent on postgres
331:02 order our Kafka broker Zipkin Discovery
331:06 server and the API Gateway so if you go
331:08 to the application dot Docker properties
331:11 for order service you can see that I
331:13 have additionally defined the driver
331:15 class name as postgresql.driver the data
331:19 source username and password Zipkin
331:21 based URL pointing to the Zipkin
331:22 container the Utica client which is
331:26 pointing to the Discovery server and
331:28 another
331:29 uh property we are adding is the spring
331:32 dot Kafka bootstrap service property and
331:35 this is pointing to the Kafka broker
331:37 Docker container and we are using the
331:40 port 29092 instead of 9092 so if I open
331:44 the application.properties here and
331:46 scroll down you can see that on just
331:49 application default application
331:50 properties we are using localhost 9092
331:53 as here we are communicating from our
331:57 host machine like this on my machine I
331:59 am communicating with the docker
332:01 container Kafka running inside the
332:03 docker container right but in this
332:06 instance we want to communicate from one
332:09 Docker container that means this order
332:11 service inside a Docker container to a
332:13 Kafka running inside another Docker
332:15 container right so in this instance we
332:18 have to use the 29092 port
332:20 why we have to use this port so let's go
332:23 to the docker compose and let me go back
332:26 to the Kafka broker environment
332:29 variables and here you can see that we
332:32 have this Kafka Advanced listeners right
332:34 we have defined two listeners here plain
332:37 text has localhost 9092 that means if I
332:40 want to communicate with the docker host
332:42 I will use this localhost 9092 and if I
332:46 want to communicate between two other
332:49 containers I am going to use the plain
332:51 text internal property right and this is
332:55 broker 29092 so that's why we are using
332:58 broker 29092 when now configuring the
333:02 application Docker dot properties for
333:04 the order service
333:05 I hope this makes sense
333:07 so let's move ahead
333:10 into the docker compose file
333:13 let's go to now we are done with the
333:16 order service let's see the inventory
333:19 service so again same we have the
333:21 container name as inventory service
333:23 image name has inventory service latest
333:25 full policy as always and we are also
333:29 defining the data source URL for our
333:32 inventory service by providing the
333:33 hostname of the postgres inventory
333:36 database and as we are using 5432 as the
333:40 port name I'm providing 5432 and this
333:43 inventory service is dependent on
333:45 postgres inventory
333:47 service Discovery server and API Gateway
333:51 let's also check the
333:53 application dot Docker file
333:56 dot properties file and you can see that
333:59 I have provided server Port as 8080
334:02 spring data source URL driver class name
334:05 username password and all the servers we
334:09 have the same information here
334:11 and I think I have defined something
334:14 wrong here should be just Eureka
334:17 yes so yeah that's it so now if I go
334:21 back to the docker compost.yaml file we
334:24 have our last service notification
334:27 service here also we have the container
334:29 name as notification service image name
334:32 has notification service latest full
334:34 policy as always and we are now
334:37 dependent on Zipkin the Kafka broker
334:40 Discovery server and the API Gateway
334:44 right I hope this whole Docker compose
334:47 file is uh file is understandable to you
334:51 if you're not aware of again if you are
334:54 not aware of doco compost I can make a
334:56 full-fledged video tutorial for you just
334:59 let me know in the comment sections
335:01 and so now all we have to do is start up
335:05 this all our services using the docker
335:08 compose command so now let's go ahead
335:10 and do that
335:26 so now I have opened the terminal and
335:29 here I'm going to type the command
335:30 docker
335:32 compose up
335:34 with minus D flag right so what this
335:37 will do is it will now pull all the
335:40 images which are required for all the
335:43 services which are defined inside the
335:45 docker compose file and it will run
335:47 those containers and it will run the
335:49 containers for those images in a demon
335:52 mode if I just put if I just now type
335:55 enter
335:56 you can see that it is trying to pull
335:59 first the services so now it is trying
336:01 to pull all the services from the docker
336:03 Hub and once they are in so it's trying
336:06 to Docker Hub and then after pulling it
336:10 will start running all those containers
336:12 once everything is done you will see
336:14 that the containers are up and running
336:17 with the state of started right you just
336:20 have to give some time so that all the
336:22 containers are up and running
336:24 and once they are up and running I think
336:27 we can go ahead to postman and start
336:29 making calls to our microservices
336:32 project meanwhile you can also verify
336:35 what are you can also have a look at the
336:38 logs to understand how is everything
336:41 going on in the background so for
336:43 example I can check the logs of Kafka
336:46 broker so I can just type Docker logs
336:49 minus F broker
336:52 and here you can see that looks like our
336:54 Kafka broker is up and running without
336:57 any errors right so that's good let's
337:00 see one of our micro service whether if
337:02 it's running if it's running fine or not
337:05 so I'm going to type Docker logs minus f
337:10 order service
337:13 and you can also see that our order
337:15 service application is started without
337:17 any problems so that's also good
337:20 now what I'm going to do is I'm going to
337:23 open Postman all right so I open Postman
337:26 and as you know the first thing we have
337:28 to do is to I have to get the access
337:30 token from Key clock right so with the
337:34 previous
337:35 um with the previous videos we already
337:37 have created this setup but what we have
337:40 to do is we know that we Define this
337:42 client ID but we also have to get the
337:45 client secret for this client ID because
337:48 as we are using Docker container now so
337:52 every time we restart our Docker
337:53 container it will try to create a new
337:56 client secret right so for that first we
337:58 have to open our key cloak I'm going to
338:01 open localhost 8080 and I'm going to
338:04 click on Administration console and here
338:06 I'm going to type in the password we
338:08 provided in the docker compose section
338:10 so that's going to be so if let's go
338:13 ahead and check it
338:15 scroll down to key cloak so it's going
338:17 to be admin and admin right so let me
338:20 log in with these credentials and you
338:23 can see that I am able to login without
338:25 any problems and I am logged in by
338:28 default into the micro service realm
338:30 right so you can see that I did not
338:32 create any Rel so it's automatically
338:34 taken from the realm export.json file
338:37 right it's already almost automatically
338:40 automatically copied this file during
338:42 the startup of the docker container
338:44 which is nice and convenient for us
338:46 so let's go to the client section here
338:49 and inside the spring Cloud client
338:52 so I'm going to click on credentials and
338:54 I'm going to regenerate the secret right
338:57 and I'm going to copy this information
339:00 go back to
339:02 Postman
339:03 and I'm going to replace the client
339:06 secret with this information right and
339:09 I'm going to click on get new access
339:10 token and you can see that this is
339:13 successful I'm going to click on proceed
339:16 and I'm going to use this token to use
339:20 this
339:21 and uh yeah now let's go ahead and try
339:24 to call the product service so I'm just
339:27 going to click on send
339:30 and you can see that we are getting a
339:32 401 unauthorized exception right this is
339:36 strange because we just got this client
339:38 secret and also the access token so to
339:41 understand what is what went wrong I am
339:44 going to open my ID
339:46 go back to the terminal and we are going
339:48 to inspect the logs of the API Gateway
339:51 project right because this is the entry
339:54 point to our microservice project right
339:57 so I'm going to type docker logs minus f
340:02 API Gateway
340:05 right so you can see that we are getting
340:07 an error JWT claim validator the ISS
340:11 claim is not valid authentication failed
340:14 the ISS claim is not valid
340:17 right so what is trying to tell here is
340:20 our token is not a valid token right so
340:25 how we can so to debug this further what
340:28 we can do is we can copy this
340:31 token and I am going to open a website
340:36 called as JWT dot IO
340:38 right I'm going to go to this website
340:40 I'm going to click on learn more about
340:43 JWT
340:45 I think this is not the one maybe
340:49 I'm just going to scroll down and I'm
340:51 going to paste my
340:53 JWT here
340:55 right so that I can inspect and find out
340:58 what went wrong because in our
341:00 API Gateway it is saying that the ISS
341:03 claim is invalid right I want to
341:05 understand what is this ISS claim and
341:07 why it is not valid right so that's why
341:09 I am going into the jwt.io website and
341:12 here I pasted my token and let's verify
341:16 what is this problem so if you check the
341:18 payload section you can see that the ISS
341:20 is HTTP localhost 8080 Realms spring
341:24 boot micro service Ram right this is
341:27 fine when you are running the
341:28 application on your local but as you can
341:31 see we are making this token we are
341:33 sending this token with ISS as localhost
341:35 right so if you send this ISS claim what
341:38 the API Gateway will do is it will try
341:40 to contact its own Local Host right so
341:43 instead of contacting its localhost it
341:45 should contact the key clock server
341:46 right so for this reason instead of
341:49 providing localhost we have to send the
341:52 token with ISS claim as key clock right
341:55 so how we can do that is I'm going to go
341:59 back to postman and here here instead of
342:01 providing the access token URL as HTTP
342:04 localhost 8080 I am going to provide
342:07 this as
342:08 key cloak 8080 right I'm going to uh
342:12 very I'm going to point to the key clock
342:15 container Docker container which is
342:16 running in our machine right so now
342:19 let's try to get a new access token
342:22 and you can see that we are not able to
342:24 contact this
342:26 key clock anymore because you are trying
342:29 to contact the docker container directly
342:31 with the host name right so to be able
342:34 to contact this Docker container from
342:37 our host what we have to do is we have
342:39 to make some small changes in our system
342:41 so I am going to open a file called as
342:45 host and I have to inform my Host
342:49 Windows my Windows host to redirect all
342:52 the traffics from Key clock to a local
342:55 host right so I'm going to add in
342:60 127.0.0.1 and I want to and I'm going to
343:03 type in this keyclock right so if I
343:06 change my host file which is in C
343:09 Windows system 32 drivers Etc host so
343:12 you have to change it in administrator
343:14 mode so if you add this particular line
343:17 our Postman will be able to contact the
343:20 key clock Docker container right so if I
343:23 click on get new access token you can
343:25 see that the authentication is now
343:26 complete so by adding this particular
343:29 line you are able to communicate from
343:31 our Local Host to the docker host
343:33 through the host name right or else
343:36 there will be some DNS problems so we
343:38 need to add this particular entry inside
343:40 your inside your host machine
343:43 keep this in mind that you need to do
343:45 this only when you wanted to contact
343:46 when you want to contact the docker
343:49 container from your local right you
343:51 don't need to do this when you are doing
343:53 it in a production
343:54 and if you are using Windows if you are
343:57 using Mac or Linux you should actually
343:59 change the slash Etc host file we can
344:04 find it inside the slash ATC folder so
344:07 now let's use this token so I'm going to
344:09 delete the previous tokens and I'm going
344:12 to click on use token here
344:14 and now I'm going to click on send
344:18 and now you can see that we are getting
344:20 a response back from product service and
344:23 this is the
344:25 um the record which I've saved to the
344:27 database
344:28 this is fine now let's try to test the
344:31 order service so I'm going to change the
344:33 url as order and I'm going to change the
344:37 HTTP method as post I'm going to click
344:40 on body and click on raw and I already
344:43 have this particular
344:44 request body ready with me if not you
344:48 can just type in this particular request
344:49 Body by pausing the video
344:51 and now I'm going to click on send
344:55 so we just have to wait for a couple of
344:57 seconds and then you can see that we are
344:59 getting the message order placed
345:01 successfully that's perfect but let's
345:04 really see whether everything is working
345:06 fine or not so I'm just going to exit
345:09 from the docker logs command I'm going
345:12 to clear out this terminal and first of
345:14 all I'm going to
345:16 verify the order service because that is
345:19 the service which is actually no we are
345:21 calling right so let's check the logs
345:24 and you can see that it's able to call
345:27 the Kafka container like the Kafka topic
345:29 like Kafka broker and send out a message
345:31 right I don't see any errors it looks
345:34 like it was able to successfully connect
345:36 to the Kafka broker and send out the
345:39 message that's fine
345:41 now let's clear this out again and I'm
345:44 going to inspect the logs of
345:46 notification service right so I'm going
345:48 to type in Docker logs minus f
345:51 notification service
345:55 and right here you can see that we are
345:57 receiving the logs received notification
345:59 for order and followed by the order ID
346:02 right that's perfect because that means
346:06 our message is also
346:09 received in the notification service and
346:11 all of our services are working well
346:13 right so this is the end of this video
346:17 guys I hope you learned how to dockerize
346:19 your applications and run those two
346:22 containers Docker containers using
346:23 Docker compose in the next video we will
346:27 refactor this Docker compose to
346:29 kubernetes and the next part and after
346:32 that we will also refactor refactor our
346:35 microservice application to use
346:37 kubernetes future instead of relying on
346:40 Futures like spring Cloud Gateway right
346:42 so I will see you in the next video
346:44 Until Then happy coding techies
346:46 welcome to Part 10 of the Springwood
346:48 micro Services project Series so in the
346:51 previous part we saw how to dockerize
346:53 our all applications inside our
346:55 microservice architecture so in this
346:57 part we will mainly concentrate on
346:59 introducing monitoring across all our
347:01 services and how to view the status of
347:05 our services in a dashboard right so to
347:09 monitor our services we will be mainly
347:11 using two softwares the first one is
347:14 Prometheus which will scrap all the
347:16 metrics from our springboot applications
347:18 and it will store it in an in-memory
347:20 database and we will visualize this
347:23 metrics using another software called as
347:25 grafana
347:26 right so now let's go ahead and
347:28 understand how to implement monitoring
347:30 in our microservice project so this is
347:32 how the the process will work so our
347:36 springboot application will make use of
347:38 the actuator endpoints so we have this
347:40 spring boot actuator which will expose
347:43 all the metrics of our application like
347:45 the jvm metrics and other metrics we
347:49 want to maintain it will be exposed our
347:51 buy our springboot application using
347:54 spring boot actuator and the Prometheus
347:57 software we will set up will pull our
347:60 spring boot actuator uh endpoint for
348:03 every you know predefined seconds and
348:06 store it inside the in-memory database
348:09 right so this Prometheus will acts as a
348:12 will act like a data source for rafana
348:15 which provides a UI dashboard so grafana
348:19 also holds Prometheus so whenever there
348:21 is a new entry grafana will you know
348:24 update it and will show the the metrics
348:27 in our dashboard
348:29 so let's set up this whole
348:31 infrastructure in our project and see
348:33 how to implement monitoring all right so
348:35 now so now I'm inside my IntelliJ IDE
348:37 and the very first thing I'm going to do
348:39 is to add the necessary dependencies for
348:43 our services to enable spring boot
348:45 actuator right so for that I'm going to
348:48 open
348:50 star.spring.io website because here I
348:52 can add the dependencies through the UI
348:55 and I can you know explore and and I can
348:58 use the palm.xml and copy the
348:60 dependencies from there right so this is
349:02 a very reliable way to get the
349:03 dependency information
349:05 so I'm going to add the dependencies
349:07 first of all for spring boot actuator
349:11 and I am also going to enable the
349:14 Prometheus support for our application
349:17 spring boot application so for that I am
349:19 going to type in Prometheus and here you
349:22 can see that this Prometheus dependency
349:24 uses micrometer to get all the metrics
349:27 and expose it in the Prometheus in the
349:30 format so that promoters will understand
349:32 right so that's what it says here
349:34 exposed micrometer Matrix in Prometheus
349:36 format and in memory dimensional time
349:39 series database with a simple built-in
349:41 UI right so I'm going to select this
349:43 particular dependency and let's see the
349:46 pom.xml so after that I am going to
349:48 click on this explore button this will
349:51 open the pom.xml so here I'm mainly
349:55 interested in this spring boot starter
349:57 actuator dependency and also micrometer
350:00 registry Prometheus dependency so I'm
350:03 going to Simply copy these two
350:05 dependencies go back to my
350:07 project
350:09 and I'm going to add this inside all of
350:12 the services right so I'm going to add
350:14 it inside the
350:17 API Gateway inside the discovery server
350:22 inside the inventory service
350:26 the notification service
350:31 product service
350:36 and lastly the order service
350:42 right after adding all these
350:44 dependencies in all our projects I'm
350:46 just going to click on this particular
350:48 Maven icon so that these dependency will
350:52 be downloaded to our machine
350:54 so after everything is downloaded
350:56 successfully we need to enable the
350:58 actuator endpoint in our application
351:01 right so but before doing that I think
351:04 IntelliJ is saying that this springboard
351:06 starter actuator is already present in
351:09 the order service so let's
351:10 search for that
351:12 and I think we have already added the
351:15 actuator before so we can remove this
351:17 dependency now
351:20 and let's update the maven configuration
351:22 one more time
351:24 that looks fine so the next thing we are
351:26 going to do is to enable the
351:29 actuator endpoints in all our micro
351:31 Services right so for that we need to
351:34 add a dependencies in the
351:37 application.properties file so for that
351:39 I am going to first open source main
351:42 resources folder and I'm going to open
351:44 the application.properties file and here
351:47 I am going to type in management
351:50 the property name management Dot
351:54 endpoints
351:56 dot web
351:58 dot exposure
352:00 dot include right so by default the all
352:04 the health endpoints will be exposed by
352:06 actuator so I also want to include the
352:10 Prometheus endpoint so that's why I am
352:12 going to type in the value as Prometheus
352:14 so by adding this value you can access
352:17 the slash actuator slash
352:21 Prometheus endpoint from our service
352:23 right so I'm going to add a small
352:27 comment
352:28 called as actuator
352:31 Prometheus endpoint
352:34 right so I'm going to copy this and I'm
352:37 going to paste this inside all other
352:40 services so first of all I'm going to
352:42 open this inside the copy this inside
352:45 the discovery server the inventory
352:48 service the notification service
352:51 and the order service and you can see in
352:55 the order service we are getting a error
352:57 that it is a duplicate property key
352:59 because we have already enabled all the
353:02 endpoints for the actuator by using the
353:05 star wildcard right so I'm now going to
353:08 remove this because we don't need this
353:11 endpoint because we don't need this
353:13 property so I am going to go ahead with
353:17 the next service the product service
353:20 and paste the same property information
353:22 inside the product service
353:25 so after adding this dependencies and
353:28 also after updating the configuration
353:30 with the inside the
353:32 application.properties file the next
353:34 step is to build the docker images using
353:38 the zip Maven plugin so for that I am
353:41 going to type in the command
353:43 clean compile jib build
353:46 so I'm going to use this particular new
353:48 and clean clean compiler build command
353:51 and now our application will be built
353:53 using the Jeep Maven plugin and this ZIP
353:56 Maven plugin will create the docker
353:58 files and will push it into the docker
353:60 registry as we saw in the previous video
354:02 so I will be back once the build is
354:05 completed all right so now the bill is
354:08 successful so the next step we have to
354:10 do is to install Prometheus and grofana
354:13 so as we are using Docker compose we can
354:15 make use of the pre-existing Docker
354:17 images and spin up Prometheus and
354:20 grafana as part of the docker compose
354:22 orchestration so for that I'm just going
354:25 to scroll down until the end of the
354:27 docker compose file and I'm going to
354:29 paste in this configuration for both
354:31 Prometheus and grafana so let me explain
354:33 this Docker compose configuration so I
354:37 am going to name this particular node as
354:39 Prometheus which stores all the
354:42 Prometheus configuration and the image I
354:44 am going to use is this promoter image
354:47 with version 2.3 37.1 which is the
354:50 latest version of the docker image the
354:52 container name I am going to use it as
354:54 Prometheus and I'm going to provide the
354:57 restore policy as until stopped and I'm
355:00 going to export the ports 9090 because
355:04 as I want to run Prometheus on these
355:06 ports and I am going to add a volume
355:11 which actually
355:13 copies the configuration the Prometheus
355:16 configuration from a file called
355:19 prometheus.yaml and this particular file
355:22 is not existing yet we will we will
355:24 create it shortly so we are instructing
355:26 Docker to copy this prometheus.yaml file
355:29 from our host machine to the slash Etc
355:32 Prometheus
355:34 prometheus.aml file during the container
355:37 startup and this Prometheus service will
355:38 be dependent on the probate service
355:40 inventory service order service and the
355:42 notification service because we are want
355:45 to mainly
355:47 monitor our main Services main business
355:50 main Services which contains the
355:52 business logic so that's why I have
355:53 added these four services but if you
355:56 want you can also add the other services
355:58 like the API Gateway and the discovery
356:00 server right so that is the
356:03 configuration for Prometheus coming to
356:05 grafana I am going to use the image
356:08 grafana OSS with the version as 8.5.2
356:13 and I'm going to provide the container
356:15 name as grafana and I'm going to use the
356:18 port 3000 to export to expose grafana
356:23 and I'm going to link this to the
356:25 Prometheus service and I'm going to
356:27 create a volume which Maps the grafana
356:30 folder to the slash where lib grafana
356:33 folder inside the docker container and
356:35 finally I'm going to add some additional
356:37 environment variables which defines the
356:40 username and password as admin and
356:42 password right so these are used when we
356:45 want to login to grafana UI so we will
356:48 see how to use that in how to do that
356:50 shortly right so after adding this
356:53 configuration
356:54 inside the docker compose file the next
356:57 step is to create the Prometheus
356:59 configuration file so for that I am
357:01 going to click right click on the root
357:03 folder and create a new folder
357:06 called as
357:09 Prometheus
357:10 and inside the Prometheus folder I'm
357:13 going to create a new file called as
357:15 Prometheus dot yaml let's add this into
357:19 the git repository and for this
357:21 configuration file so let me paste this
357:25 uh so let me paste some configuration
357:28 file so and I will you know walk you
357:30 through the whole configuration so first
357:33 of all we are going to provide some
357:34 global settings here so first one is
357:37 going to be called as scrape interval in
357:39 the previous diagram so I showed you
357:42 Prometheus will be scraping our spring
357:44 boot application for every predefined
357:46 set of seconds so this interval we will
357:49 we have set as 10 seconds here using the
357:52 script interval field and for every 10
357:56 seconds we are also going to evaluate
357:58 the internal rules inside the Prometheus
358:00 right but we don't have any rules set up
358:03 but this is the default configuration we
358:06 can use which evaluation interval has 10
358:08 seconds so what this will do is if there
358:11 are some custom rules inside Prometheus
358:13 these rules will be evaluated for every
358:15 10 seconds
358:17 right and next we have to define the
358:20 scrape configuration so inside the
358:22 scrape configuration we can Define
358:24 multiple jobs and as I mentioned before
358:26 we will be setting up the jobs to scrape
358:29 our main business services that is the
358:32 product service order service inventory
358:34 service and the notification service
358:36 right so we have four jobs we have total
358:40 of four jobs and for each job we are
358:43 going to first provide the metrics path
358:45 right so this Matrix Path is nothing but
358:48 the actual URL which the Prometheus has
358:50 to call for our product service this is
358:52 going to be slash actuator slash
358:55 Prometheus because this is the default
358:56 URL for all our springboot applications
358:59 right so this Matrix Path is going to be
359:02 the same for all the jobs as you can
359:04 observe because this is a default URL
359:07 and inside the static configs we have to
359:09 we have to provide the target URL so for
359:11 product service that is going to be
359:13 product service call in 8080 because
359:15 product service is running on the
359:17 support 8080 and we are going to define
359:19 a label called as product service
359:21 application
359:22 right similarly we have also the job
359:25 configuration for order service I have
359:27 defined the target as order service
359:29 colon 8080 because this is the URL where
359:32 the order service is running similarly
359:34 we also have the inventory service and
359:36 the notification service
359:37 so this is the main configuration of
359:40 Prometheus
359:41 so now let's go ahead and run our Docker
359:45 compose
359:46 command all right so let's start our
359:49 Docker container so let's start our all
359:52 our services using the docker compose
359:53 command so I'm going to type in docker
359:56 compose of minus d
359:59 so this will run all our services
360:04 so let's wait until all our services are
360:06 up up and running you can see that all
360:09 the services are up and running now I
360:11 can check the logs of Prometheus and
360:13 grafana to see whether they are working
360:15 they are running fine or not so I can
360:18 type in Docker logs minus f
360:21 ometheus and here you can see the
360:24 command so you can see the message
360:25 service is ready to receive web request
360:27 that's good so let's check also the logs
360:31 for grafana so I'm going to type in
360:33 Docker logs minus f
360:36 grafana
360:38 and here you can see that also grafana
360:42 is working fine it's trying to access
360:44 some default dashboards but it's fine as
360:47 we did not create any dashboards yet
360:49 so this is also fine so I'm going to
360:52 first open a web browser and I'm going
360:54 to go and so I'm going to open
360:58 localhost
361:01 1990 and here you can see that we are
361:05 able to access the default Prometheus UI
361:08 and here I can search for some default
361:11 metrics which are exposed by our spring
361:15 boot actuator endpoints from our micro
361:17 services so I can search for different
361:20 kinds of
361:22 events so I will first search for this
361:24 log back events total
361:26 and click on execute and here you can
361:29 see that it's trying to read all the no
361:33 log back events metrics which are
361:36 exposed by the inventory service the
361:38 order service and the product service
361:40 application not only that you can see
361:42 the status of our services by clicking
361:46 on this status and service Discovery and
361:49 here you can see the all the different
361:51 Services which are you know identified
361:55 by Prometheus so I can also go to the
361:59 status and targets and you can see that
362:02 the inventory service is in the status
362:04 up and it's working fine
362:06 it looks like the notification service
362:08 is not working we will get back to this
362:10 later but let's see how to set up
362:14 grafana and visualize all the metrics so
362:17 to configure grafana I'm going to open
362:19 the URL localhost
362:22 3000
362:24 and this will open a default grafana
362:26 dashboard so as we defined the username
362:30 and password
362:31 has admin and password I guess so let's
362:34 go to the docker compose file and yeah
362:37 so the username is admin and the
362:39 password is password right so I'm going
362:41 to type in admin
362:42 and password and I'm going to login
362:46 and here you can see the default grafana
362:48 home page the first thing you can do
362:51 here is to you have to First add the
362:53 data source for grafana right as I
362:56 mentioned before grafana will needs a
362:58 greater source to do to visualize the
363:00 metrics we will be using Prometheus as a
363:03 data source so let's click on add your
363:05 first data source option here and here
363:07 you can see there are different kinds of
363:09 data source you can add the time series
363:12 databases we can use Prometheus graphite
363:14 and influx DB so I'm going to use
363:17 Prometheus
363:18 and I'm going to name this as
363:22 Prometheus
363:24 micro services
363:26 and I am going to define the URL as http
363:31 Prometheus colon 1990 right because
363:35 Prometheus is the name of the container
363:37 doc container which is running and as
363:39 this is running inside grafana is
363:41 running inside a Docker container I am
363:43 going to provide the name of the
363:45 container so that it will communicate
363:46 with Prometheus using the hostname
363:49 right so after adding this configuration
363:52 I'm just going to scroll down and click
363:54 on Save and test and you can see that
363:56 the data source is updated and the data
363:58 source is working fine using this status
364:01 notification that's fine so the next
364:04 thing I'm going to do is I am going to
364:06 create a dashboard right to visualize
364:09 all our services so we can create our
364:12 own dashboards but this is not so nice
364:15 because we had such creating a dashboard
364:17 is not not an easy task it needs a lot
364:20 of configuration so what I can do is I
364:22 can import some existing dashboard
364:25 configuration so for that I am going to
364:27 open my GitHub repository and when you
364:32 open my GitHub repository and you can
364:34 open and when you open the branch part
364:36 10 you can find a Json file here called
364:39 as grafana dashboard.json so click on
364:43 this Json file and copy everything which
364:45 is inside this particular Json file so
364:48 all the way down I am going to copy
364:51 everything
364:52 and I'm going to paste this inside the
364:54 import via panel Json text area
364:58 right and I'm going to click on load
365:01 like this and here I am going to select
365:03 the Prometheus data source right so
365:06 Prometheus data source going to be
365:08 Prometheus micro Services because this
365:10 is the data source we created
365:12 and I'm going to click on import
365:15 right and now you can see a beautiful
365:18 dashboard with all the information like
365:20 the uptime so and the start time when is
365:24 this particular service started and what
365:27 is the Heap memory which is used and
365:28 what is the non memory which is used and
365:31 you can also see the CPU usage and the
365:34 average load of the services and you can
365:37 see different kinds of statistics for
365:39 the algerium statistics memory you can
365:41 see the Heap space allocation for
365:44 different spaces in our garbage
365:46 collector and different kinds of no
365:50 metrics and dashboards you can visualize
365:52 it here right so you can switch between
365:56 our services using this instance so the
365:59 present view you are seeing is for the
366:02 inventory service you can switch to the
366:04 product service for example and you can
366:06 see that the statistics has now changed
366:08 the Heap space is used as 3.6 percent
366:11 and here you can observe that as our
366:13 product service is not using any
366:16 relational database it's not using the
366:19 the default Hikari connection pooling so
366:22 that's why you see this as none whereas
366:24 if you open order service you can see
366:26 that you can you know select the default
366:29 Hikari connection pools as we are using
366:32 the mySQL database so this is how you
366:34 can monitor your services using
366:37 Prometheus and grafana I hope you
366:39 learned something new I will see you in
366:41 the next video and until then Happy
366:43 coding the keys
